{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mL0ZoKwmSIrL"
      },
      "outputs": [],
      "source": [
        "#Self-hyperparam selection: https://link.springer.com/article/10.1007/s11063-024-11578-0\n",
        "#Self-pruning: https://github.com/skarifahmed/seMLP/blob/main/src/Prune.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PMr_2rxGcZyG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mHello World !\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from utils import Utils\n",
        "from color import color \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "# libraries\n",
        "import joblib\n",
        "\n",
        "# scale features\n",
        "from sklearn import preprocessing\n",
        "from sklearn import impute\n",
        "# classifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "# scoring metrics\n",
        "from sklearn.metrics import confusion_matrix, matthews_corrcoef\n",
        "\n",
        "# custom scripts\n",
        "import sys\n",
        "sys.path.insert(0, \"%s\" % \"CV/\")\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, GroupShuffleSplit, StratifiedShuffleSplit, cross_validate, StratifiedKFold\n",
        "from sklearn.metrics import roc_curve, auc, recall_score, accuracy_score, precision_score, confusion_matrix, make_scorer, matthews_corrcoef, jaccard_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "site_path = \"/Users/sanjanayasna/csc334/MLP_MAHOMES/sites_calculated_features.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mAll features:\u001b[0m\n",
            "sites: 3981 \tcolumns: 485\n",
            "Set   Catalytic\n",
            "data  False        2636\n",
            "      True          829\n",
            "test  False         345\n",
            "      True          171\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#read in feature set:\n",
        "sites = pd.read_csv(site_path)\n",
        "sites = sites.set_index('SITE_ID',drop=True)\n",
        "\n",
        "# The following labels need to be changed after looking over literature (see Feehan, Franklin, Slusky 2021)\n",
        "change_site_labels = [\"5zb8_0\", \"6aci_0\", \"6oq7_0\", \"6pjv_1\", \"6q55_0\",\n",
        "                      \"6q55_2\", \"6rmg_0\", \"6rtg_0\", \"6rw0_0\", \"6v77_0\"]\n",
        "\n",
        "# The following sites are removed due to unkopwn correct labels (see Feehan, Franklin, Slusky 2021)\n",
        "sites.loc[sites.index.isin(change_site_labels), 'Catalytic']=True\n",
        "remove_sites = [\"6mf0_1\", \"6okh_0\", \"6qwo_0\", \"6r9n_0\"]\n",
        "sites=sites.loc[~sites.index.isin(remove_sites)]\n",
        "\n",
        "#print shape of dataset\n",
        "print(color.BOLD + \"All features:\" + color.END)\n",
        "print(\"sites: %s \\tcolumns: %s\"%(sites.shape[0], sites.shape[1]))\n",
        "sizes = sites.groupby([\"Set\", \"Catalytic\"]).size()\n",
        "print(sizes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save_models toggel\n",
        "save_models = False\n",
        "#pkl output path\n",
        "pkl_out = r'/Users/sanjanayasna/csc334/MLP_MAHOMES/pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Catalytic</th>\n",
              "      <th>MetalCodes</th>\n",
              "      <th>MetalAtoms</th>\n",
              "      <th>fa_atr_Sum_3.5</th>\n",
              "      <th>fa_rep_Sum_3.5</th>\n",
              "      <th>fa_sol_Sum_3.5</th>\n",
              "      <th>fa_intra_atr_xover4_Sum_3.5</th>\n",
              "      <th>fa_intra_rep_xover4_Sum_3.5</th>\n",
              "      <th>fa_intra_sol_xover4_Sum_3.5</th>\n",
              "      <th>lk_ball_Sum_3.5</th>\n",
              "      <th>...</th>\n",
              "      <th>geom_cn8</th>\n",
              "      <th>geom_cn9</th>\n",
              "      <th>geom_Filled</th>\n",
              "      <th>geom_PartFilled</th>\n",
              "      <th>geom_AvgN</th>\n",
              "      <th>geom_AvgO</th>\n",
              "      <th>geom_AvgS</th>\n",
              "      <th>geom_AvgOther</th>\n",
              "      <th>SC_vol_perc</th>\n",
              "      <th>Set</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SITE_ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6s9z_0</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-33.20757</td>\n",
              "      <td>20.22373</td>\n",
              "      <td>26.34441</td>\n",
              "      <td>-1.88617</td>\n",
              "      <td>0.46054</td>\n",
              "      <td>2.14096</td>\n",
              "      <td>14.05052</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.910384</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6g5l_0</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-27.04899</td>\n",
              "      <td>39.17134</td>\n",
              "      <td>22.76555</td>\n",
              "      <td>-1.71942</td>\n",
              "      <td>0.45999</td>\n",
              "      <td>2.05517</td>\n",
              "      <td>12.94894</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.862189</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6hwz_0</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-27.30433</td>\n",
              "      <td>35.04867</td>\n",
              "      <td>23.45195</td>\n",
              "      <td>-1.62146</td>\n",
              "      <td>0.35902</td>\n",
              "      <td>1.91231</td>\n",
              "      <td>13.06378</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.991431</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6qww_0</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-25.36664</td>\n",
              "      <td>12.54178</td>\n",
              "      <td>27.17902</td>\n",
              "      <td>-1.14349</td>\n",
              "      <td>0.22087</td>\n",
              "      <td>1.68091</td>\n",
              "      <td>11.47631</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.864546</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6qww_1</th>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-30.53159</td>\n",
              "      <td>8.99318</td>\n",
              "      <td>27.77842</td>\n",
              "      <td>-1.00782</td>\n",
              "      <td>0.39657</td>\n",
              "      <td>1.04229</td>\n",
              "      <td>13.23736</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.990893</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 485 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Catalytic  MetalCodes  MetalAtoms  fa_atr_Sum_3.5  fa_rep_Sum_3.5  \\\n",
              "SITE_ID                                                                      \n",
              "6s9z_0        True           1           1       -33.20757        20.22373   \n",
              "6g5l_0        True           1           1       -27.04899        39.17134   \n",
              "6hwz_0        True           1           1       -27.30433        35.04867   \n",
              "6qww_0        True           1           1       -25.36664        12.54178   \n",
              "6qww_1       False           1           1       -30.53159         8.99318   \n",
              "\n",
              "         fa_sol_Sum_3.5  fa_intra_atr_xover4_Sum_3.5  \\\n",
              "SITE_ID                                                \n",
              "6s9z_0         26.34441                     -1.88617   \n",
              "6g5l_0         22.76555                     -1.71942   \n",
              "6hwz_0         23.45195                     -1.62146   \n",
              "6qww_0         27.17902                     -1.14349   \n",
              "6qww_1         27.77842                     -1.00782   \n",
              "\n",
              "         fa_intra_rep_xover4_Sum_3.5  fa_intra_sol_xover4_Sum_3.5  \\\n",
              "SITE_ID                                                             \n",
              "6s9z_0                       0.46054                      2.14096   \n",
              "6g5l_0                       0.45999                      2.05517   \n",
              "6hwz_0                       0.35902                      1.91231   \n",
              "6qww_0                       0.22087                      1.68091   \n",
              "6qww_1                       0.39657                      1.04229   \n",
              "\n",
              "         lk_ball_Sum_3.5  ...  geom_cn8  geom_cn9  geom_Filled  \\\n",
              "SITE_ID                   ...                                    \n",
              "6s9z_0          14.05052  ...       0.0       0.0          0.0   \n",
              "6g5l_0          12.94894  ...       0.0       0.0          0.0   \n",
              "6hwz_0          13.06378  ...       0.0       0.0          0.0   \n",
              "6qww_0          11.47631  ...       0.0       0.0          0.0   \n",
              "6qww_1          13.23736  ...       0.0       0.0          1.0   \n",
              "\n",
              "         geom_PartFilled  geom_AvgN  geom_AvgO  geom_AvgS  geom_AvgOther  \\\n",
              "SITE_ID                                                                    \n",
              "6s9z_0               1.0        3.0        0.0        0.0            0.0   \n",
              "6g5l_0               1.0        3.0        0.0        0.0            0.0   \n",
              "6hwz_0               1.0        3.0        0.0        0.0            0.0   \n",
              "6qww_0               0.0        0.0        3.0        0.0            0.0   \n",
              "6qww_1               0.0        0.0        4.0        0.0            1.0   \n",
              "\n",
              "         SC_vol_perc   Set  \n",
              "SITE_ID                     \n",
              "6s9z_0      0.910384  test  \n",
              "6g5l_0      0.862189  test  \n",
              "6hwz_0      0.991431  test  \n",
              "6qww_0      0.864546  test  \n",
              "6qww_1      0.990893  test  \n",
              "\n",
              "[5 rows x 485 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sites.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mAll scaled data-set features:\u001b[0m\n",
            "sites: 3465 \tcolumns: 484\n",
            "Catalytic\n",
            "False    2636\n",
            "True      829\n",
            "dtype: int64\n",
            "\u001b[1m\n",
            "All scaled T-metal-site features:\u001b[0m\n",
            "sites: 516 \tcolumns: 484\n",
            "Catalytic\n",
            "False    345\n",
            "True     171\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#Get scaled features\n",
        "data_scaled, Tsites_scaled = Utils.get_scaled_features(sites =sites, pkl_out=pkl_out, save_models=save_models)\n",
        "#Print stats\n",
        "print(color.BOLD + \"All scaled data-set features:\" + color.END)\n",
        "print(\"sites: %s \\tcolumns: %s\"%(data_scaled.shape[0], data_scaled.shape[1]))\n",
        "print(data_scaled.groupby([\"Catalytic\"]).size())\n",
        "\n",
        "print(color.BOLD + \"\\nAll scaled T-metal-site features:\" + color.END)\n",
        "print(\"sites: %s \\tcolumns: %s\"%(Tsites_scaled.shape[0], Tsites_scaled.shape[1]))\n",
        "print(Tsites_scaled.groupby([\"Catalytic\"]).size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "dir = \"/Users/sanjanayasna/csc334/MLP_MAHOMES/data/\"\n",
        "#save the scaled data\n",
        "data_scaled.to_csv(os.path.join(dir, \"data_scaled.csv\"))\n",
        "Tsites_scaled.to_csv(os.path.join(dir, \"Tsites_scaled.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#set feature set type\n",
        "MAHOMES_feature_set = \"AllMeanSph\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Well sampled training data\n",
        "#X is train\n",
        "#y is target for train\n",
        "X, y = Utils.get_training_data(MAHOMES_feature_set, random_seed = 1, data_scaled= data_scaled)\n",
        " ## prepare test-set\n",
        "testX = Tsites_scaled.copy()\n",
        "testY = testX['Catalytic']; del testX['Catalytic']\n",
        "testX = Utils.feature_subset(testX, MAHOMES_feature_set, noBSA=True)\n",
        "\n",
        "## get multiple predictions for test-set w/ diff random seeds\n",
        "test_site_preds = {'actual': pd.Series(testY, index=testX.index)}\n",
        "\n",
        "#Overview:\n",
        "# X: training data\n",
        "# y: target for training data\n",
        "# testX: test data\n",
        "# testY: target for test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "181\n"
          ]
        }
      ],
      "source": [
        "#Train \n",
        "init_features = len(X.columns)\n",
        "print(init_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Prelim mlp\n",
        "#Possible avenue for bias and weight matrix initialization:\n",
        "## Initialize weights using Xavier uniform initialization\n",
        "# init.xavier_uniform_(linear_layer.weight)\n",
        " \n",
        "# ## Initialize bias to zero\n",
        "# init.zeros_(linear_layer.bias)\n",
        "#---------------------------------\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "class MLP(nn.Module):  # nn.Module is the base class for all models in PyTorch\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(init_features, 181),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(181, 90),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(90, 1),\n",
        "            #try to make output binary (0 or 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "     #   x =  self.layers(x)\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Loads to torch tensors\n",
        "class dataLoader:\n",
        "    #Use ONLY train data \n",
        "    def __init__(self, X, y):\n",
        "        #converts x and y to numpy arr so they can be torch tensor\n",
        "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
        "            X = X.to_numpy()\n",
        "            y = y.to_numpy()\n",
        "        #x_train\n",
        "        # if not torch.is_tensor(X):\n",
        "        #     self.X = torch.from_numpy(X)\n",
        "        # #y_train\n",
        "        # if not torch.is_tensor(y):\n",
        "        #     self.y = torch.from_numpy(y)\n",
        "        self.X = torch.from_numpy(X)\n",
        "        self.y = torch.from_numpy(y)\n",
        "    def get_trainloader(dataset):\n",
        "        return torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "    def get_testloader(dataset):\n",
        "        return torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "    #to get lenght, for enumerator use\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "#to set num samples variable for dataset\n",
        "num_samples = len(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.utils.data.sampler as sampler\n",
        "#Will use subsetRandomSampler (which assumes a shuffle=trfue data loading argument)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "#initialize dataloader with random sampling of size 10 \n",
        "dataset = dataLoader(X, y)\n",
        "trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "#mlp init\n",
        "mlp = MLP()\n",
        "#set loss function and gradient descet optimizer\n",
        "loss_function = nn.L1Loss()\n",
        "optimizer = torch.optim.Adagrad(mlp.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<enumerate at 0x13b6d38d0>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#check that enumerate works\n",
        "enumerate(trainloader, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Epoch 1\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.006\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.006\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.007\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.003\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.006\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.006\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.007\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.006\n",
            "Loss after mini-batch   311: 0.006\n",
            "Loss after mini-batch   321: 0.003\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 1 done\n",
            "Starting Epoch 2\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.003\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.006\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.006\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.003\n",
            "Loss after mini-batch   131: 0.006\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.006\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.006\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.006\n",
            "Loss after mini-batch   231: 0.006\n",
            "Loss after mini-batch   241: 0.006\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.006\n",
            "Loss after mini-batch   301: 0.006\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 2 done\n",
            "Starting Epoch 3\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.003\n",
            "Loss after mini-batch   101: 0.006\n",
            "Loss after mini-batch   111: 0.006\n",
            "Loss after mini-batch   121: 0.006\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.006\n",
            "Loss after mini-batch   151: 0.006\n",
            "Loss after mini-batch   161: 0.003\n",
            "Loss after mini-batch   171: 0.006\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.006\n",
            "Loss after mini-batch   201: 0.007\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.006\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.006\n",
            "Loss after mini-batch   331: 0.006\n",
            "Epoch 3 done\n",
            "Starting Epoch 4\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.006\n",
            "Loss after mini-batch    51: 0.006\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.006\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.006\n",
            "Loss after mini-batch   131: 0.006\n",
            "Loss after mini-batch   141: 0.006\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.006\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.006\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.006\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.006\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 4 done\n",
            "Starting Epoch 5\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.006\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.006\n",
            "Loss after mini-batch    41: 0.006\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.006\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.006\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.003\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.006\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.006\n",
            "Loss after mini-batch   241: 0.003\n",
            "Loss after mini-batch   251: 0.006\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.006\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.006\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.006\n",
            "Epoch 5 done\n",
            "Starting Epoch 6\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.006\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.003\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.006\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.006\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.006\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.006\n",
            "Loss after mini-batch   171: 0.006\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.007\n",
            "Loss after mini-batch   201: 0.006\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.006\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 6 done\n",
            "Starting Epoch 7\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.006\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.003\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.007\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.006\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.006\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.006\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.006\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.007\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.007\n",
            "Epoch 7 done\n",
            "Starting Epoch 8\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.007\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.006\n",
            "Loss after mini-batch   121: 0.006\n",
            "Loss after mini-batch   131: 0.007\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.006\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.006\n",
            "Loss after mini-batch   231: 0.006\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.006\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 8 done\n",
            "Starting Epoch 9\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.006\n",
            "Loss after mini-batch    31: 0.003\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.006\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.007\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.006\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.006\n",
            "Loss after mini-batch   241: 0.006\n",
            "Loss after mini-batch   251: 0.003\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.007\n",
            "Loss after mini-batch   311: 0.006\n",
            "Loss after mini-batch   321: 0.006\n",
            "Loss after mini-batch   331: 0.007\n",
            "Epoch 9 done\n",
            "Starting Epoch 10\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.006\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.006\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.006\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.007\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.007\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.006\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.006\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.006\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.006\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 10 done\n"
          ]
        }
      ],
      "source": [
        "#train for this many epochs\n",
        "for epoch in range(0,10):\n",
        "    print(f'Starting Epoch {epoch+1}')\n",
        "\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0], 1))\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = mlp(inputs)\n",
        "\n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        current_loss += loss.item()\n",
        "        if i%10 == 0:\n",
        "            print(f'Loss after mini-batch %5d: %.3f'%(i+1, current_loss/500))\n",
        "            current_loss = 0.0\n",
        "    \n",
        "    print(f'Epoch {epoch+1} done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data = torch.from_numpy(testX.to_numpy()).float()\n",
        "test_targets = torch.from_numpy(testY.to_numpy()).float()\n",
        "print(\"Test data outputs look like this\", test_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=181, out_features=181, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=181, out_features=90, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=90, out_features=1, bias=True)\n",
              "    (5): ReLU()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Run mlp model on test data\n",
        "mlp.eval() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 0.33139534953873107\n",
            "R2 Score: -0.4956522703341628\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "with torch.no_grad():\n",
        "    outputs = mlp(test_data)\n",
        "    predicted_labels = outputs.squeeze().tolist()\n",
        "\n",
        "predicted_labels = np.array(predicted_labels)\n",
        "test_targets = np.array(test_targets)\n",
        "\n",
        "mse = mean_squared_error(test_targets, predicted_labels)\n",
        "r2 = r2_score(test_targets, predicted_labels)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R2 Score:\", r2)\n",
        "# Mean Squared Error: 0.18271737790066617\n",
        "# R2 Score: 0.17536060337896386"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
