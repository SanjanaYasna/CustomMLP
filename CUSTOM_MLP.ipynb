{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mL0ZoKwmSIrL"
      },
      "outputs": [],
      "source": [
        "#Self-hyperparam selection: https://link.springer.com/article/10.1007/s11063-024-11578-0\n",
        "#Self-pruning: https://github.com/skarifahmed/seMLP/blob/main/src/Prune.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PMr_2rxGcZyG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mHello World !\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from utils import Utils\n",
        "from color import color \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "# libraries\n",
        "import joblib\n",
        "\n",
        "# scale features\n",
        "from sklearn import preprocessing\n",
        "from sklearn import impute\n",
        "# classifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "# scoring metrics\n",
        "from sklearn.metrics import confusion_matrix, matthews_corrcoef\n",
        "\n",
        "# custom scripts\n",
        "import sys\n",
        "sys.path.insert(0, \"%s\" % \"CV/\")\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, GroupShuffleSplit, StratifiedShuffleSplit, cross_validate, StratifiedKFold\n",
        "from sklearn.metrics import roc_curve, auc, recall_score, accuracy_score, precision_score, confusion_matrix, make_scorer, matthews_corrcoef, jaccard_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "site_path = \"/Users/sanjanayasna/csc334/MLP_MAHOMES/data/sites_calculated_features.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mAll features:\u001b[0m\n",
            "sites: 3981 \tcolumns: 485\n",
            "Set   Catalytic\n",
            "data  False        2636\n",
            "      True          829\n",
            "test  False         345\n",
            "      True          171\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#read in feature set:\n",
        "sites = pd.read_csv(site_path)\n",
        "sites = sites.set_index('SITE_ID',drop=True)\n",
        "\n",
        "# The following labels need to be changed after looking over literature (see Feehan, Franklin, Slusky 2021)\n",
        "change_site_labels = [\"5zb8_0\", \"6aci_0\", \"6oq7_0\", \"6pjv_1\", \"6q55_0\",\n",
        "                      \"6q55_2\", \"6rmg_0\", \"6rtg_0\", \"6rw0_0\", \"6v77_0\"]\n",
        "\n",
        "# The following sites are removed due to unkopwn correct labels (see Feehan, Franklin, Slusky 2021)\n",
        "sites.loc[sites.index.isin(change_site_labels), 'Catalytic']=True\n",
        "remove_sites = [\"6mf0_1\", \"6okh_0\", \"6qwo_0\", \"6r9n_0\"]\n",
        "sites=sites.loc[~sites.index.isin(remove_sites)]\n",
        "\n",
        "#print shape of dataset\n",
        "print(color.BOLD + \"All features:\" + color.END)\n",
        "print(\"sites: %s \\tcolumns: %s\"%(sites.shape[0], sites.shape[1]))\n",
        "sizes = sites.groupby([\"Set\", \"Catalytic\"]).size()\n",
        "print(sizes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save_models toggel\n",
        "save_models = False\n",
        "#pkl output path\n",
        "pkl_out = r'/Users/sanjanayasna/csc334/MLP_MAHOMES/pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Catalytic</th>\n",
              "      <th>MetalCodes</th>\n",
              "      <th>MetalAtoms</th>\n",
              "      <th>fa_atr_Sum_3.5</th>\n",
              "      <th>fa_rep_Sum_3.5</th>\n",
              "      <th>fa_sol_Sum_3.5</th>\n",
              "      <th>fa_intra_atr_xover4_Sum_3.5</th>\n",
              "      <th>fa_intra_rep_xover4_Sum_3.5</th>\n",
              "      <th>fa_intra_sol_xover4_Sum_3.5</th>\n",
              "      <th>lk_ball_Sum_3.5</th>\n",
              "      <th>...</th>\n",
              "      <th>geom_cn8</th>\n",
              "      <th>geom_cn9</th>\n",
              "      <th>geom_Filled</th>\n",
              "      <th>geom_PartFilled</th>\n",
              "      <th>geom_AvgN</th>\n",
              "      <th>geom_AvgO</th>\n",
              "      <th>geom_AvgS</th>\n",
              "      <th>geom_AvgOther</th>\n",
              "      <th>SC_vol_perc</th>\n",
              "      <th>Set</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SITE_ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6s9z_0</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-33.20757</td>\n",
              "      <td>20.22373</td>\n",
              "      <td>26.34441</td>\n",
              "      <td>-1.88617</td>\n",
              "      <td>0.46054</td>\n",
              "      <td>2.14096</td>\n",
              "      <td>14.05052</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.910384</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6g5l_0</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-27.04899</td>\n",
              "      <td>39.17134</td>\n",
              "      <td>22.76555</td>\n",
              "      <td>-1.71942</td>\n",
              "      <td>0.45999</td>\n",
              "      <td>2.05517</td>\n",
              "      <td>12.94894</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.862189</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6hwz_0</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-27.30433</td>\n",
              "      <td>35.04867</td>\n",
              "      <td>23.45195</td>\n",
              "      <td>-1.62146</td>\n",
              "      <td>0.35902</td>\n",
              "      <td>1.91231</td>\n",
              "      <td>13.06378</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.991431</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6qww_0</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-25.36664</td>\n",
              "      <td>12.54178</td>\n",
              "      <td>27.17902</td>\n",
              "      <td>-1.14349</td>\n",
              "      <td>0.22087</td>\n",
              "      <td>1.68091</td>\n",
              "      <td>11.47631</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.864546</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6qww_1</th>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-30.53159</td>\n",
              "      <td>8.99318</td>\n",
              "      <td>27.77842</td>\n",
              "      <td>-1.00782</td>\n",
              "      <td>0.39657</td>\n",
              "      <td>1.04229</td>\n",
              "      <td>13.23736</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.990893</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 485 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Catalytic  MetalCodes  MetalAtoms  fa_atr_Sum_3.5  fa_rep_Sum_3.5  \\\n",
              "SITE_ID                                                                      \n",
              "6s9z_0        True           1           1       -33.20757        20.22373   \n",
              "6g5l_0        True           1           1       -27.04899        39.17134   \n",
              "6hwz_0        True           1           1       -27.30433        35.04867   \n",
              "6qww_0        True           1           1       -25.36664        12.54178   \n",
              "6qww_1       False           1           1       -30.53159         8.99318   \n",
              "\n",
              "         fa_sol_Sum_3.5  fa_intra_atr_xover4_Sum_3.5  \\\n",
              "SITE_ID                                                \n",
              "6s9z_0         26.34441                     -1.88617   \n",
              "6g5l_0         22.76555                     -1.71942   \n",
              "6hwz_0         23.45195                     -1.62146   \n",
              "6qww_0         27.17902                     -1.14349   \n",
              "6qww_1         27.77842                     -1.00782   \n",
              "\n",
              "         fa_intra_rep_xover4_Sum_3.5  fa_intra_sol_xover4_Sum_3.5  \\\n",
              "SITE_ID                                                             \n",
              "6s9z_0                       0.46054                      2.14096   \n",
              "6g5l_0                       0.45999                      2.05517   \n",
              "6hwz_0                       0.35902                      1.91231   \n",
              "6qww_0                       0.22087                      1.68091   \n",
              "6qww_1                       0.39657                      1.04229   \n",
              "\n",
              "         lk_ball_Sum_3.5  ...  geom_cn8  geom_cn9  geom_Filled  \\\n",
              "SITE_ID                   ...                                    \n",
              "6s9z_0          14.05052  ...       0.0       0.0          0.0   \n",
              "6g5l_0          12.94894  ...       0.0       0.0          0.0   \n",
              "6hwz_0          13.06378  ...       0.0       0.0          0.0   \n",
              "6qww_0          11.47631  ...       0.0       0.0          0.0   \n",
              "6qww_1          13.23736  ...       0.0       0.0          1.0   \n",
              "\n",
              "         geom_PartFilled  geom_AvgN  geom_AvgO  geom_AvgS  geom_AvgOther  \\\n",
              "SITE_ID                                                                    \n",
              "6s9z_0               1.0        3.0        0.0        0.0            0.0   \n",
              "6g5l_0               1.0        3.0        0.0        0.0            0.0   \n",
              "6hwz_0               1.0        3.0        0.0        0.0            0.0   \n",
              "6qww_0               0.0        0.0        3.0        0.0            0.0   \n",
              "6qww_1               0.0        0.0        4.0        0.0            1.0   \n",
              "\n",
              "         SC_vol_perc   Set  \n",
              "SITE_ID                     \n",
              "6s9z_0      0.910384  test  \n",
              "6g5l_0      0.862189  test  \n",
              "6hwz_0      0.991431  test  \n",
              "6qww_0      0.864546  test  \n",
              "6qww_1      0.990893  test  \n",
              "\n",
              "[5 rows x 485 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sites.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mAll scaled data-set features:\u001b[0m\n",
            "sites: 3465 \tcolumns: 484\n",
            "Catalytic\n",
            "False    2636\n",
            "True      829\n",
            "dtype: int64\n",
            "\u001b[1m\n",
            "All scaled T-metal-site features:\u001b[0m\n",
            "sites: 516 \tcolumns: 484\n",
            "Catalytic\n",
            "False    345\n",
            "True     171\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#Get scaled features\n",
        "data_scaled, Tsites_scaled = Utils.get_scaled_features(sites =sites, pkl_out=pkl_out, save_models=save_models)\n",
        "#Print stats\n",
        "print(color.BOLD + \"All scaled data-set features:\" + color.END)\n",
        "print(\"sites: %s \\tcolumns: %s\"%(data_scaled.shape[0], data_scaled.shape[1]))\n",
        "print(data_scaled.groupby([\"Catalytic\"]).size())\n",
        "\n",
        "print(color.BOLD + \"\\nAll scaled T-metal-site features:\" + color.END)\n",
        "print(\"sites: %s \\tcolumns: %s\"%(Tsites_scaled.shape[0], Tsites_scaled.shape[1]))\n",
        "print(Tsites_scaled.groupby([\"Catalytic\"]).size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "dir = \"/Users/sanjanayasna/csc334/MLP_MAHOMES/data/\"\n",
        "#save the scaled data\n",
        "data_scaled.to_csv(os.path.join(dir, \"data_scaled.csv\"))\n",
        "Tsites_scaled.to_csv(os.path.join(dir, \"Tsites_scaled.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#set feature set type\n",
        "MAHOMES_feature_set = \"AllMeanSph\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Well sampled training data\n",
        "#X is train\n",
        "#y is target for train\n",
        "X, y = Utils.get_training_data(MAHOMES_feature_set, random_seed = 1, data_scaled= data_scaled)\n",
        " ## prepare test-set\n",
        "testX = Tsites_scaled.copy()\n",
        "testY = testX['Catalytic']; del testX['Catalytic']\n",
        "testX = Utils.feature_subset(testX, MAHOMES_feature_set, noBSA=True)\n",
        "\n",
        "## get multiple predictions for test-set w/ diff random seeds\n",
        "test_site_preds = {'actual': pd.Series(testY, index=testX.index)}\n",
        "\n",
        "#Overview:\n",
        "# X: training data\n",
        "# y: target for training data\n",
        "# testX: test data\n",
        "# testY: target for test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.utils.data.sampler as sampler\n",
        "#Will use subsetRandomSampler (which assumes a shuffle=trfue data loading argument)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "181\n"
          ]
        }
      ],
      "source": [
        "#Train input feed params\n",
        "init_features = len(X.columns)\n",
        "twice = init_features * 2\n",
        "print(init_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import torch\n",
        "def mse_r2_calculator(mlp, test_data, test_targets):\n",
        "    with torch.no_grad():\n",
        "        outputs = mlp(test_data)\n",
        "        predicted_labels = outputs.squeeze().tolist()\n",
        "\n",
        "    predicted_labels = np.array(predicted_labels)\n",
        "    test_targets = np.array(test_targets)\n",
        "\n",
        "    mse = mean_squared_error(test_targets, predicted_labels)\n",
        "    r2 = r2_score(test_targets, predicted_labels)\n",
        "    print(\"Mean Squared Error:\", mse)\n",
        "    print(\"R2 Score:\", r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Loads to torch tensors\n",
        "class dataLoader:\n",
        "    #Use ONLY train data \n",
        "    def __init__(self, X, y):\n",
        "        #converts x and y to numpy arr so they can be torch tensor\n",
        "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
        "            X = X.to_numpy()\n",
        "            y = y.to_numpy()\n",
        "        #x_train\n",
        "        # if not torch.is_tensor(X):\n",
        "        #     self.X = torch.from_numpy(X)\n",
        "        # #y_train\n",
        "        # if not torch.is_tensor(y):\n",
        "        #     self.y = torch.from_numpy(y)\n",
        "            \n",
        "        #To convert boolean to int, do .long()\n",
        "        self.X = torch.from_numpy(X)\n",
        "        self.y = torch.from_numpy(y).long()\n",
        "    def get_trainloader(dataset):\n",
        "        return torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "    def get_testloader(dataset):\n",
        "        return torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "    #to get lenght, for enumerator use\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Epoch 1\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.007\n",
            "Loss after mini-batch    21: 0.007\n",
            "Loss after mini-batch    31: 0.006\n",
            "Loss after mini-batch    41: 0.006\n",
            "Loss after mini-batch    51: 0.006\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.006\n",
            "Loss after mini-batch    81: 0.006\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.006\n",
            "Loss after mini-batch   121: 0.006\n",
            "Loss after mini-batch   131: 0.006\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.006\n",
            "Loss after mini-batch   171: 0.006\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.007\n",
            "Loss after mini-batch   241: 0.006\n",
            "Loss after mini-batch   251: 0.006\n",
            "Loss after mini-batch   261: 0.006\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 1 done\n",
            "Starting Epoch 2\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.006\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.006\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.006\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 2 done\n",
            "Starting Epoch 3\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 3 done\n",
            "Starting Epoch 4\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 4 done\n",
            "Starting Epoch 5\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.006\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 5 done\n",
            "Starting Epoch 6\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.006\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.006\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 6 done\n",
            "Starting Epoch 7\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.006\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 7 done\n",
            "Starting Epoch 8\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.006\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 8 done\n",
            "Starting Epoch 9\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 9 done\n",
            "Starting Epoch 10\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.003\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 10 done\n",
            "Starting Epoch 11\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.003\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 11 done\n",
            "Starting Epoch 12\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.003\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 12 done\n",
            "Starting Epoch 13\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 13 done\n",
            "Starting Epoch 14\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.003\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 14 done\n",
            "Starting Epoch 15\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.003\n",
            "Loss after mini-batch   251: 0.003\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 15 done\n",
            "Starting Epoch 16\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 16 done\n",
            "Starting Epoch 17\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 17 done\n",
            "Starting Epoch 18\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 18 done\n",
            "Starting Epoch 19\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.003\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 19 done\n",
            "Starting Epoch 20\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.003\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.003\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.003\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 20 done\n",
            "Starting Epoch 21\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.003\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.003\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 21 done\n",
            "Starting Epoch 22\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 22 done\n",
            "Starting Epoch 23\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.003\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.006\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.003\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 23 done\n",
            "Starting Epoch 24\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 24 done\n",
            "Starting Epoch 25\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 25 done\n",
            "Starting Epoch 26\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.003\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 26 done\n",
            "Starting Epoch 27\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.003\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 27 done\n",
            "Starting Epoch 28\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.003\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 28 done\n",
            "Starting Epoch 29\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.003\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 29 done\n",
            "Starting Epoch 30\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.003\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.003\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 30 done\n",
            "Starting Epoch 31\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.003\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.003\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 31 done\n",
            "Starting Epoch 32\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.003\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 32 done\n",
            "Starting Epoch 33\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 33 done\n",
            "Starting Epoch 34\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.003\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.003\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 34 done\n",
            "Starting Epoch 35\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.003\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 35 done\n",
            "Starting Epoch 36\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.003\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.003\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.003\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 36 done\n",
            "Starting Epoch 37\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.003\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 37 done\n",
            "Starting Epoch 38\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 38 done\n",
            "Starting Epoch 39\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.003\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 39 done\n",
            "Starting Epoch 40\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.003\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.003\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 40 done\n",
            "Starting Epoch 41\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.003\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 41 done\n",
            "Starting Epoch 42\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.003\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.003\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 42 done\n",
            "Starting Epoch 43\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.003\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 43 done\n",
            "Starting Epoch 44\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.003\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 44 done\n",
            "Starting Epoch 45\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.003\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.003\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.003\n",
            "Loss after mini-batch   241: 0.003\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.003\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.003\n",
            "Epoch 45 done\n",
            "Starting Epoch 46\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.006\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.003\n",
            "Epoch 46 done\n",
            "Starting Epoch 47\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.003\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.003\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.003\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.003\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 47 done\n",
            "Starting Epoch 48\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.003\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.003\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 48 done\n",
            "Starting Epoch 49\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.003\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.003\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.003\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 49 done\n",
            "Starting Epoch 50\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.003\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.003\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.003\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.003\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 50 done\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 0.09421305668796993\n",
            "R2 Score: 0.5747979797344294\n"
          ]
        }
      ],
      "source": [
        "#Another attempt https://github.com/daenuprobst/theia/blob/main/src/theia/ml/mlp_classifier.py \n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "class theiaMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(theiaMLP, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.fc1(x)\n",
        "        tanh = self.tanh(hidden)\n",
        "        output = self.fc2(tanh)\n",
        "        return output\n",
        "    \n",
        "\n",
        "#initialize dataloader with random sampling of size 10 \n",
        "dataset = dataLoader(X, y)\n",
        "trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle=True)\n",
        "\n",
        "#mlp init\n",
        "mlp = theiaMLP(init_features, 1000, 1)\n",
        "#set loss function and gradient descet optimizer\n",
        "loss_function = nn.L1Loss()\n",
        "optimizer = torch.optim.Adagrad(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "#train loss\n",
        "train_loss = []\n",
        "mlp.train()\n",
        "\n",
        "#train for this many epochs\n",
        "for epoch in range(0,50):\n",
        "    print(f'Starting Epoch {epoch+1}')\n",
        "\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0], 1))\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = mlp(inputs)\n",
        "\n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        current_loss += loss.item()\n",
        "        if i%10 == 0:\n",
        "            print(f'Loss after mini-batch %5d: %.3f'%(i+1, current_loss/500))\n",
        "            current_loss = 0.0\n",
        "    train_loss.append(loss.item())\n",
        "    print(f'Epoch {epoch+1} done')\n",
        "\n",
        "\n",
        "plt.plot(train_loss)\n",
        "plt.show()\n",
        "\n",
        "test_data = torch.from_numpy(testX.to_numpy()).float()\n",
        "test_targets = torch.from_numpy(testY.to_numpy()).float()\n",
        "\n",
        "#print mse and r2\n",
        "mse_r2_calculator(mlp, test_data, test_targets)\n",
        "#Results\n",
        "# Mean Squared Error: 0.11795077403554882\n",
        "# R2 Score: 0.46766500127569677"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Epoch 1\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.007\n",
            "Loss after mini-batch    21: 0.007\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.006\n",
            "Loss after mini-batch    61: 0.006\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.006\n",
            "Loss after mini-batch   101: 0.006\n",
            "Loss after mini-batch   111: 0.006\n",
            "Loss after mini-batch   121: 0.006\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.006\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.006\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.006\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.006\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 1 done\n",
            "Starting Epoch 2\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 2 done\n",
            "Starting Epoch 3\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.006\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 3 done\n",
            "Starting Epoch 4\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 4 done\n",
            "Starting Epoch 5\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 5 done\n",
            "Starting Epoch 6\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 6 done\n",
            "Starting Epoch 7\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 7 done\n",
            "Starting Epoch 8\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.003\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.003\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 8 done\n",
            "Starting Epoch 9\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 9 done\n",
            "Starting Epoch 10\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 10 done\n",
            "Starting Epoch 11\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.003\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.003\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 11 done\n",
            "Starting Epoch 12\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 12 done\n",
            "Starting Epoch 13\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.003\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.003\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 13 done\n",
            "Starting Epoch 14\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.003\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 14 done\n",
            "Starting Epoch 15\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.003\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.006\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 15 done\n",
            "Starting Epoch 16\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.003\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 16 done\n",
            "Starting Epoch 17\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.003\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.003\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 17 done\n",
            "Starting Epoch 18\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.003\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.003\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 18 done\n",
            "Starting Epoch 19\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.003\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 19 done\n",
            "Starting Epoch 20\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.003\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.003\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 20 done\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 0.09783460591148274\n",
            "R2 Score: 0.5584532171244132\n"
          ]
        }
      ],
      "source": [
        "#Prelim mlp\n",
        "#Possible avenue for bias and weight matrix initialization:\n",
        "## Initialize weights using Xavier uniform initialization\n",
        "# init.xavier_uniform_(linear_layer.weight)\n",
        " \n",
        "# ## Initialize bias to zero\n",
        "# init.zeros_(linear_layer.bias)\n",
        "#---------------------------------\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "#Previous experiment\n",
        "class MLP(nn.Module):  # nn.Module is the base class for all models in PyTorch\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(init_features, 181),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(181, 90),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(90, 1),\n",
        "            nn.Sigmoid()\n",
        "            #try to make output binary (0 or 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "     #   x =  self.layers(x)\n",
        "        return self.layers(x)\n",
        "    \n",
        "\n",
        "#initialize dataloader with random sampling of size 10 \n",
        "dataset = dataLoader(X, y)\n",
        "trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle=True)\n",
        "\n",
        "#mlp init\n",
        "mlp = theiaMLP(init_features, 1000, 1)\n",
        "#set loss function and gradient descet optimizer\n",
        "loss_function = nn.L1Loss()\n",
        "optimizer = torch.optim.Adagrad(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "#train loss\n",
        "train_loss = []\n",
        "mlp.train()\n",
        "\n",
        "#train for this many epochs\n",
        "for epoch in range(0,20):\n",
        "    print(f'Starting Epoch {epoch+1}')\n",
        "\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0], 1))\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = mlp(inputs)\n",
        "\n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "     \n",
        "\n",
        "        current_loss += loss.item()\n",
        "        if i%10 == 0:\n",
        "            print(f'Loss after mini-batch %5d: %.3f'%(i+1, current_loss/500))\n",
        "            current_loss = 0.0\n",
        "    train_loss.append(loss.item())\n",
        "    \n",
        "    print(f'Epoch {epoch+1} done')\n",
        "\n",
        "\n",
        "plt.plot(train_loss)\n",
        "plt.show()\n",
        "\n",
        "test_data = torch.from_numpy(testX.to_numpy()).float()\n",
        "test_targets = torch.from_numpy(testY.to_numpy()).float()\n",
        "\n",
        "mse_r2_calculator(mlp, test_data, test_targets)\n",
        "#results\n",
        "# Mean Squared Error: 0.09783460591148274\n",
        "# R2 Score: 0.5584532171244132"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.003\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.003\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.003\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.003\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.003\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.003\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.006\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.003\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.003\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.003\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.003\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.003\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.003\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.003\n",
            "Loss after mini-batch   171: 0.003\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.003\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.003\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.003\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.003\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.003\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.003\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.003\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.003\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.003\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.003\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.003\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.003\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.003\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.003\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.003\n",
            "Loss after mini-batch   271: 0.003\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.003\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.003\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.003\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.003\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.003\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.003\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.003\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.003\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.003\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.003\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.003\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.003\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.003\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.003\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.003\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.003\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.003\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.003\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.003\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.003\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.003\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.003\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.006\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.003\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.003\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.003\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.003\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.003\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.003\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.003\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.003\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.003\n",
            "Loss after mini-batch    61: 0.003\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.003\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.003\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.003\n",
            "Loss after mini-batch   231: 0.003\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.003\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.003\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.003\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.003\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.003\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.003\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.003\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.003\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.003\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.003\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.003\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.003\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.003\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.003\n",
            "Loss after mini-batch   141: 0.003\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.003\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.003\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.003\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.003\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.003\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.003\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.003\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.003\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.003\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.003\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.003\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.003\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.003\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.003\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.003\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.003\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.003\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.003\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.006\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.003\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.003\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.003\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.003\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.003\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.003\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.003\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.003\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.003\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.003\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.003\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.003\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.003\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.003\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.003\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.003\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.003\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.003\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.003\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.003\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.003\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.003\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.003\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.003\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.003\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.003\n",
            "Loss after mini-batch   171: 0.003\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.003\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.003\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.003\n",
            "Loss after mini-batch    71: 0.003\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.003\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.003\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.003\n",
            "Loss after mini-batch   291: 0.003\n",
            "Loss after mini-batch   301: 0.003\n",
            "Loss after mini-batch   311: 0.003\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.003\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.003\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.003\n",
            "Loss after mini-batch   201: 0.003\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.003\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.003\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.003\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.003\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.003\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.003\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.003\n",
            "Loss after mini-batch    71: 0.003\n",
            "Loss after mini-batch    81: 0.003\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.003\n",
            "Loss after mini-batch   121: 0.003\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.003\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.006\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.003\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.003\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.003\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.003\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.003\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.003\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.003\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.003\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.003\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.003\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.003\n",
            "Loss after mini-batch   291: 0.003\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.003\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.003\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.003\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.003\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.003\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.003\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.003\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.003\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.003\n",
            "Loss after mini-batch   251: 0.003\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.003\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.003\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.003\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.003\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.003\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.003\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.003\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.003\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.003\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.003\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.003\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.003\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.003\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.003\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.003\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.003\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.003\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.003\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.003\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.003\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.003\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.003\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.003\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.003\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.003\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.003\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.003\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.003\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.003\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.003\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.003\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.003\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.003\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.003\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.003\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.003\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.003\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.003\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.003\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.003\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.003\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.003\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.003\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.003\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.003\n",
            "Loss after mini-batch   291: 0.003\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.003\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.003\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.003\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.003\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.003\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.003\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.003\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.003\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.003\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.003\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.003\n",
            "Loss after mini-batch   201: 0.003\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.003\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.003\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.006\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.003\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.003\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.003\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.003\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.003\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.003\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.003\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.003\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.003\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.003\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.003\n",
            "Loss after mini-batch    51: 0.003\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.003\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.003\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.003\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.003\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.003\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.003\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.003\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.003\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.003\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.003\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.003\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.003\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.003\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.003\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.003\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.003\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.003\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.003\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.003\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.003\n",
            "Loss after mini-batch   281: 0.003\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.003\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.003\n",
            "Loss after mini-batch   241: 0.003\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.003\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.003\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.003\n",
            "Loss after mini-batch   271: 0.003\n",
            "Loss after mini-batch   281: 0.003\n",
            "Loss after mini-batch   291: 0.003\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.003\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.003\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.003\n",
            "Loss after mini-batch    31: 0.003\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.003\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.003\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.003\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.003\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.004\n",
            "Last iteration loss value: 0.4497932493686676\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 0.09421305668796993\n",
            "R2 Score: 0.5747979797344294\n"
          ]
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "batch_size = 10\n",
        "#This one uses dropout before the output layer\n",
        "\n",
        "#Layer size\n",
        "n_hidden1 = 800  # Number of hidden nodes\n",
        "n_hidden2 = 100\n",
        "n_output =  1   # Number of output nodes = for binary classifier\n",
        "\n",
        "class ChurnModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ChurnModel, self).__init__()\n",
        "        self.layer_1 = nn.Linear(init_features, n_hidden1) \n",
        "        self.layer_2 = nn.Linear(n_hidden1, n_hidden2)\n",
        "        self.layer_out = nn.Linear(n_hidden2, n_output) \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid =  nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(n_hidden1)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(n_hidden2)\n",
        "        \n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.sigmoid(self.layer_out(x))\n",
        "        \n",
        "        return x\n",
        "    \n",
        "\n",
        "model = ChurnModel()\n",
        "\n",
        "#Loss Computation\n",
        "loss_func = nn.BCELoss()\n",
        "#Optimizer\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#After plotting loss functions, lowest value at 17 epochs, but then it goes crazy the rest of the way. More epochs needed\n",
        "epochs = 100\n",
        "\n",
        "model.train()\n",
        "train_loss = []\n",
        "for epoch in range(epochs):\n",
        "    #Within each epoch run the subsets of data = batch sizes.\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0], 1))\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = mlp(inputs)\n",
        "\n",
        "        loss = loss_function(outputs, targets)\n",
        "        #This time optimizer initialized to zero with each epoch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        current_loss += loss.item()\n",
        "        if i%10 == 0:\n",
        "            print(f'Loss after mini-batch %5d: %.3f'%(i+1, current_loss/500))\n",
        "            current_loss = 0.0\n",
        "    train_loss.append(loss.item())\n",
        "print('Last iteration loss value: '+ str(loss.item()))\n",
        "\n",
        "plt.plot(train_loss)\n",
        "plt.show()\n",
        "\n",
        "test_data = torch.from_numpy(testX.to_numpy()).float()\n",
        "test_targets = torch.from_numpy(testY.to_numpy()).float()\n",
        "\n",
        "mse_r2_calculator(mlp, test_data, test_targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-10 12:56:53.470803: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/var/folders/dc/rx31slbx2fx8zv76zpst7ry80000gq/T/ipykernel_2815/3151587895.py:5: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner.tuners import RandomSearch\n"
          ]
        }
      ],
      "source": [
        "#to resolve missing distutils if your python is most recent:\n",
        "import setuptools.dist \n",
        "#other crap\n",
        "import tensorflow as tf\n",
        "from kerastuner.tuners import RandomSearch\n",
        "import keras_tuner\n",
        "import keras\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"torch\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Functional' object has no attribute 'parameters'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/Users/sanjanayasna/csc334/MLP_MAHOMES/CUSTOM_MLP.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sanjanayasna/csc334/MLP_MAHOMES/CUSTOM_MLP.ipynb#X42sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m#default adam optimizer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sanjanayasna/csc334/MLP_MAHOMES/CUSTOM_MLP.ipynb#X42sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sanjanayasna/csc334/MLP_MAHOMES/CUSTOM_MLP.ipynb#X42sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sanjanayasna/csc334/MLP_MAHOMES/CUSTOM_MLP.ipynb#X42sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Instantiate a torch loss function that is default l1loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sanjanayasna/csc334/MLP_MAHOMES/CUSTOM_MLP.ipynb#X42sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m loss_fn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mL1Loss()\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'parameters'"
          ]
        }
      ],
      "source": [
        "#Now try for Keras hyperparameter optimizer, using the MLP model outlined above:\n",
        "\n",
        "\n",
        " # function builder, like original MLP, uses 2 relus, and 2 hidden layers, with signoid at end\n",
        "def init_model():\n",
        "    inputs = keras.Input(shape=(init_features,), name = \"inputs\")\n",
        "    #first relu\n",
        "    actv1 = keras.layers.Dense(init_features, activation=\"relu\")(inputs)\n",
        "    layer1 = keras.layers.Dense(init_features, activation=\"relu\")(actv1)\n",
        "    #second relu\n",
        "    actv2 = keras.layers.Dense(init_features, activation=\"relu\")(layer1)\n",
        "    layer2 = keras.layers.Dense(90, activation=\"relu\")(actv2)\n",
        "    #sigmoid activation\n",
        "    actv3 = keras.layers.Dense(90, activation=\"relu\")(layer2)\n",
        "    output = keras.layers.Dense(1, activation=\"sigmoid\", name = \"outputs\")(actv3) #shoudl be one binary output, hopefully\n",
        "    model = keras.Model(inputs=inputs, outputs=output)\n",
        "    return model\n",
        "    \n",
        "#10 is default batch size for everything here\n",
        "batch_size = 10\n",
        "#initialize model\n",
        "model = init_model()\n",
        "#default adam optimizer\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Instantiate a torch loss function that is default l1loss\n",
        "loss_fn = torch.nn.L1Loss()\n",
        "#dataloaders\n",
        "dataset = dataLoader(X, y)\n",
        "trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle=True)\n",
        "epochs = 20\n",
        "\n",
        "#train the model\n",
        "for epoch in range(0,20):\n",
        "    print(f'Starting Epoch {epoch+1}')\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0], 1))\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #forward pass\n",
        "        logits = model(inputs)\n",
        "        #compute loss \n",
        "        loss = loss_fn(logits, targets)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        current_loss += loss.item()\n",
        "        if i%10 == 0:\n",
        "            print(f'Loss after mini-batch %5d: %.3f'%(i+1, current_loss/500))\n",
        "            current_loss = 0.0\n",
        "    train_loss.append(loss.item())\n",
        "    \n",
        "    print(f'Epoch {epoch+1} done')\n",
        "\n",
        "# #initialize dataloader with random sampling of size 10 \n",
        "# dataset = dataLoader(X, y)\n",
        "# trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle = True)\n",
        "# testloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle=True)\n",
        "\n",
        "# #mlp init\n",
        "# mlp = theiaMLP(init_features, 1000, 1)\n",
        "# #set loss function and gradient descet optimizer\n",
        "# loss_function = nn.L1Loss()\n",
        "# optimizer = torch.optim.Adagrad(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "# #train loss\n",
        "# train_loss = []\n",
        "# mlp.train()\n",
        "\n",
        "# #train for this many epochs\n",
        "# for epoch in range(0,20):\n",
        "#     print(f'Starting Epoch {epoch+1}')\n",
        "\n",
        "#     current_loss = 0.0\n",
        "#     for i, data in enumerate(trainloader, 0):\n",
        "#         inputs, targets = data\n",
        "#         inputs, targets = inputs.float(), targets.float()\n",
        "#         targets = targets.reshape((targets.shape[0], 1))\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         outputs = mlp(inputs)\n",
        "\n",
        "#         loss = loss_function(outputs, targets)\n",
        "\n",
        "#         loss.backward()\n",
        "\n",
        "#         optimizer.step()\n",
        "     \n",
        "\n",
        "#         current_loss += loss.item()\n",
        "#         if i%10 == 0:\n",
        "#             print(f'Loss after mini-batch %5d: %.3f'%(i+1, current_loss/500))\n",
        "#             current_loss = 0.0\n",
        "#     train_loss.append(loss.item())\n",
        "    \n",
        "#     print(f'Epoch {epoch+1} done')\n",
        "\n",
        "\n",
        "# plt.plot(train_loss)\n",
        "# plt.show()\n",
        "\n",
        "# test_data = torch.from_numpy(testX.to_numpy()).float()\n",
        "# test_targets = torch.from_numpy(testY.to_numpy()).float()\n",
        "\n",
        "# mse_r2_calculator(mlp, test_data, test_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Epoch 1\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.007\n",
            "Loss after mini-batch    21: 0.007\n",
            "Loss after mini-batch    31: 0.008\n",
            "Loss after mini-batch    41: 0.006\n",
            "Loss after mini-batch    51: 0.007\n",
            "Loss after mini-batch    61: 0.006\n",
            "Loss after mini-batch    71: 0.007\n",
            "Loss after mini-batch    81: 0.007\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.006\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.007\n",
            "Loss after mini-batch   131: 0.006\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.006\n",
            "Loss after mini-batch   161: 0.006\n",
            "Loss after mini-batch   171: 0.006\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.006\n",
            "Loss after mini-batch   211: 0.006\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.006\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.006\n",
            "Loss after mini-batch   261: 0.006\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.006\n",
            "Loss after mini-batch   331: 0.006\n",
            "Epoch 1 done\n",
            "Starting Epoch 2\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.006\n",
            "Loss after mini-batch    31: 0.006\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.006\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.006\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.006\n",
            "Loss after mini-batch   121: 0.006\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.006\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 2 done\n",
            "Starting Epoch 3\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.006\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.006\n",
            "Loss after mini-batch   281: 0.006\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.006\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 3 done\n",
            "Starting Epoch 4\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.006\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.006\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.006\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 4 done\n",
            "Starting Epoch 5\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.006\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.006\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.006\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 5 done\n",
            "Starting Epoch 6\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 6 done\n",
            "Starting Epoch 7\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.003\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.006\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 7 done\n",
            "Starting Epoch 8\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.006\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.006\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 8 done\n",
            "Starting Epoch 9\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.003\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.006\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.006\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 9 done\n",
            "Starting Epoch 10\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 10 done\n",
            "Test data correct outputs look like this tensor([1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
            "Mean Squared Error: 0.10848907651952155\n",
            "R2 Score: 0.5103674996383281\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGhCAYAAABCse9yAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABaA0lEQVR4nO3deXRU9fk/8PedPctk3zeSsIXIkkAg4oZLLPq1VlsVtLYoVdtasbX5dpG2alvbopb65We1UqnUrVZs1bZai9UgrsgeQCBhSUL2fZmss97fH7MkgRAyyczce2fer3M4R8Lk5uFgkiefz7MIoiiKICIiIpIxldQBEBEREZ0LExYiIiKSPSYsREREJHtMWIiIiEj2mLAQERGR7DFhISIiItljwkJERESyx4SFiIiIZI8JCxEREckeExYiIiKSvUklLE899RSys7NhMBhQXFyMXbt2nfW1r7/+OoqKihATE4OIiAgUFBTgxRdfHPUaURTx4IMPIjU1FWFhYSgpKcHx48cnExoREREFIa8Tli1btqC0tBQPPfQQ9u3bhwULFmD58uVobW0d8/VxcXH46U9/ih07duDgwYNYvXo1Vq9ejXfeecfzmsceewxPPPEENm7ciJ07dyIiIgLLly/H0NDQ5P9mREREFDQEb5cfFhcXY/HixXjyyScBAA6HA5mZmbj33ntx//33T+gZCxcuxDXXXIOHH34YoigiLS0N//u//4sf/OAHAICenh4kJyfjueeew80333zO5zkcDjQ2NsJoNEIQBG/+OkRERCQRURTR29uLtLQ0qFTjn6FovHmwxWLB3r17sXbtWs/bVCoVSkpKsGPHjgkFtm3bNlRWVuLRRx8FAFRXV6O5uRklJSWe10VHR6O4uBg7duwYM2Exm80wm82e3zc0NCA/P9+bvwoRERHJRF1dHTIyMsZ9jVcJS3t7O+x2O5KTk0e9PTk5GRUVFWd9v56eHqSnp8NsNkOtVuMPf/gDrrzySgBAc3Oz5xmnP9P9Z6dbt24dfvGLX5zx9rq6OkRFRXnzVyIiIiKJmEwmZGZmwmg0nvO1XiUsk2U0GlFeXo6+vj6UlZWhtLQUubm5uPTSSyf1vLVr16K0tNTze/dfOCoqigkLERGRwkyknMOrhCUhIQFqtRotLS2j3t7S0oKUlJSzvp9KpcKMGTMAAAUFBTh69CjWrVuHSy+91PN+LS0tSE1NHfXMgoKCMZ+n1+uh1+u9CZ2IiIgUzKsuIZ1Oh0WLFqGsrMzzNofDgbKyMixdunTCz3E4HJ4alJycHKSkpIx6pslkws6dO716JhEREQUvr6+ESktLcdttt6GoqAhLlizBhg0b0N/fj9WrVwMAVq1ahfT0dKxbtw6As96kqKgI06dPh9lsxttvv40XX3wRTz/9NADnMdB9992HX/3qV5g5cyZycnLwwAMPIC0tDddff73v/qZERESkWF4nLCtXrkRbWxsefPBBNDc3o6CgAFu3bvUUzdbW1o5qTerv78d3vvMd1NfXIywsDHl5eXjppZewcuVKz2t+9KMfob+/H9/85jfR3d2Niy66CFu3boXBYPDBX5GIiIiUzus5LHJkMpkQHR2Nnp4eFt0SEREphDffv7lLiIiIiGSPCQsRERHJHhMWIiIikj0mLERERCR7TFiIiIhI9piwEBERkewxYSEiIiLZY8JCivJ5Qw9e3lmLIBgfREREXgjItmYiX/nh3w/iaJMJRoMG1y5IkzocIiIKEJ6wkGLY7A6caO0FALy6p07iaIiIKJCYsJBi1HcNwmp3XgV9fKId9V0DEkdERESBwoSFFKO6vd/z36IIvLa3QcJoiIjO7VB9D1pMQ1KHERSYsJBiVLkSlki9s/Tqb3vr4HCw+JaI5OnV3XW49smPcefze6QOJSgwYSHFqG7vAwCsKMqE0aBBfdcgdlR1SBwVEdGZPj3Zjp+8cQgA8HljDwYsNokjUj4mLKQY7iuhOalGXFfg7BBi8S0Ryc3Jtj7c/dI+2FwnwKIIVDT3ShyV8jFhIcWobnMmLLmJEVhZlAUA+M/nzegZsEoZFhGRR1e/Bd94bjd6Bq1YmBWD4pw4AMDRJpPEkSkfExZShAGLDY09zsK1nIRIzE2PQl6KERabA/862ChxdEREgNlmx7de3ItTHQPIiA3DM6uKUJAVAwA40siEZaqYsJAi1LQ7W5ijw7SIDddCEASsKMoE4CxsIyKSkiiKWPv6Ieyq6YRRr8Hm2xcjIVKP/NQoADxh8QUmLKQI7vqV3MQICIIAALi+MB1atYBDDT386YWIJPXU+yfw+r4GqFUCnrp1IWYlGwHAk7BUNPeyq3GKmLCQIrg7hHISIjxvi4vQ4Qv5KQCcLc5ERFJ462Aj1v/3GADg5186D5fMSvT8WU5CBHQaFQYsdtR2ctjlVDBhIUVwz2DJHZGwAMBNRRkAgH/sb4DZZg94XEQU2vbXduF/Xz0AAPjGhTn4+vnTRv25Rq3CbNdpC6+FpoYJCylClatDKCchctTbL56ZiJQoA7oGrHjvSKsUoRFRiKrrHMBdL+yB2ebAFXlJ+Ok1c8Z83ZxUZ8JyhAnLlDBhIdkTRRFVbWdeCQGAWiXgxkXOUxbOZCGiQOkdsuLO5/egvc+COalReOKWQqhVwpivncPCW59gwkKy1zVghWnIOSXy9IQFGL4W+vB4Gxq7BwMaGxGFHpvdgTUv70dlSy+SjHo8e1sRIlwrQ8Yy3CnE4XFTwYSFZM9dcJsWbUCYTn3Gn0+Lj8D5uXEQReD1ffWBDo+IQswv3zqCD461waBV4dnbFiMtJmzc1+e5EpaG7kEOupwCJiwke576lcQzT1fcPDNZ9tSzdZCI/Oa5T6rxwo5TEARgw8pCzMuIPuf7RIdpke5KaljHMnlMWEj23DNYxroOcrt6biqMeg1qOwews7ozUKERUQjZVtGCX751BADw46vycNXclAm/L+tYpo4JC8ne2TqERgrTqXGtayHi31h8S0Q+drTJhHtf3g+HCKwsysS3Lsn16v3z05iwTBUTFpK96rPMYDmd+1ro7c+bYBriPTER+UaraQh3PLcb/RY7lubG4+Hr53ombk9Uvqu1+WgzE5bJYsJCsuZwiKjuGB7LP54FGdGYlRyJIasDbx7gQkQimrpBix13vrAHjT1DyE2MwMavLYJO4/23TveV0LGWPljtDl+HGRKYsJCsNfYMwmJzQKsWPEVrZzNqIeIedgsR0dQ4HCJKXy3HwfoexIRrsfm2xYgO107qWZmx4YjQqWGxOTzX3OQdJiwka+7roKy4cGjU5/7f9cuF6dCoBByo60ZlM2ceENHk/fa/lfjP583QqgU88/UiZJ/jWno8KpXgaW9mHcvkMGEhWRvuEDp7we1I8ZF6lMxJBsDJt0Q0ea/uqcPT208CAB69YT6W5MRN+Zn5TFimhAkLyZr76PRc9SsjrVjsnHz7xv4GWGy8KyYi7+w42YGfvH4IAHDv5TPwlYUZPnmuu46Fs1gmhwkLyVrVBGawnO6SmYlIMurR2W/BtooWf4VGREGoqq0P335pL2wOEV+cn4rvl8zy2bPdSxB5wjI5TFhI1txj+c/V0jySRq0asRCRxbdENDFd/RZ847nd6Bm0ojArButvWgDVWRYaTsbsFCMEAWjvs6C1d8hnzw0VTFhItsw2O+q7nMsMxxvLP5abXN1C2ytb0dzDLwxEND6LzYFvvbQXNR0DSI8JwzNfL4JBe+busqkI12mQE+/8WsZFiN5jwkKyVdsxAFEEIvUaJEbqvXrfnIQILMmOg0MEXuNCRCIahyiKWPv6Ieyq7kSkXoPNty9GotG7rzkTNYcTbyeNCQvJ1sj6FW+nSgLATUXOa6G/7amDKHIhIhGN7Q/bT+K1ffVQqwQ8detCzE4x+u1jsVNo8piwkGwN7xCa3OyDa+anIkKnRk3HAHbXdPkyNCIKEv8+2ITfvlMJAPj5tflYNivRrx/PXXh7pJEJi7eYsJBsuQtuJ5uwhOs0uHaBcyHilt2cyUJEo+2v7ULpq+UAgNUXZuPrS7P9/jHdrc1V7f0Ystr9/vGCCRMWki3P0kMvC25Hchffvn2oCb1ciEhELvVdA7jrhb0w2xy4PC8JP7smPyAfNyXKgNhwLewOEcdb+gLyMYMFExaSreEtzRObcjuWhVkxmJ4YgUGrHf8+2OSr0IhIwXqHrLjjuT1o7zMjL8WIJ24phNqH7cvjEQTBc8rCOhbvMGEhWeoZtKK9zwIAyE4In/RzBEHAysXuhYi8FiIKdTa7A/f+dT8qW3qRaNRj8+2LEanXBDQGTrydHCYsJEs1rtOVRKMeRsPktqO6fbkwA2qVgH213TjRytkHRKHs4beOYHtlGwxaFf60qghp59gC7w9MWCaHCQvJUtUUC25HSjTqcXleEgBOviUKZc99Uo3nd5wCAGxYWYAFmTGSxDFyRD9HLkwcExaSpWpXS/P0KRTcjrTCVXz7+r56WO1ciEgUat6vaMUv3zoCAPjxVXm4am6qZLHMTDJCqxbQO2RDQ/egZHEoDRMWkqXJLD0cz2WzE5Fo1KO9z4L3K1p98kwiUoajTSaseXkfHCKwoigD316WK2k8Oo0K0xMjXbHxmnqimLCQLFV7EpbJdwiNpFGr8JWF6QBYfEsUSlp7h3DHc7vRb7Hj/Nw4/Or6eZOanO1r7om3HCA3cUxYSHZEURyRsPjmhAUAblrkvBZ6v7INrSYuRCQKdoMWO+56fg8ae4aQmxCBjV9bBJ1GHt/22NrsPXn8yxGN0NprxoDFDpUAZMVNvqX5dDOSIrFoWizsDhGv72/w2XOJSH4cDhH/+7dyHKjvQUy4Fs/evhgx4Tqpw/LIdy9BbGbCMlFMWEh2TrY5O4Qy48J9/tPQyqLhmSyszicKXuv/W4m3DzVDqxbwx68t8ulprS+4T1hOdQygz2yTOBplYMJCsjM84db3X2D+Z34qwnVqVLX1Y+8pLkQkCkZ/21OHP2w/CQB45CvzUZwbL3FEZ4qL0CE5Sg8AqOQpy4QwYSHZqW7zbcHtSJF6Da6Z52xnZPEtUfD5rKoDP3njEABgzWUzcMOiDIkjOrs5LLz1ChMWkh1Pwa2PZrCcboVrVP9bB5vQz6NYoqBR1daHb724F1a7iGvmpaL0yllShzSu4Ym3bG2eCCYsJDv+vBICgKJpschNiMCAxY5/H+JCRAouVrsjJOuzuvotuOP5PegZtKIgMwa/W7EAqgAtNJysfHYKeYUJC8mK1e5AbecAAN+2NI8kCAJuchff7ua1EAWPPTWdyHtgKy58ZBt+8sYhvHekBYMWu9Rh+Z3F5sC3X9qL6vZ+pMeEYdOqIhi0aqnDOif3CUtlcy/sjtBLMr0V2BWVROdQ1zkAm0OEQatCSpTBbx/nhoXpWP/fSuw51YWTbX2eqZNESvbWwSbYHSIae4bw8s5avLyzFjqNChdMj8fleUm4bHYSMn04KkAORFHE2tcPYWd1JyL1Gjx7exESjXqpw5qQnIQIGLQqDFrtqOno59ehc2DCQrIycsKtP49zk6IMuHRWIsoqWvG3PfW4/+o8v30sokApr+sGANxxUQ6sdge2VbSivmsQ2yvbsL2yDcBhzEyKdCYveUlYNC0WWrWyD9r/sP0kXttXD5UAPPnVQuSlREkd0oSpVQJmJxtxoL4HR5tMTFjOgQkLyYq/61dGWrE4E2UVrXhtXz1+8IVZ0Cj8CzeFNrPN7uk2WbV0GqbFR+AXXxJxorUPZRWt2FbRir2nunC8tQ/HW/vwxw+rEGXQ4JJZibg8LwnLZiUiPlIZJxNubx9qwm/fqQQA/PxL5+HS2UkSR+S9/LQoT8LyxflpUocja0xYSFZ8vfRwPJfnJSEhUoe2XjO2V7ahJD/Z7x+TyF+ONvXCYncgLkLnmRAtCAJmJhsxM9mIby+bjp4BKz483oZtFa3YXtmKrgEr3jrYhLcONkEQgILMGFw+23n6cl5alCx27pxNeV03vr+lHABw+wXZWLU0W9J4Jmt4RD87hc6FCQvJyvAMFv8nLFq1Cl8uTMemj6rx6p46JiykaOW1zkGICzKiz5poRIdrce2CNFy7IA12h4jyum687zp9OdJkwv7abuyv7cbv3j2G5Ci9p+7lwhkJiNDL59tFQ/cg7nx+D8w2By6bnYifXTNH6pAmjTuFJk4+/wcSAahqd47l99cMltOtKMrEpo+qsa2iFW29ZsUU6xGdzl2/UpAZO6HXq1UCFk2LxaJpsfjB8tlo6hnE+xXO05dPTrSjxWTGX3fV4a+76qBTq1CcG4fL85JweV4SpsVLN+a+d8iKO57bjfY+M/JSjPj9Vxcq+jo3L8UIAGjqGUJXvwWxEfLZdyQ3TFhINvrNNrSYzAACU8MCADOTjSjMisH+2m78Y38D7rokNyAfl8jX9rsTlqyYSb1/anQYvlqcha8WZ2HIasfO6k7P6Utt5wA+Ot6Oj4634xdvHkFuYgSucBXuLs6OC1jhrs3uwL1/3Y+K5l4kROrx7O2LESmjk5/JMBq0yIwLQ13nII42mXDBjASpQ5ItZf9LU1BxF9zGRegCulV1RVEm9td2Y8ueOtx5cY6s7+2JxtLZb8GpDuf8ooKMmCk/z6BVY9msRCyblYiHrs3HybZ+bKtowbaKVuyp6UJVWz+q2qqx6aNqGPUaXDwrAZfNTsKls5P8ekr5q38fxfbKNug1KvzptiKkx4T57WMFUn5qFOo6B3GECcu4mLCQbFQHsOB2pC/OT8Uv3jyME6192F/XjYVZEztSJ5KLA67TldyECESHa336bEEQMCMpEjOSIvHNS6bDNGTFR8faPYW7Hf0WvH2oGW8faoYgAPMznIW7l7sKd301nuD5T2vw3Kc1AID/W1mAgswYnzxXDuakRuGdwy0svD2HSZ3jPfXUU8jOzobBYEBxcTF27dp11tdu2rQJF198MWJjYxEbG4uSkpIzXn/77bdDEIRRv6666qrJhEYKJlXCYjRo8T+uhYh/40JEUiDPdVAAvolHGbS4Zn4qfrdiAXb/tARvfOcCfPfyGZibHgVRdCZP//feMVz75McoXleGH/39ALZ+3oS+Kezter+iFb948zAA4IfLZ3s+X4PF8E4hFt6Ox+sTli1btqC0tBQbN25EcXExNmzYgOXLl6OyshJJSWf2wG/fvh233HILLrjgAhgMBjz66KP4whe+gMOHDyM9Pd3zuquuugp//vOfPb/X61n8GGqkSlgAYGVRJl7f14A3DzThgS/mI1zHw0dSDnfBbeEk61cmS6USUJgVi8KsWJR+YTZaTEPYXtmKsqOt+PhEO9p6zXh1Tz1e3VMPrVpAcU48LnMV7k7087yi2YR7/7ofDhG4cVEGvnPpdD//rQLPvVPoRGsvLDYHdBrlFhH7k9dflR9//HHcddddWL16NQBg48aN+Pe//43Nmzfj/vvvP+P1f/nLX0b9/k9/+hNee+01lJWVYdWqVZ636/V6pKSkeBsOBZGqNmeHUKAKbkdakhOH7Phw1HQM4O1DzbhRxivpiUYSRdFzJTTRDiF/SY4yYOXiLKxcnAWzzY5d1Z3YVtGK9ytaUdMxgI9PtOPjE+14+K0jyEmIwGWuq6MlOXFjfpNu7R3CHc/tQZ/ZhuKcOPzmy/OCssYsIzYMRr0GvWYbTrb1eU5caDSv0jiLxYK9e/eipKRk+AEqFUpKSrBjx44JPWNgYABWqxVxcXGj3r59+3YkJSVh9uzZuPvuu9HR0XHWZ5jNZphMplG/SNlEUfQMjcuVYDz1qIWIvBYiBalu70fPoBV6jQp5qUapw/HQa9S4eGYiHrr2PGz/4WXY9r/L8LNr5uCC6fHQqARUt/dj8yfV+NqzO7Hw4Xfx7Rf34tXddWjtHQIADFntuOuFvWjoHkROQgT++PVFQXvyIAgC57FMgFcnLO3t7bDb7UhOHj1gKzk5GRUVFRN6xo9//GOkpaWNSnquuuoqfOUrX0FOTg5OnjyJn/zkJ7j66quxY8cOqNVnbtxct24dfvGLX3gTOslcR78FvUM2CAIwLV6a5Ww3LMzA7/5biV3Vnahu75fkaorIW+7roLnp0bLeC5SbGIncxEjceXEueoes+Pi4s3D3/co2tPeZsfVwM7YebgYAzEuPhkGrwoG6bsSEa7H59sUB7RyUwpxUI3bVdDJhGUdAL+ofeeQRvPLKK9i+fTsMhuFNvDfffLPnv+fNm4f58+dj+vTp2L59O6644ooznrN27VqUlpZ6fm8ymZCZmenf4Mmv3PUradFhkq2FT4k2YNmsRLxf2Ya/763DD5dzISLJ3/7abgCBKbj1FaNBi6vnpeLqealwOER83tiDsqOteL+yFQfre3CooQcAoFUL2Pi1RSHxwwMLb8/Nq4QlISEBarUaLS0to97e0tJyzvqT9evX45FHHsF7772H+fPnj/va3NxcJCQk4MSJE2MmLHq9nkW5QcY9kj83QBNuz2ZFUaYrYanH90u4EJHkrzyAHUL+oFIJmJ8Rg/kZMfj+lbPQ2juE7ZVt2FnViavmpuD83HipQwyIkTuFRFEMylqdqfLqq7FOp8OiRYtQVlbmeZvD4UBZWRmWLl161vd77LHH8PDDD2Pr1q0oKio658epr69HR0cHUlODq3WNzi6QSw/Hc8WcZMRF6NBiMuOj4+2SxkJ0LkNWu+cKQakJy+mSjAasKMrE71YswJUhtN9rdooRKsE5BLC11yx1OLLk9Y+PpaWl2LRpE55//nkcPXoUd999N/r7+z1dQ6tWrcLatWs9r3/00UfxwAMPYPPmzcjOzkZzczOam5vR1+fsCOnr68MPf/hDfPbZZ6ipqUFZWRmuu+46zJgxA8uXL/fRX5Pkzt0hJHXCotOocH2Bs92exbckd4cbe2BziEiI1CEjNjimvoYqg1btaTjgtdDYvE5YVq5cifXr1+PBBx9EQUEBysvLsXXrVk8hbm1tLZqamjyvf/rpp2GxWHDjjTciNTXV82v9+vUAALVajYMHD+JLX/oSZs2ahTvuuAOLFi3CRx99xGufEFItYYfQ6VYudtZDvXe0BR19/EmH5Gu4fiWWVwhBwFPH0siEZSyTKrpds2YN1qxZM+afbd++fdTva2pqxn1WWFgY3nnnncmEQUHC7hA9e1CkmMFyutkpRizIiMaB+h68sb8Bd17MhYgkT1INjCP/mJNqxJsH2Np8NqwoJMk1dg/CYndAp1YhTSbLzEbOZBFFUeJoiMam9IJbGo2zWMbHhIUk5y64nRYfDrWPFqVN1bUL0qDXqHCspQ8H63ukDofoDG29ZtR3DboWDkZLHQ75wHmuhKW6vR9DVrvE0cgPExaSXLVMCm5Hig4bXojI4luSI/fpyozESBgNvt3QTNJINOoRH6GDQwQqm7m5+XRMWEhynpZmiWewnO6mIuc+oX+VN2LQwp92SF7K67oA8DoomIwc0c9OoTMxYSHJuTuEpidI3yE00vk58ciMC0Ov2Yath5vO/Q5EAeSpX2HBbVCZ49oHxTqWMzFhIclVtcnzhEWlEnDTIlfx7e56iaMhGuZwiDhY56yt4glLcMlPY+Ht2TBhIUkNWe1o7BkEIK8aFrcbF2VAEIAdVR041dEvdThEAICTbX3oNdsQplVjdrJ8NjTT1I0c0e9wsENxJCYsJKlTHQMQRcBo0CA+Qn7bWNNiwnDxzEQAwN/38pSF5GG/6zpoXkY0910FmemJkdCpVegz21DfNSh1OLLC/9NJUtXtzg6h3IQI2U7qXOEqvv373nrY+RMPyYBnYByvg4KOVq3CjCSO6B8LExaS1Mk2eSw9HM+V+cmICdeiqWcIH5/gQkSS3vBI/hhJ4yD/4AC5sTFhIUnJaYfQ2eg16uGFiLs5k4WkNWCxobLZtaGZHUJBiYW3Y2PCQpJyJyxyPmEBgBWuUf3/PdKMzn6LxNFQKDtU3wOHCCRH6ZEaLY9VFuRbntbmZiYsIzFhIUkpJWHJT4vC3PQoWO0i/lneIHU4FMK4Pyj45buuhOo6B2EaskocjXwwYSHJdA9YPKcVck9YgOFTli27uRCRpDO8oTlW2kDIb2LCdUiNNgAAKpo4ot+NCQtJxj2SPzlKjwi9RuJozu26BenQaVSoaO7F5w08qiVp8IQlNOSz8PYMTFhIMtWuDqFcmY3kP5vocC2uOi8FABcikjRaTENo6hmCSgDmpXNDczBjp9CZmLCQZKpluvRwPO5roX+WN3D9OwWcu515VrJREaeSNHlMWM7EhIUk42lpVkD9itsF0+ORHhMG05AN7xxuljocCjH7XRuaC9nOHPTcnUIVzb2w2R0SRyMPTFhIMlUK6RAaSaUScJNr8i2vhSjQyjkwLmRMi49AmFYNs82BGu4xA8CEhSTicIioUWDCAgwvRPzkRAfqOgekDodChN0h4lCDe0MzO4SCnVolIM91ynKEnUIAmLCQRJpNQxi02qFRCciMC5c6HK9kxIbjwukJALgQkQLnWEsvBix2ROjUnl0zFNxYxzIaExaShLt+JSsuHFoFbpu9acRCRK6Ap0BwtzMvyIyBWiXPRaHkW+6E5UgjExaACQtJRIn1KyMtPy8FUQYNGroH8clJLkQk/2P9SujJd4/o5wkLACYsJJFqBWxpHo9Bq8b1ha6FiHt4LUT+x4FxoWd2ivOEpbXXjI4+s8TRSI8JC0miur0PgLJmsJzOPZPlncPN6B7gQkTynz6zDcdanYWX3NAcOiL1GmTHO2v8jrLwlgkLSUMpSw/Hc15aFOakRsFic+BfBxqlDoeC2MG6bogikB4ThiSjQepwKIBYeDuMCQsFnMXmQF3XIADljOUfiyAIWOkqvt2ymzNZyH/28zooZHkKb5mwMGGhwKvtHIDdISJcp0ZylF7qcKbkuoJ06NQqHG404XPXjAwiX2P9SujiCcswJiwUcCOvgwRB2e2ZsRE6XHleMgDOZCH/EEVxOGFh/UrIyU9zJiwnWvtgtoX2/jImLBRwnoJbBdevjOQuvn1jPxciku819gyhrdcMjUrA3DRuaA41adEGRBk0sDlEnGjtkzocSTFhoYBT4tLD8Vw0IwFp0Qb0DFrx7pEWqcOhIOOev5KXakSYTi1tMBRwgiBwgJwLExYKuCr3DBYFtzSPpFYJuHERFyKSf5S7NjSzfiV0DdexhHZrMxMWCrjhKbfK7RA63Y2LnNdCH59oR0P3oMTRUDAZLrjlwsNQlc/CWwBMWCjAeoesaOt1TmwMlhoWAMiKD8fS3HiIIvAai2/JR6x2Bw7Wuzc0x0gbDEnGXXh7tNkEUQzd3WVMWCigatoHAAAJkTpEh2kljsa3Vi52nrK8uqeOCxHJJyqbe2G2OWA0aIKm5ou8NyMpEmqVgO4BK5pNQ1KHIxkmLBRQVUHWITTSVXNTYDRoUN81iM+qOqQOh4LAyIFxKm5oDlkGrRrTXTV/oVx4y4SFAioYRvKfjUGrxpcWpAFg8S35Bjc0kxsHyDFhoQCrDsKC25HcM1n+83kzegatEkdDSufuECrkwLiQl89OISYsFFjulubcIGlpPt38jGjkpRhh5kJEmqKeQStOuj5fFmTESBsMSY4nLExYKIBEUQy6oXGnEwQBN7lOWf7GayGagoP13QCArLhwxEcqe+cWTZ07Yanu6MeAxSZxNNJgwkIB09ZnRp/ZBkFwtgEHq+sL0qBVCzhY3xPSPw3R1Oxn/QqNkGjUIyFSD1EEKppD81qICQsFTLXreDsjNgx6TfCOGI+P1KNkjnMh4t/2cCYLTQ43NNPp5qQaAYTutRATFgqYYC+4HWnFYvdCxPqQ37BK3uOGZhqLZ4AcExYi/wr2+pWRLpmZiJQoA7oGrCg72ip1OKQwdZ2D6Oy3QKsWPN0hRKHeKcSEhQLmZJB3CI2kVgm4YVE6AM5kIe/td7Uz56dGwaAN3utT8s7ITqFQnKbNhIUCpjqIp9yO5SbXQsQPj7WhqYcLEWni3NdBhVlceEjDchMioNOoMGCxo7ZzQOpwAo4JCwWEze7wfIKFSsKSnRCB4pw4OLgQkbzEglsai0atwuzk0C28ZcJCAdHQPQirXYROo0JadJjU4QSMe/Ltq3vqQ/IIl7xnttlx2LUvhgkLnS6UO4WYsFBAVLk7hOIjQmqJ29XzUhCp16C2cwC7ajqlDocU4GhTLyw2B2LDtZgWxPOKaHLcdSxHmLAQ+Yd7JH+oXAe5hes0uHZBKgAW39LElNc6C24XZMZAEEInuaeJmRPCnUJMWCgg3AW3odAhdDr3tdDbh5pgGuJCRBof61doPHNSnAlLQ/cgegZC6+sJExYKiOGhcaGXsBRkxmBmUiSGrA68daBJ6nBI5piw0Hiiw7VIj3HWAR5tDq1rISYsFBDVITSD5XSCIIwovuW1EJ1dV78FNR3ObjomLHQ2obq5mQkL+d2gxY7GniEAoTGWfyxfXpgOjUpAeV03jrWE3t0zTUy5a0NzbkIEYsJ10gZDspXv6hQ60siEhcinajqcpyvRYVrEhmsljkYaCZF6XDEnCQDw6m6estDYyrmhmSbAc8LCKyEi3xrZIRTKXQ/ua6E39jfAYnNIHA3JERce0kS4E5ZjLX2w2UPnawkTFvK7UO4QGmnZrEQkGvXo6LdgWwUXItJoozY084SFxpEVF44InRoWm8Mz4yoUMGEhv6sKoS3N49GoVbhhYQYAFt/Smarb+9EzaIVOo0JeCjc009mpVALy3APkQqiOhQkL+d1wS3NoFtyOtKLImbBsr2xFi2lI4mhITtynK3PToqDT8EszjS8UR/Tzs4L8LpRnsJwuNzESi7NjnQsR93EhIg0bvg7ihmY6t1Ac0c+Ehfyqq9+Cbtc0xuwE7kUBgJtcxbd/21MPUeRCRHJyJyyFLLilCcgPwRH9TFjIr6pcBbdp0QaE6zQSRyMP18xLRYROjer2frx1kJNvCRiy2j1H+yy4pYmYnWKEIADtfWa09obG9TITFvIrT0tziHcIjRSh1+CrxVkAgO9vKcfWz5sljoikdrjRBKtdREKkDhmxYVKHQwoQrtMgJ975dTVUTlmYsJBfsX5lbD++Kg/XFaTB5hBxz8v78PYhnrSEspHtzKE8q4i8E2oj+pmwkF+xQ2hsGrUKj68owFcK02F3iLj3r/vx5oFGqcMiieyv7QLA6yDyTqh1CjFhIb+q5gyWs1KrBPz2pgW4cVEG7A4R33tlP/5Z3iB1WCQBdgjRZOSn8YSFyCccDpFXQuegVgl47Ib5WFmUCYforGl5Yz/bnUNJe58Z9V2DEARgfma01OGQgrivhE629WPIapc4Gv+bVMLy1FNPITs7GwaDAcXFxdi1a9dZX7tp0yZcfPHFiI2NRWxsLEpKSs54vSiKePDBB5GamoqwsDCUlJTg+PHjkwmNZKSxZxBmmwNatcBCwnGoVALWfWUeblmSBYcIlL56AH/fy6QlVLgXHk5PjESUITSXg9LkpEQZEBOuhd0h4nhLn9Th+J3XCcuWLVtQWlqKhx56CPv27cOCBQuwfPlytLaOvRtl+/btuOWWW/D+++9jx44dyMzMxBe+8AU0NAwffT/22GN44oknsHHjRuzcuRMRERFYvnw5hoZCo1UrWLlPV7LiwqFR8zBvPCqVgF9fPxdfOz8Logj88O8HuNU5RHjmr7B+hbwkCALmpITOtZDX30Uef/xx3HXXXVi9ejXy8/OxceNGhIeHY/PmzWO+/i9/+Qu+853voKCgAHl5efjTn/4Eh8OBsrIyAM7TlQ0bNuBnP/sZrrvuOsyfPx8vvPACGhsb8Y9//GPMZ5rNZphMplG/SH5YcOsdlUrAw9fNxW1Lp0EUgR+9dhB/3VUrdVjkZ9zQTFPhrmMJhYm3XiUsFosFe/fuRUlJyfADVCqUlJRgx44dE3rGwMAArFYr4uLiAADV1dVobm4e9czo6GgUFxef9Znr1q1DdHS051dmZqY3fw0KEPcMllDf0uwNQRDw8y+dh9UXZgMA1r5+CC99dkraoMhvHA4RB7ihmaYglFqbvUpY2tvbYbfbkZycPOrtycnJaG6e2PCrH//4x0hLS/MkKO738+aZa9euRU9Pj+dXXR2PzuWIBbeTIwgCHvxiPu68KAcA8LN/fI4XdtRIGxT5xcm2PvSabQjTqjE72Sh1OKRA7tbmI02moF/1EdBZ6Y888gheeeUVbN++HQaDYdLP0ev10Ov1PoyM/IEJy+QJgoCfXjMHapWAP35YhQf/eRh2h4jVF+ZIHRr50H7X6cq89GjWedGkzEiKhEYloHfIhobuQWTEBu/ONq8+QxISEqBWq9HS0jLq7S0tLUhJSRn3fdevX49HHnkE//3vfzF//nzP293vN5lnknyZbXbUdw0A4JXQZAmCgPuvzsN3Lp0OAPjFm0fwp4+qJI6KfIn1KzRVeo0aM5KcdYLBPqLfq4RFp9Nh0aJFnoJZAJ4C2qVLl571/R577DE8/PDD2Lp1K4qKikb9WU5ODlJSUkY902QyYefOneM+k+SttmMADhGI1GuQGMnTsMkSBAE/XD4b914+AwDwq38fxTMfnpQ4KvIVd0sz61doKvJDpI7F6yuh0tJS3HbbbSgqKsKSJUuwYcMG9Pf3Y/Xq1QCAVatWIT09HevWrQMAPProo3jwwQfx8ssvIzs721OXEhkZicjISAiCgPvuuw+/+tWvMHPmTOTk5OCBBx5AWloarr/+et/9TSmgqkZcB3E3ytQIgoDSK2dBJQj4f2XH8Zu3K2BziPjOpTOkDo2mYNBiR2WL8ydiJiw0FXNSo4D9DUxYTrdy5Uq0tbXhwQcfRHNzMwoKCrB161ZP0WxtbS1UquGDm6effhoWiwU33njjqOc89NBD+PnPfw4A+NGPfoT+/n5885vfRHd3Ny666CJs3bp1SnUuJC3Wr/iWIAj4/pWzoFYJePzdY3hsayUcDhFrLp8pdWg0SYcaemB3iEiO0iM1ml/raPLcnULB3to8qaLbNWvWYM2aNWP+2fbt20f9vqam5pzPEwQBv/zlL/HLX/5yMuGQDFW3MWHxh+9eMdO5g+idSqz/7zHYHcD3Spi0KFF53fDCQ55C0lS4O4VOdQygz2xDpD6g/TQBw7J08gvP0kMW3PrcPZfNwI+vygMA/N97x/D4u8eCvp0xGHHhIflKfKQeyVHOWsHK5uA9ZWHCQn5R1e7ca5HLKbd+cfel0/HT/5kDAHii7Dh+918mLUqznwW35EPD10LB2ynEhIV8rmfQivY+CwAgOyF4ZwJI7a5LcvGza5xJy5Pvn8Bj71QyaVGIFtMQmnqGoBKA+Rnc0ExT50lYGnnCQjRhNa7roESjHkZun/WrOy/OxUPX5gMAnt5+Euv+U8GkRQHcpyuzko2ICNJ6AwqsUBjRz4SFfI4dQoG1+sIc/PK68wAAz3xYhV/9+yiTFpkr5/4g8rF8V+FtZXMv7I7g/PxnwkI+557BksuEJWBWLc3Gr788FwDw7MfV+MWbR5i0yJi7Q6iQE27JR3ISImHQqjBoteNUR7/U4fgFExbyuao2V8EtO4QC6tbiaVj3lXkAgOc+rcFD/zrMpEWG7A4Rh+p7ALBDiHxHrRI8CzSDdUQ/ExbyueErIXYIBdotS7Lw2A3zIQjACztO4Wf/+ByOID0eVqrjrb3ot9gRoRveAUPkC8OdQj0SR+IfTFjIp0RRZA2LxFYszsRvb1wAQQD+srMWP/3HISYtMuLeHzQ/IwZqFQfGke8MF97yhIXonFp7zRiw2KESgKw4tjRL5cZFGXh8xQKoBOCvu+pw/+sHmbTIhGf+CutXyMfy04K7U4gJC/lUlWskf2ZcOHQa/u8lpS8XZuD/VhZAJQCv7qnHD/9+MGi7B5SEHULkL3kpzhqWpp4hdPVbJI7G9/gdhXyK10Hycl1BOp64pRBqlYDX9tXjB387wKRFQn1mG461Oo/rC5mwkI8ZDVpkxoUBCM5TFiYs5FOeDiEW3MrGF+en4fe3FEKjEvDG/gZ8f0s5bHaH1GGFpIP13RBFIC3agKQobmgm35uTErybm5mwkE95TljY0iwr/zMvFU9+dSE0KgH/OtCI+5i0SMJ9HVSYxXZm8o9gLrxlwkI+Vc2hcbJ11dwUPP21RdCqBbx1sAnffWU/rExaAqqcCw/Jz4K58JYJC/mM1e5AbecAANawyNWV+cnY+LVF0KlVePtQM9a8vA8WG5OWQBBFcbjglh1C5Cf5rhOWE619Qfe5zYSFfKa+axA2hwiDVoUU3s/L1hVzkvHHry+CTqPCO4dbcA+TloBo6hlCa68ZapWAuWnc0Ez+kREbBqNeA4vdgZOumsJgwYSFfKa63fnJkR0fARUHYsnaZXlJ2LSqCHqNCu8eacHdL+2F2WaXOqyg5p6/kpdiRJhOLW0wFLQEQUBeqntEf3BdCzFhIZ9xz2CZnsgOISVYNisRz962GHqNCmUVrfj2i3sxZGXS4i/uhYesXyF/Gy68ZcJCNKYqzmBRnItmJuDPty+GQavC+5Vt+CaTFr/hwDgKlPwg7RRiwkI+U93GhEWJLpiRgOdWL0GYVo0Pj7Xhrhf2YNDCpMWXrHYHDjU4F9IVsuCW/Gx4CaIpqDa2M2Ehn+EMFuU6Pzcez39jCcJ1anx0vB13PL+bSYsPVTb3YsjqgNGg4VBF8rvZKUaoBKCz34LWXrPU4fgMExbyiX6zDc2mIQCcwaJUS3Li8MI3liBCp8anJzuw+rldGLDYpA4rKIy8DmJBOvmbQav2nHQH08RbJizj6B2y4uWdtfjlm0ekDkX2ajqcpyux4VrEhOskjoYmqyg7Di/cUYxIvQafVXXi9s270W9m0jJVrF+hQMt3tc4HU+EtE5ZxDFkd+Ok/DmHzJ9VocZ0e0NjcHUK57BBSvEXTYvHiHUtg1Guwq6YTt23ehT4mLVOyv5YdQhRYczytzcFTeMuEZRyJRj0WZMQAAMqOtkobjMxxS3NwKcyKxUt3FiPKoMGeU11Y9exOmIasUoelSD2DVpx0JfRMWChQPIW3jT0SR+I7TFjO4cr8ZADAe0dbJI5E3piwBJ8FmTF4+a7zER2mxb7abqx6dheTlkk4WN8NAMiMC0N8pF7aYChkuFubq9v7g2ZUAROWcyiZ40xYPjnRzgLEcVRx6WFQmpsejb/cWYyYcC3K67rx9T/tRM8AkxZvDC885IZmCpwkox5xETo4RGeXWjBgwnIOs5IjkRkXBrPNgY+Pt0sdjiyJoohq184KtjQHn7np0Xj5zvMRF6HDgfoe3PrsZ+gesEgdlmKw4JakIAjCiAFywVF4y4TlHARBwBV5vBYaT2e/BaYhGwTBuUeIgk9+WhT+etf5iI/Q4fMGE766aSe6+pm0nMvIDc0cGEeBNifIdgoxYZkAdx3LtopWOBzBMzXQV9zXQWnRYTBoudQtWM1OMeKv3zwfCZE6HGky4ZZNn6GjL3iGUvlDfdcgOvot0KqHf9olCpSRE2+DAROWCViSEwejQYP2PgvKXQV0NKza09LM05VgNyvZiFe+eT4SjXpUNPfiq5t2op1Jy1ntd52u5KdGMZmngHMnLBVNvUExop8JywRo1SpcOjsJAPDeEV4LnY5LD0PLjCRn0pJk1KOypRe3PMOTlrPh/BWS0vTESOjUKvSabajvGpQ6nCljwjJBJXNcCQvrWM5Q3e4quGXCEjKmJ0Ziy7eWIiXKgOOtfVj/30qpQ5IlT8Et61dIAjqNCjOSnMM8g+FaiAnLBF06KwlqlYBjLX2o7RiQOhxZ4QyW0JSTEIHff7UQAPD3vfVo6Fb+T3C+ZLE5cLjR+U2CLc0kleEBckxYQkZ0uBZLsuMA8JRlJLtDRI0rgZvOsfwhZ3F2HC6YHg+rXcQf3j8hdTiycrTJBIvNgZhwLbLjw6UOh0JUMHUKMWHxQgmn3p6hsXsQFpsDOrUKaTFhUodDEvjeFTMBAK/uqUMjT1k83NdBCzJiIAjc0EzS8MxiaWbCElLcdSw7qzs57dPFXXA7LT4cahW/KIei4tx4nJ8bB6tdxNPbT0odjmxw/grJgftKqK5zEL0KX63BhMUL0+IjMDMpEnaHiO3HuAwRwPCEW9avhLTvuk5ZtuyuQ1MPT1kATrgleYiN0CE12gAAqFD4iH4mLF5yXwtxe7OTp+CWM1hC2tLceCzJjoPF7sAfP6iSOhzJdfVbPJ8bTFhIasFSeMuExUvuZYjvV7bCandIHI30uPSQAOcKi++VOE9ZXt5VixbTkMQRScs9YDInIQIx4Tppg6GQFyyFt0xYvFSQGYOESB16h2zYXd0pdTiSq/JMuWWHUKi7YHo8iqbFwmJzYOMHoV3LMryhOUbSOIgAID81GgATlpCjVgm4zDX19t0Q7xYastrR6KpXYA0LjTpl2VmL1hA+ZWH9CsmJ+4SlorkXNgXfDDBhmYSR7c3BsJ9hsk51DEAUAaNBg/gIHnsTcNGMBCzMioHZ5sAfPwzNWhZRFHHAdSXEhIXkYFp8BMK0aphtDtR09EsdzqQxYZmEi2cmQKdRoa5zEMdb+6QORzLukfy5CRGcM0EAnKcs7o6hv+w8hbbe0NsxVNMxgO4BK3QalafYkUhKapWA2SnOU5YjTcrtFGLCMgnhOg0umpEAAHg3hJchcukhjWXZrEQsyIzBkNWBTR+F3ilLeZ1z4eHctCjoNPwSS/LgTp6VXMfCz6ZJuoLLEFHd5k5YWHBLwwRBwH2uU5YXd5xCe4htch4uuOX+IJKP/DQmLCHrijxnHUt5XXdIHnsDI1qaOYOFTnPp7ETMz4jGoNUecqcs3NBMcpQfBK3NTFgmKSXagPkZ0RBF4P2K0Bwixy3NdDaCIHh2DL244xQ6+y0SRxQYQ1Y7jri+IRSy4JZkZHaK84SlxWRGh0JPPZmwTIF7iFwotjd3D1g834SYsNBYLs9Lwtz0KAxYQueU5XCjCVa7iPgIHTJiuQyU5CNSr8E019bwowotvGXCMgXuOpaPjrdhyGqXOJrAcp+uJEfpEaHXSBwNyZEgCPju5c5Tlhc+rUFXCJyyjJy/ws45kps5KcquY2HCMgX5qVFIizZgyOrApyfbpQ4noHgdRBNxZX4y8lOj0G+x49mPq6UOx+84MI7kTOmFt0xYpkAQBM8QuXePhFYdS3U7R/LTuY2cy/LcpzXoHgjuUxZ3S3NhFjuESH48SxCZsIQmdx1L2dEWOByhM/XWs0OIJyx0Dl/IT0ZeihF9Zhs2B/EpS0efGXWdgxAEYH5mtNThEJ3BPaL/RGsfzDbllTEwYZmi4tw4ROjUaO0141BDj9ThBAyHxtFEqVTDHUN//qQGPQNWiSPyD/d10PTESEQZtNIGQzSG9JgwRBk0sDlEnFDglHYmLFOk16ixbHYigNAZIudwiKhhwkJeWH5eCmYnG9FrtmHzJ8F5yrKfG5pJ5gRBGDHxVnmdQkxYfMB9LfTe0dCoY2npHcKg1Q61SkBmXLjU4ZACqFTDtSybP6lGz2DwnbKw4JaUQMkj+pmw+MBls5OgEpz/A9R3DUgdjt+5R/JnxYVDq+b/QjQxV89NwcykSPQO2fD8pzVSh+NTDoeIA0xYSAHy3YW3jUxYQlJshA5F0+IAAGUhcMriGcnP6yDygkol4F7XKcuzH1ejdyh4Tlmq2vvQa7bBoFUhz7UVl0iOPCcszSaIorIaRZiw+EhJfugsQ6xqY/0KTc4181IxPTECPYPWoDplcdevzEuPhoanjiRjM5MjoVYJ6B6wotk0JHU4XuFnlo+461g+q+oIqp8cx1Ld7qwuz+HSQ/KSekQty58+rkaf2SZxRL7hrl/h/BWSO4NWjemur91Kq2NhwuIjuYmRyE2MgNUu4sNjwT31llNuaSq+OD8NuYkR6B4InlMWFtySksxRaB0LExYfGu4WCt5rIYvNgbquQQBAbgKn3JL31CoB914+AwDwp4+q0K/wU5ZBix0Vzc4WUSYspARKbW1mwuJD7oRlW0UrbHaHxNH4R13XAOwOEeE6NZKj9FKHQwp17fw0ZMeHo2vAihc/OyV1OFNyqKEHdoeIJKMeqdEGqcMhOieltjYzYfGhhVkxiA3XomfQir2nuqQOxy+qRxTcchstTZZGrcIa1ybnTR9WYcCi3FMW9/4gbmgmpXC3Nld39Cvqc48Jiw9p1Cpclhfc3UJV7oJb1q/QFF1fkIZp8eHo6LfgJQWfsnjqV7JiJI2DaKISjXokROohikBls3KuhSaVsDz11FPIzs6GwWBAcXExdu3addbXHj58GDfccAOys7MhCAI2bNhwxmt+/vOfQxCEUb/y8vImE5rk3NdC7x5pUVyP+0RUcwYL+YhGrcI9lzlrWZ75sAqDFuUtYwOAco7kJwVyL0JU0uZmrxOWLVu2oLS0FA899BD27duHBQsWYPny5WhtHXtg2sDAAHJzc/HII48gJSXlrM8977zz0NTU5Pn18ccfexuaLFwyKxE6tQo1HQM46bo+CSaeGSxsaSYf+HJhOjLjwtDeZ8FfdirvlKXVNITGniHnhuaMGKnDIZqwfAXWsXidsDz++OO46667sHr1auTn52Pjxo0IDw/H5s2bx3z94sWL8dvf/hY333wz9PqzF2lqNBqkpKR4fiUkJHgbmixE6jU4f3o8AKAsCK+Fhlua2SFEU6dVq7DGdcqy8QPlnbLsd10HzU42IlKvkTYYIi8osVPIq4TFYrFg7969KCkpGX6ASoWSkhLs2LFjSoEcP34caWlpyM3Nxa233ora2tqzvtZsNsNkMo36JSdXzgnOOpY+sw2tvWYArGEh3/lyYQbSY8LQ3mfGX3ed/fNejjh/hZQqP82ZsFQ0meBwKKN8wauEpb29HXa7HcnJyaPenpycjObm5kkHUVxcjOeeew5bt27F008/jerqalx88cXo7R0781u3bh2io6M9vzIzMyf9sf3hClcdy95TXejoM0scje/UuE5XEiJ1iA7TShwNBQudZriWZeMHJzFkVc4pC+tXSKlyEyKg06jQb7GjTiFLe2XRJXT11Vfjpptuwvz587F8+XK8/fbb6O7uxquvvjrm69euXYuenh7Pr7q6ugBHPL60mDDkp0bBIQLvV7ZJHY7PnGxjhxD5x42LnKcsrb1mvKKQUxa7Q8TB+m4A7BAi5dGoVZiV7LzaV8rEW68SloSEBKjVarS0jL7qaGlpGbeg1lsxMTGYNWsWTpw4Meaf6/V6REVFjfolNyX5zlOWYKpj4Uh+8hedRoW7L50OAHhaIacsx1t70W+xI0Knxswkbmgm5ZmToqzCW68SFp1Oh0WLFqGsrMzzNofDgbKyMixdutRnQfX19eHkyZNITU312TMD7UrXtdAHx9oU8cV3IlhwS/50U1EGUqMNaDGZ8eoeeZ2ajsV9HTQvIxpqFQfGkfK461iOKKTw1usrodLSUmzatAnPP/88jh49irvvvhv9/f1YvXo1AGDVqlVYu3at5/UWiwXl5eUoLy+HxWJBQ0MDysvLR52e/OAHP8AHH3yAmpoafPrpp/jyl78MtVqNW265xQd/RWnMTY9CcpQeAxY7PqvqkDocn+AJC/mTXqPGd9ynLNtPwmyTd6I/XHDLDc2kTEob0e91wrJy5UqsX78eDz74IAoKClBeXo6tW7d6CnFra2vR1NTkeX1jYyMKCwtRWFiIpqYmrF+/HoWFhbjzzjs9r6mvr8ctt9yC2bNnY8WKFYiPj8dnn32GxMREH/wVpSEIgqf4Nhi6hURR9Izln84ZLOQnNxVlIjlKj6aeIfxtT73U4YyLHUKkdO4roYbuQfQMWCWO5twmNThgzZo1WLNmzZh/tn379lG/z87OPufE11deeWUyYcjelXOS8fLOWpQdbcXD14mK3jPS1mdGr9kGQQCy4sOlDoeClEGrxt3LpuPnbx7B09tPYkVRJnQaWfQGjNJvtuFYi/MYvZAFt6RQ0eFapMeEoaF7EEebTTg/N17qkMYlv68EQWTp9HiEadVo6hnCYYVUYZ+N+3QlIzYMeo1a4mgomN28JAtJRj0augfx973yPGU5WN8DhwikRRuQHMUNzaRc7hH9SrgWYsLiRwatGpfMck7sVfq1EAtuKVAMWjW+vcxZy/LU+ydgsTkkjuhMXHhIwUJJI/qZsPhZsNSxcOkhBdJXi7OQ6DpleX2f/E5Z9td2AWD9Cimfkkb0M2Hxs8vzkiAIwOcNJjT1DEodzqRVsUOIAsigVeNbl+QCAJ58/wSsdvmcsoiiyA4hChruhKWypRc2GX2ejYUJi58lROqxMMv5Ra3s6NgbrZXAc8LCDiEKkFuLpyEhUof6rkG8sa9B6nA8mnqG0NprhlolYF56tNThEE1JVlw4InRqWGwOzw+mcsWEJQBKFH4tZLM7cKqDJywUWGE6Nb454pRFLj/9lY/Y0BymYwE6KZtKJWB2ijIKb5mwBECJa3vzpyc60G+2SRyN9xq6B2G1i9BpVEiLDpM6HAohXzt/GuIjdKjtHMA/yhulDgfAcMLCdmYKFsMTb5mwhLwZSZGYFh8Oi92Bj463Sx2O1zz1K/ERUHEEOQVQuE6Du9ynLNuOy+KUhRuaKdi461jkvgSRCUsACIKg6Gsh9wwWXgeRFL5+/jTERehQ0zGAfx2Q9pTFZnfgYEM3AJ6wUPBQSqcQE5YAcScs2ypaYXeMP/lXbjwzWFhwSxKI0Gtw58U5AIAnt52Q9POnorkXQ1YHjAYNcjmTiIJEXooRggC095nR1muWOpyzYsISIEXZsYgyaNDZb/HMcFAKzmAhqa1amo2YcC2q2vvxpoSnLO76lQUZMbwepaARrtMgJ9759V3OhbdMWAJEq1bhsjxn8e17CmtvrmrrA8CWZpJOpF6DOy9ynrL8fttxyU5ZuPCQgpUSNjczYQkgJdaxDFrsaOwZAsCx/CSt2y7IRnSYFifb+vHvQ03nfgc/YMJCwcq9U0jOnUJMWAJo2exEaFQCTrT2ea5Z5K7GNX8lOkyL2HCtxNFQKDMatLjDfcpSdhyOAJ+ymIasOOk6beQOIQo2PGGhUaIMWhTnxgEAyhRyylI9YiS/IPDOnqR1+4XZiDJocLy1D29/HthTloN1PRBFIDMuDAmR+oB+bCJ/cycsJ9v6MWS1SxzN2JiwBJj7WujdI8pKWFhwS3IQZdDiG65TlicCfMpSXudeeMj9QRR8UqMNiAnXwu4QcaK1T+pwxsSEJcDcCcueU13oHrBIHM25VbVxhxDJy+oLcmDUa3CspQ9bDzcH7OOyfoWCmSAImJMi7wFyTFgCLDMuHHkpRtgdIrZXtkkdzjlVtTszbRbcklxEh2ux+sJsAIE7ZRFFEfs54ZaCnGfirUzrWJiwSOAK126hdxVQxzKyhoVILr5xUQ4i9RpUNPfivwG4Xq3vGkRHvwVatYDzXHtXiIKNu1NIroW3TFgk4L4W+qCyDRab9LtRzqar34LuASsAIDshXOJoiIbFhOtw+wXZAJynLKLo31OW/a7roDmpUTBouaGZgpN7CeLRJpPfP6cmgwmLBBZkxCAhUo8+sw27qjulDues3EsPU6MNCNdpJI6GaLQ7LspBhE6NI00mvxexc+EhhYIZSZHQqASYhmye+VtywoRFAiqVgJI57qm38r0W8nQIseCWZCg2QofbXKcs/8/PpyzDHUIxfvsYRFLTa9SYkeSsV5Rj4S0TFolcMaK9WY5HbwBQ7Sm4ZcJC8nTnxbkI16lxuNGEMj+tvLDYHPjc9cW7MIstzRTc5DxAjgmLRC6akQC9RoWG7kFUNMtzpbe7pZkdQiRXcRE6fH3pNADAE9v8c8pS0WyCxeZATLgW2fGs5aLgJufCWyYsEgnTqXHxzAQA8p16y6FxpATfvDgXYVo1Dtb3+GVUwMgNzZz2TMEuPzUaABMWOo1n6q0Mtzc7HCJbmkkR4iP1nlOWDX6oZeH8FQol7hOWmo4B9JltEkczGhMWCV2e5yy8PVDXjVaTvCqym0xDMNsc0KoFZMSGSR0O0bjuujgXBq0KB+q68cEx356yeCbccuEhhYD4SD2SjM5dWZXN8jplYcIioaQoAxa4fmorq5DXKUu1q34lKy4cGjX/NyF5SzTq8bVi5ymLLzuGugcsnpPGgowYnzyTSO6GJ97Kq76S34kkdqWrvVludSzVHMlPCvPNZbnQa1TYX9uNj463++SZ7tOV7PhwxEbofPJMIrkbOUBOTpiwSKwk31nH8tHxdgxa5LPS+ySXHpLCJBkN+GpxFgDfnbJw4SGFIrm2NjNhkdjsZCPSY8Jgtjnw8Qnf/FToCyy4JSX69rLp0GlU2HuqC5+e7Jjy89wJC+evUCjJdxXeVjT1wh6A5aITxYRFYoIg4ErXKct7AVjiNlFMWEiJkqMM+OoS1ynLe1M7ZRFFEQd4wkIhKDs+AnqNCoNWO0519EsdjgcTFhlwtzeXVbTCIYNs1myzo75rAABnsJDyfHvZdOjUKuyq6cSOqsmfspzqGEDXgBU6jcpzRE4UCjRqFWanuAfIyafwlgmLDCzJiYNRr0F7nxkH6rulDgd1nQNwiECkXoNEV3sbkVKkRBtw85JMAM5Tlsna79ofdF5aFHQafqmk0JIvwzoWfhbKgE6jwrLZiQDksQzxZNvwdRAne5ISfXvZdGjVAnZWd+KzSZ6ycEMzhbLh1mYmLHQa97XQe0ekn8fC+hVSurSYMKwocp6yPFE2uVMWdghRKJNjpxATFpm4dHYi1CoBlS29qOsckDSW6jYmLKR837lsBrRqAZ+e7MDumk6v3nfIavf8ZFmYyQ4hCj15rk6hpp4hdA9YJI7GiQmLTMSE67A42/mFUeprIc/SQ85gIQVLjwnDjYsmV8typMkEq11EXIQOmXFcTUGhJ8qg9axlkcu1EBMWGfFcC0mcsFTxSoiCxHcunQ6NSsDHJ9qx99TET1nc9SuFmdzQTKFruPBWHp1CTFhkxJ2w7KzqhGnIKkkMpiEr2vvMAJiwkPJlxoXjxkUZAIANXpyysH6FaEThbSNPWOg02QkRmJEUCZtDxAeVvt04O1Hu+pVEox5Gg1aSGIh86Z7LZkCtEvDR8Xbsq+2a0PtwQzOR/ApvmbDIjNTXQuwQomCTGReOrxSmA5hYx1BHnxm1rsL3+dzQTCHMfSV0orUPVrtD4miYsMjOlfnO7c3vV7RK8j+Iu36FE24pmKy53HnKsr2yzXN6cjbuP5+eGIHoMJ4yUujKiA2DUa+Bxe7AybY+qcNhwiI3BZmxiIvQwTRk87oV0xd4wkLBaFp8BK4vmNgpy3D9CtuZKbSpVIKnvVkO10JMWGRGrRJweZ7zlKXsaOCHyFW3O7Po3MTIgH9sIn9ac/kMqARgW0UrDo6zAoP1K0TD5FR4y4RFhkbWsUxl26y3RFHk0DgKWjkJ5z5lcThET8JSyA4hohGFt9K3NjNhkaGLZyZAp1HhVMcATrQG7t6wtdeMfosdKgHIigsP2MclCpR7XKcs7x1txecNPWf8eVV7P3qHbDBoVchzbaslCmUjO4UC+QP0WJiwyFCEXoMLpscDAN4NYLdQlet0JTMunNtpKShNT4zEtQvSAAD/b4xTFvfpyrz0aGjU/Bwgmp1shEoAOvotaOs1SxoLPyNlangZYuASFhbcUii49/IZEATg3SMtONw4+pRlv2tOCwfGETmF6dSe7wmHJS68ZcIiU1fMcRbe7q/r9kye9Td3wS0TFgpmM5KM+OJ85ynL78tOjPozdggRnUkuA+SYsMhUanQY5qVHQxSdXQ2BMLz0kB1CFNy+6zpl2Xq42fNFeNBiR0Wzs7CQHUJEw+RSeMuERcbcpyyBuhbi0DgKFTOTjfifeakAgN9vc9ayfN7YA7tDRKJRj7Rog5ThEclKfhpPWOgc3HUsHx1vx5DV7tePZbU7UNvhHEfOKyEKBd+9fCYA4O1Dzahs7vVsaC7ghmaiUdwj+qva+vz+vWg8TFhk7Ly0KKRGGzBotWPHyQ6/fqz6rkHYHCIMWhVSovjTJQW/2SlGXD03BQDwxLbjw/NXeB1ENEqSUY+fX5uPF+8ohkrCZJ4Ji4wJguA5ZfF3e7O74DY7PgIqFX+6pNDw3SvcpyxN+PC4c0M6O4SIRhMEAbdfmIMLZyRIOvKCCYvMuetYyo62wOHw39Ae9wyW3EReB1HomJMaheXnJUMUgd4hGwSBG5qJ5IoJi8wtnR6PCJ0aLSYzPm88czKnr3g6hBLYIUShxX3KAgCzkoyI1GskjIaIzoYJi8zpNWpcMisRgHOcuL9waByFqvPSonFlvvPqdeG0GGmDIaKzYsKiAIGYeuu+EsrhlRCFoF9/eS6+vWw67r185rlfTESSYMKiAJflJUElAEeaTGjoHvT58/vNNjSbhgBwBguFpiSjAfdfnYe0mDCpQyGis2DCogBxETosmuYcFV7mh26hmg7n6UpsuBYx4TqfP5+IiGiqmLAohOdayA91LKxfISIiuWPCohAlrqLAHSfb0Ttk9emzq9u4Q4iIiOSNCYtCTE+MRE5CBKx2ER8db/fps3nCQkREcseERUFK/LQM8SSXHhIRkcwxYVEQdx3L+5WtsNkdPnmmKIqobnOO5WdLMxERydWkEpannnoK2dnZMBgMKC4uxq5du8762sOHD+OGG25AdnY2BEHAhg0bpvzMULVoWixiwrXoGrBin2uz7FR19ltgco0kz45nwkJERPLkdcKyZcsWlJaW4qGHHsK+ffuwYMECLF++HK2tY3evDAwMIDc3F4888ghSUlJ88sxQpVGrcNls17WQj9qb3fUradFhMGjVPnkmERGRr3mdsDz++OO46667sHr1auTn52Pjxo0IDw/H5s2bx3z94sWL8dvf/hY333wz9Hq9T54Zynw99baqnUsPiYhI/rxKWCwWC/bu3YuSkpLhB6hUKCkpwY4dOyYVwGSeaTabYTKZRv0KFZfMSoBWLaCqvR8nXbUnU+EZyc+CWyIikjGvEpb29nbY7XYkJyePentycjKam5snFcBknrlu3TpER0d7fmVmZk7qYyuR0aDF+bnxAHwz9ba63VVwy4SFiIhkTJFdQmvXrkVPT4/nV11dndQhBdTwtdDUa3w4g4WIiJTAq4QlISEBarUaLS2jf7JvaWk5a0GtP56p1+sRFRU16lcoucI1j2XPqU509Vsm/Ry7Q0RNxwAAIDeBU26JiEi+vEpYdDodFi1ahLKyMs/bHA4HysrKsHTp0kkF4I9nBruM2HDMSY2CQ3TOZJmsxu5BWGwO6NQqpMdySy0REcmX11dCpaWl2LRpE55//nkcPXoUd999N/r7+7F69WoAwKpVq7B27VrP6y0WC8rLy1FeXg6LxYKGhgaUl5fjxIkTE34mnenKOVNvb3ZfB02LD4daJfgkLiIiIn/QePsOK1euRFtbGx588EE0NzejoKAAW7du9RTN1tbWQqUazoMaGxtRWFjo+f369euxfv16LFu2DNu3b5/QM+lMJfnJeGLbCXxQ2QazzQ69xvsZKlVtLLglIiJlEERRFKUOYqpMJhOio6PR09MTMvUsDoeI89eVobXXjOe/sQTLZiV6/YyH/vk5nt9xCt9alou1V8/xQ5RERERn5833b0V2CRGgUgm4wtUtNNn25iouPSQiIoVgwqJgV+YPb2+ezEHZcEszO4SIiEjemLAo2AXTExCmVaOxZwhHmryb9jtktaOhexAAx/ITEZH8MWFRMINWjYtmJgDwfohcbecARBEwGjSIj9D5IzwiIiKfYcKicFe661gqvKtjcXcI5SZEQBDY0kxERPLGhEXhLstLgiAAB+t70NwzNOH3q+JIfiIiUhAmLAqXaNSjMDMGgHenLNVtLLglIiLlYMISBK7wLEP0ImFxn7Cw4JaIiBSACUsQuDLfmbB8crIDAxbbhN6nmjNYiIhIQZiwBIGZSZHIiguHxebAR8fbz/n6ngErOlxbnlnDQkRESsCEJQgIgoASL66FqtqdHULJUXpE6L1eJ0VERBRwTFiCRIlre/O2ilbYHeNPva1mhxARESkME5YgsTgnDkaDBh39FpTXdY37Wo7kJyIipWHCEiS0ahUum+3aLXR0/Km37hks09khRERECsGEJYiU5E+sjmV4BgsTFiIiUgYmLEFk2axEaFQCjrf2ocZ1inI6URRZw0JERIrDhCWIRIdpsSQnDgDw3tGxT1maTUMYtNqhVgnIjAsPZHhERESTxoQlyLjbm8vOUsfivg7KiguHVs1/fiIiUgZ+xwoy7oRlV00negasZ/w5lx4SEZESMWEJMlnx4ZiVHAm7Q8T2Y2eesnAkPxERKRETliDkPmV5d4xuIS49JCIiJWLCEoTc7c0fHGuDxeYY9WfsECIiIiViwhKECjJikBCpQ++QDbtrOj1vt9gcqO0cAADkcsotEREpCBOWIKRSCbg8zzn1duS1UF3XAOwOEWFaNZKj9FKFR0RE5DUmLEHKs735aAtE0bkMceSEW0EQJIuNiIjIW0xYgtRFMxOg16hQ3zWIYy19AEZ0CLHgloiIFIYJS5AK12lw0YwEAMNTb6vY0kxERArFhCWIXXFae3NVm/OkhS3NRESkNExYgtgVc5yFt+V13WjtHRrR0swOISIiUhYmLEEsOcqABRnRAIA3DzShtdcMAMiJ5wkLEREpCxOWIOfuFtr8cTUAID5Ch+hwrZQhEREReY0JS5Bz17E0dA8CYIcQEREpExOWIDcn1Yj0mDDP7zmSn4iIlIgJS5ATBAElruJbgAW3RESkTExYQoB7GSLAExYiIlImJiwhoDgnHtFhzkLbvBSjxNEQERF5TyN1AOR/Oo0Kz39jCVpNQ8jmCQsRESkQE5YQUZAZI3UIREREk8YrISIiIpI9JixEREQke0xYiIiISPaYsBAREZHsMWEhIiIi2WPCQkRERLLHhIWIiIhkjwkLERERyR4TFiIiIpI9JixEREQke0xYiIiISPaYsBAREZHsMWEhIiIi2QuKbc2iKAIATCaTxJEQERHRRLm/b7u/j48nKBKW3t5eAEBmZqbEkRAREZG3ent7ER0dPe5rBHEiaY3MORwONDY2wmg0QhAEnz7bZDIhMzMTdXV1iIqK8umzyXv895AX/nvID/9N5IX/HuMTRRG9vb1IS0uDSjV+lUpQnLCoVCpkZGT49WNERUXxfzYZ4b+HvPDfQ374byIv/Pc4u3OdrLix6JaIiIhkjwkLERERyR4TlnPQ6/V46KGHoNfrpQ6FwH8PueG/h/zw30Re+O/hO0FRdEtERETBjScsREREJHtMWIiIiEj2mLAQERGR7DFhISIiItljwkJERESyx4TlHJ566ilkZ2fDYDCguLgYu3btkjqkkLRu3TosXrwYRqMRSUlJuP7661FZWSl1WOTyyCOPQBAE3HfffVKHErIaGhrwta99DfHx8QgLC8O8efOwZ88eqcMKSXa7HQ888ABycnIQFhaG6dOn4+GHH57Qgj86OyYs49iyZQtKS0vx0EMPYd++fViwYAGWL1+O1tZWqUMLOR988AHuuecefPbZZ3j33XdhtVrxhS98Af39/VKHFvJ2796NP/7xj5g/f77UoYSsrq4uXHjhhdBqtfjPf/6DI0eO4He/+x1iY2OlDi0kPfroo3j66afx5JNP4ujRo3j00Ufx2GOP4fe//73UoSka57CMo7i4GIsXL8aTTz4JwLlkMTMzE/feey/uv/9+iaMLbW1tbUhKSsIHH3yASy65ROpwQlZfXx8WLlyIP/zhD/jVr36FgoICbNiwQeqwQs7999+PTz75BB999JHUoRCAL37xi0hOTsazzz7redsNN9yAsLAwvPTSSxJGpmw8YTkLi8WCvXv3oqSkxPM2lUqFkpIS7NixQ8LICAB6enoAAHFxcRJHEtruueceXHPNNaM+Tyjw/vWvf6GoqAg33XQTkpKSUFhYiE2bNkkdVsi64IILUFZWhmPHjgEADhw4gI8//hhXX321xJEpW1Bsa/aH9vZ22O12JCcnj3p7cnIyKioqJIqKAOdJ13333YcLL7wQc+fOlTqckPXKK69g37592L17t9ShhLyqqio8/fTTKC0txU9+8hPs3r0b3/3ud6HT6XDbbbdJHV7Iuf/++2EymZCXlwe1Wg273Y5f//rXuPXWW6UOTdGYsJDi3HPPPfj888/x8ccfSx1KyKqrq8P3vvc9vPvuuzAYDFKHE/IcDgeKiorwm9/8BgBQWFiIzz//HBs3bmTCIoFXX30Vf/nLX/Dyyy/jvPPOQ3l5Oe677z6kpaXx32MKmLCcRUJCAtRqNVpaWka9vaWlBSkpKRJFRWvWrMFbb72FDz/8EBkZGVKHE7L27t2L1tZWLFy40PM2u92ODz/8EE8++STMZjPUarWEEYaW1NRU5Ofnj3rbnDlz8Nprr0kUUWj74Q9/iPvvvx8333wzAGDevHk4deoU1q1bx4RlCljDchY6nQ6LFi1CWVmZ520OhwNlZWVYunSphJGFJlEUsWbNGrzxxhvYtm0bcnJypA4ppF1xxRU4dOgQysvLPb+Kiopw6623ory8nMlKgF144YVntPkfO3YM06ZNkyii0DYwMACVavS3V7VaDYfDIVFEwYEnLOMoLS3FbbfdhqKiIixZsgQbNmxAf38/Vq9eLXVoIeeee+7Byy+/jH/+858wGo1obm4GAERHRyMsLEzi6EKP0Wg8o34oIiIC8fHxrCuSwPe//31ccMEF+M1vfoMVK1Zg165deOaZZ/DMM89IHVpIuvbaa/HrX/8aWVlZOO+887B//348/vjj+MY3viF1aMom0rh+//vfi1lZWaJOpxOXLFkifvbZZ1KHFJIAjPnrz3/+s9ShkcuyZcvE733ve1KHEbLefPNNce7cuaJerxfz8vLEZ555RuqQQpbJZBK/973viVlZWaLBYBBzc3PFn/70p6LZbJY6NEXjHBYiIiKSPdawEBERkewxYSEiIiLZY8JCREREsseEhYiIiGSPCQsRERHJHhMWIiIikj0mLERERCR7TFiIiIhI9piwEBERkewxYSEiIiLZY8JCREREsvf/AV3Cz10ZAh10AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#USING SKLEARN (original) to lsee loss\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "MAHOMES_clf = MLPClassifier(learning_rate_init = 0.01, activation='relu', hidden_layer_sizes= (100,), alpha = 0.001 )\n",
        "\n",
        "#initialize dataloader with random sampling of size 10 \n",
        "dataset = dataLoader(X, y)\n",
        "trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle=True)\n",
        "\n",
        "#mlp init\n",
        "mlp = theiaMLP(init_features, 256, 1)\n",
        "#set loss function and gradient descet optimizer\n",
        "loss_function = nn.L1Loss()\n",
        "optimizer = torch.optim.Adagrad(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "train_loss = []\n",
        "#train for this many epochs\n",
        "for epoch in range(0,10):\n",
        "    print(f'Starting Epoch {epoch+1}')\n",
        "\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0], 1))\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = mlp(inputs)\n",
        "\n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        current_loss += loss.item()\n",
        "        if i%10 == 0:\n",
        "            print(f'Loss after mini-batch %5d: %.3f'%(i+1, current_loss/500))\n",
        "            current_loss = 0.0\n",
        "    train_loss.append(loss.item())\n",
        "    print(f'Epoch {epoch+1} done')\n",
        "\n",
        "\n",
        "test_data = torch.from_numpy(testX.to_numpy()).float()\n",
        "test_targets = torch.from_numpy(testY.to_numpy()).float()\n",
        "print(\"Test data correct outputs look like this\", test_targets)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "with torch.no_grad():\n",
        "    outputs = mlp(test_data)\n",
        "    predicted_labels = outputs.squeeze().tolist()\n",
        "\n",
        "predicted_labels = np.array(predicted_labels)\n",
        "test_targets = np.array(test_targets)\n",
        "\n",
        "mse = mean_squared_error(test_targets, predicted_labels)\n",
        "r2 = r2_score(test_targets, predicted_labels)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R2 Score:\", r2)\n",
        "\n",
        "plt.plot(train_loss)\n",
        "plt.show()\n",
        "#Results\n",
        "# Mean Squared Error: 0.10884078202201086\n",
        "# R2 Score: 0.508780183660541"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#to set num samples variable for dataset\n",
        "num_samples = len(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#initialize dataloader with random sampling of size 10 \n",
        "dataset = dataLoader(X, y)\n",
        "trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<enumerate at 0x1387db150>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#check that enumerate works\n",
        "enumerate(trainloader, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_result_metrics(alg, feat_set, prediction_df):\n",
        "    mcc = matthews_corrcoef(prediction_df['actual'], prediction_df['bool_pred'])\n",
        "    TN, FP, FN, TP = confusion_matrix(prediction_df['actual'], prediction_df['bool_pred']).ravel()\n",
        "\n",
        "    TPR=(TP/(TP+FN))*100\n",
        "    TNR=(TN/(TN+FP))*100\n",
        "    acc=((TP+TN)/(TP+TN+FP+FN))*100\n",
        "    Prec=(TP/(TP+FP))*100\n",
        "    return(pd.DataFrame([[alg, feat_set, acc, mcc, TPR, TNR, Prec]],\n",
        "        columns=['Algorithm', 'Feature Set', 'Accuracy', 'MCC', 'Recall', 'TrueNegRate', 'Precision']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#train for this many epochs\n",
        "for epoch in range(0,10):\n",
        "    print(f'Starting Epoch {epoch+1}')\n",
        "\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0], 1))\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = mlp(inputs)\n",
        "\n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        current_loss += loss.item()\n",
        "        if i%10 == 0:\n",
        "            print(f'Loss after mini-batch %5d: %.3f'%(i+1, current_loss/500))\n",
        "            current_loss = 0.0\n",
        "    \n",
        "    print(f'Epoch {epoch+1} done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test data correct outputs look like this tensor([1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n"
          ]
        }
      ],
      "source": [
        "test_data = torch.from_numpy(testX.to_numpy()).float()\n",
        "test_targets = torch.from_numpy(testY.to_numpy()).float()\n",
        "print(\"Test data correct outputs look like this\", test_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=181, out_features=181, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=181, out_features=90, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=90, out_features=1, bias=True)\n",
              "    (5): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Run mlp model on test data\n",
        "mlp.eval() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.55222297,  0.60099828,  0.33700261,  0.16569516,  0.12786084,\n",
              "       -0.09439341,  0.61105984,  0.29831496,  0.37959674,  0.37540489,\n",
              "        0.53442001,  0.51242977,  0.37768754,  0.4436639 ,  0.3295171 ,\n",
              "        0.00305723, -0.11090554, -0.03113469,  0.55659735,  0.51181453,\n",
              "        0.52106255,  0.63306159,  0.35841542,  0.48058605,  0.35919982,\n",
              "        0.57094032,  0.26341119,  0.48409846,  0.51234937,  0.50355482,\n",
              "        0.4831036 ,  0.76673108,  0.54336435,  0.14401197,  0.07484119,\n",
              "        0.07294383,  0.05787735,  0.14886454, -0.05228285,  0.01234505,\n",
              "        0.48948708,  0.2463991 ,  0.58187437,  0.76498759,  0.40710023,\n",
              "        0.8641566 ,  0.9011901 ,  0.20198449,  0.08673712,  0.44957846,\n",
              "        0.02019858,  0.47610286,  0.45234445,  0.51072431, -0.03460576,\n",
              "        0.57820815,  0.77991104,  0.7748298 ,  0.64841223,  0.65430748,\n",
              "        1.04801929,  0.54572093,  0.49992767,  0.51271564,  0.46790716,\n",
              "        0.69760609,  0.78038943,  0.14870211,  0.3384459 ,  0.12521482,\n",
              "        0.25420824,  0.17399278,  0.59865952,  0.67811644,  0.47927952,\n",
              "        0.41642097,  0.43828693,  0.69871223,  0.92828614,  1.02623141,\n",
              "        0.91991138,  0.5678705 ,  0.29187906,  0.70981902,  0.6028192 ,\n",
              "        0.451186  ,  0.91285658,  0.84644866,  0.90941161,  0.63238466,\n",
              "        0.72816133,  0.5712893 ,  0.68196416,  0.65541244,  1.16145647,\n",
              "        0.9141016 ,  0.62281489,  0.82212889,  0.58794039,  0.05200776,\n",
              "       -0.07032435,  0.65148765,  0.05814828,  0.42395723,  0.55865115,\n",
              "        0.53393114,  0.96097374,  0.29410723,  0.16645068,  0.61426574,\n",
              "        0.92108071,  0.44042477,  0.33052045, -0.01896035,  0.24082091,\n",
              "        0.00599654,  0.36456132,  0.66086662,  0.65447557,  0.34704664,\n",
              "        0.18558609,  0.23173729,  0.37331513,  0.09599399,  0.65112484,\n",
              "        0.91665339, -0.06088463,  0.87991822,  0.01421605,  0.56489414,\n",
              "        0.45146018,  0.41148669,  0.77444685,  0.58425069,  0.53342855,\n",
              "        0.30884835,  0.58839524,  0.65117729,  0.55341965,  0.57064486,\n",
              "        0.09588137, -0.0423647 , -0.13112059,  0.02154792,  0.63854432,\n",
              "        0.41879404,  0.13439271,  0.41580626,  0.49005839,  0.43961522,\n",
              "        0.69109303,  0.14487724, -0.01600595,  0.33302835,  0.92247587,\n",
              "        0.71468139,  0.69420767,  0.52977377,  0.2252152 ,  0.56140971,\n",
              "        0.36633801,  0.23066869,  0.38318142,  0.15344954,  0.34985706,\n",
              "        0.52029747,  0.57172734,  0.17250878,  0.37572119,  0.87120831,\n",
              "        0.27125111,  0.28446096,  0.47137251,  0.22022796,  0.71816969,\n",
              "        0.67946875,  0.15835479,  0.99171495,  0.21115845,  0.70215738,\n",
              "        0.9120847 ,  0.46003544,  0.42292872,  0.59413141,  0.59937382,\n",
              "        0.62977219,  0.50645524,  0.36917976,  0.40039018,  0.68371993,\n",
              "        0.76393104,  0.50576478,  0.46870938,  0.69626164,  0.6015839 ,\n",
              "        0.48505193,  0.42263454,  0.62065887,  0.84072649, -0.10866759,\n",
              "        0.01308868,  0.19453123,  0.19347325,  0.27314094,  0.05107067,\n",
              "        0.10191983,  0.20964384,  0.24145454, -0.01376899,  0.01239339,\n",
              "        0.03411955, -0.14553046, -0.28376889,  0.07893039,  0.0319862 ,\n",
              "        0.55709749, -0.0200194 ,  0.02865186,  0.46935844,  0.22379968,\n",
              "        0.42362025,  0.70209646, -0.12277818,  0.08340953,  0.02798615,\n",
              "        0.11299177,  0.03938349,  0.02148497, -0.12788036,  0.10822842,\n",
              "        0.21247563,  0.13921276,  0.13411954,  0.01207746,  0.0769556 ,\n",
              "       -0.03391098,  0.25835201,  0.15947071, -0.22789863, -0.16920879,\n",
              "       -0.18465513, -0.26028213,  0.23938939, -0.05158955, -0.01287477,\n",
              "        0.22765547,  0.41351894,  0.51957494,  0.04955632,  0.25575069,\n",
              "        0.31670931, -0.01023458, -0.02662113,  0.03066721,  0.15821362,\n",
              "        0.04962061, -0.07429049,  0.28280303,  0.1153279 , -0.16179621,\n",
              "       -0.23630363, -0.11535931,  0.5830102 ,  0.57527685,  0.25814873,\n",
              "       -0.1966978 ,  0.06478539, -0.01492967,  0.04147985,  0.04535801,\n",
              "        0.04381529,  0.15710095,  0.07465146,  0.16338274,  0.01441198,\n",
              "        0.30790544,  0.65292394,  0.48495412,  0.06144077,  0.08044963,\n",
              "       -0.02972855,  0.01217965, -0.11755801, -0.22060329,  0.08473409,\n",
              "       -0.18991962, -0.02629012,  0.14765775, -0.11331905,  0.00608907,\n",
              "        0.00363377,  0.07964458,  0.06570172, -0.07003249,  0.28568307,\n",
              "        0.13613623, -0.0676053 ,  0.14568405,  0.23166442,  0.27138883,\n",
              "       -0.08136133, -0.00677695, -0.02023939,  0.02893924, -0.0269851 ,\n",
              "       -0.08961231, -0.12259284,  0.1125332 ,  0.10142728, -0.08446147,\n",
              "       -0.03019818, -0.01544529,  0.0094946 ,  0.07005445,  0.03166848,\n",
              "        0.00439863,  0.20948219, -0.08718199,  0.0346756 ,  0.04344705,\n",
              "        0.23522753,  0.02142792,  0.07084042,  0.36210117, -0.04287828,\n",
              "       -0.04159237, -0.1580452 , -0.02934693,  0.12892227,  0.02196094,\n",
              "        0.44120362,  0.23687646,  0.24752975,  0.64244711,  0.54828405,\n",
              "       -0.08557422,  0.07402875, -0.02126162, -0.08609198,  0.30819246,\n",
              "        0.21728486,  0.32873866,  0.14587483,  0.17598408, -0.08112917,\n",
              "        0.00744379,  0.32522556,  0.20194966, -0.03326504,  0.09889145,\n",
              "       -0.25017545,  0.16240528,  0.41444623, -0.07527332, -0.12904945,\n",
              "        0.10987083,  0.10361979,  0.08800087,  0.66870856,  0.17689875,\n",
              "        0.13448271,  0.15976253, -0.01421573, -0.15101361,  0.29420322,\n",
              "        0.22146249,  0.02182383,  0.01708038,  0.25957182,  0.01924838,\n",
              "       -0.09410945,  0.15853637, -0.21458828,  0.24189037, -0.07940344,\n",
              "        0.3993988 ,  0.5888837 , -0.00780624,  0.08043097,  0.06252689,\n",
              "        0.27880275,  0.21284571,  0.45892709,  0.10239298, -0.03212727,\n",
              "        0.49108085,  0.11690759,  0.08533102,  0.10106193,  0.06559744,\n",
              "       -0.07616053,  0.01351162,  0.09278397, -0.14917755,  0.09578769,\n",
              "        0.14467257,  0.16246721,  0.79160553,  0.52534586, -0.17140916,\n",
              "        0.03409453,  0.05054891,  0.09624276,  0.06623647,  0.05369876,\n",
              "        0.00898914, -0.03397875,  0.11187872,  0.04906147, -0.02644818,\n",
              "        0.13517404,  0.24735835,  0.01580444,  0.0679566 ,  0.0223969 ,\n",
              "        0.24475822, -0.05852837,  0.00439103, -0.01947853,  0.07371775,\n",
              "        0.83615029, -0.00939929,  0.04562781, -0.11794808, -0.01385105,\n",
              "        0.06977548,  0.08802321,  0.1022329 ,  0.06199373,  0.07052244,\n",
              "        0.16059014, -0.26783136, -0.11737723,  0.19720903,  0.11519051,\n",
              "        0.11787632,  0.17039204, -0.19681293, -0.00130337,  0.03623645,\n",
              "        0.10924061, -0.03633149,  0.32882237,  0.04908478, -0.29834452,\n",
              "        0.20303139, -0.19054565,  0.19815254, -0.18813539,  0.11090262,\n",
              "        0.15034685, -0.06967187, -0.09550907,  0.18002009,  0.19379064,\n",
              "        0.09514172,  0.20132706,  0.43630165,  0.2848939 ,  0.11053121,\n",
              "        0.32722083,  0.11181837, -0.00885849,  0.050386  ,  0.18363881,\n",
              "        0.12725903, -0.05892592,  0.42769048,  0.50968015,  0.21308178,\n",
              "       -0.02725001,  0.1648109 ,  0.00573848,  0.0870747 , -0.14731669,\n",
              "        0.07611825,  0.46365267, -0.19411138,  0.10116784,  0.11415551,\n",
              "        0.09211033,  0.46657774,  0.03642913,  0.17693549,  0.30295876,\n",
              "       -0.09594674, -0.09906995,  0.03045315,  0.13852507,  1.13735974,\n",
              "        0.2975212 ,  0.23372921,  0.10526545, -0.04578678, -0.02753144,\n",
              "       -0.11013845, -0.11737493, -0.17481068,  0.023602  ,  0.26840219,\n",
              "       -0.00859263,  0.73633099,  0.09158636, -0.0244863 ,  0.1683526 ,\n",
              "        0.65155506,  0.55215323,  0.13555118, -0.08277931,  0.36217132,\n",
              "        0.47325408,  0.11864922, -0.12483043,  0.16137359, -0.37091035,\n",
              "        0.66359991])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 0.1768569084712416\n",
            "R2 Score: 0.20181005241179772\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "with torch.no_grad():\n",
        "    outputs = mlp(test_data)\n",
        "    predicted_labels = outputs.squeeze().tolist()\n",
        "\n",
        "predicted_labels = np.array(predicted_labels)\n",
        "test_targets = np.array(test_targets)\n",
        "\n",
        "mse = mean_squared_error(test_targets, predicted_labels)\n",
        "r2 = r2_score(test_targets, predicted_labels)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R2 Score:\", r2)\n",
        "# Mean Squared Error: 0.18271737790066617\n",
        "# R2 Score: 0.17536060337896386\n",
        "# So absolute crap, but it's a start"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
