{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "mL0ZoKwmSIrL"
      },
      "outputs": [],
      "source": [
        "#Self-hyperparam selection: https://link.springer.com/article/10.1007/s11063-024-11578-0\n",
        "#Self-pruning: https://github.com/skarifahmed/seMLP/blob/main/src/Prune.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "PMr_2rxGcZyG"
      },
      "outputs": [],
      "source": [
        "from utils import Utils\n",
        "from color import color \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "# libraries\n",
        "import joblib\n",
        "\n",
        "# scale features\n",
        "from sklearn import preprocessing\n",
        "from sklearn import impute\n",
        "# classifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "# scoring metrics\n",
        "from sklearn.metrics import confusion_matrix, matthews_corrcoef\n",
        "\n",
        "# custom scripts\n",
        "import sys\n",
        "sys.path.insert(0, \"%s\" % \"CV/\")\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, GroupShuffleSplit, StratifiedShuffleSplit, cross_validate, StratifiedKFold\n",
        "from sklearn.metrics import roc_curve, auc, recall_score, accuracy_score, precision_score, confusion_matrix, make_scorer, matthews_corrcoef, jaccard_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "site_path = \"/Users/sanjanayasna/csc334/MLP_MAHOMES/data/sites_calculated_features.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mAll features:\u001b[0m\n",
            "sites: 3981 \tcolumns: 485\n",
            "Set   Catalytic\n",
            "data  False        2636\n",
            "      True          829\n",
            "test  False         345\n",
            "      True          171\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#read in feature set:\n",
        "sites = pd.read_csv(site_path)\n",
        "sites = sites.set_index('SITE_ID',drop=True)\n",
        "\n",
        "# The following labels need to be changed after looking over literature (see Feehan, Franklin, Slusky 2021)\n",
        "change_site_labels = [\"5zb8_0\", \"6aci_0\", \"6oq7_0\", \"6pjv_1\", \"6q55_0\",\n",
        "                      \"6q55_2\", \"6rmg_0\", \"6rtg_0\", \"6rw0_0\", \"6v77_0\"]\n",
        "\n",
        "# The following sites are removed due to unkopwn correct labels (see Feehan, Franklin, Slusky 2021)\n",
        "sites.loc[sites.index.isin(change_site_labels), 'Catalytic']=True\n",
        "remove_sites = [\"6mf0_1\", \"6okh_0\", \"6qwo_0\", \"6r9n_0\"]\n",
        "sites=sites.loc[~sites.index.isin(remove_sites)]\n",
        "\n",
        "#print shape of dataset\n",
        "print(color.BOLD + \"All features:\" + color.END)\n",
        "print(\"sites: %s \\tcolumns: %s\"%(sites.shape[0], sites.shape[1]))\n",
        "sizes = sites.groupby([\"Set\", \"Catalytic\"]).size()\n",
        "print(sizes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save_models toggel\n",
        "save_models = False\n",
        "#pkl output path\n",
        "pkl_out = r'/Users/sanjanayasna/csc334/MLP_MAHOMES/pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Catalytic</th>\n",
              "      <th>MetalCodes</th>\n",
              "      <th>MetalAtoms</th>\n",
              "      <th>fa_atr_Sum_3.5</th>\n",
              "      <th>fa_rep_Sum_3.5</th>\n",
              "      <th>fa_sol_Sum_3.5</th>\n",
              "      <th>fa_intra_atr_xover4_Sum_3.5</th>\n",
              "      <th>fa_intra_rep_xover4_Sum_3.5</th>\n",
              "      <th>fa_intra_sol_xover4_Sum_3.5</th>\n",
              "      <th>lk_ball_Sum_3.5</th>\n",
              "      <th>...</th>\n",
              "      <th>geom_cn8</th>\n",
              "      <th>geom_cn9</th>\n",
              "      <th>geom_Filled</th>\n",
              "      <th>geom_PartFilled</th>\n",
              "      <th>geom_AvgN</th>\n",
              "      <th>geom_AvgO</th>\n",
              "      <th>geom_AvgS</th>\n",
              "      <th>geom_AvgOther</th>\n",
              "      <th>SC_vol_perc</th>\n",
              "      <th>Set</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SITE_ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6s9z_0</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-33.20757</td>\n",
              "      <td>20.22373</td>\n",
              "      <td>26.34441</td>\n",
              "      <td>-1.88617</td>\n",
              "      <td>0.46054</td>\n",
              "      <td>2.14096</td>\n",
              "      <td>14.05052</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.910384</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6g5l_0</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-27.04899</td>\n",
              "      <td>39.17134</td>\n",
              "      <td>22.76555</td>\n",
              "      <td>-1.71942</td>\n",
              "      <td>0.45999</td>\n",
              "      <td>2.05517</td>\n",
              "      <td>12.94894</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.862189</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6hwz_0</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-27.30433</td>\n",
              "      <td>35.04867</td>\n",
              "      <td>23.45195</td>\n",
              "      <td>-1.62146</td>\n",
              "      <td>0.35902</td>\n",
              "      <td>1.91231</td>\n",
              "      <td>13.06378</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.991431</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6qww_0</th>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-25.36664</td>\n",
              "      <td>12.54178</td>\n",
              "      <td>27.17902</td>\n",
              "      <td>-1.14349</td>\n",
              "      <td>0.22087</td>\n",
              "      <td>1.68091</td>\n",
              "      <td>11.47631</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.864546</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6qww_1</th>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-30.53159</td>\n",
              "      <td>8.99318</td>\n",
              "      <td>27.77842</td>\n",
              "      <td>-1.00782</td>\n",
              "      <td>0.39657</td>\n",
              "      <td>1.04229</td>\n",
              "      <td>13.23736</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.990893</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 485 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Catalytic  MetalCodes  MetalAtoms  fa_atr_Sum_3.5  fa_rep_Sum_3.5  \\\n",
              "SITE_ID                                                                      \n",
              "6s9z_0        True           1           1       -33.20757        20.22373   \n",
              "6g5l_0        True           1           1       -27.04899        39.17134   \n",
              "6hwz_0        True           1           1       -27.30433        35.04867   \n",
              "6qww_0        True           1           1       -25.36664        12.54178   \n",
              "6qww_1       False           1           1       -30.53159         8.99318   \n",
              "\n",
              "         fa_sol_Sum_3.5  fa_intra_atr_xover4_Sum_3.5  \\\n",
              "SITE_ID                                                \n",
              "6s9z_0         26.34441                     -1.88617   \n",
              "6g5l_0         22.76555                     -1.71942   \n",
              "6hwz_0         23.45195                     -1.62146   \n",
              "6qww_0         27.17902                     -1.14349   \n",
              "6qww_1         27.77842                     -1.00782   \n",
              "\n",
              "         fa_intra_rep_xover4_Sum_3.5  fa_intra_sol_xover4_Sum_3.5  \\\n",
              "SITE_ID                                                             \n",
              "6s9z_0                       0.46054                      2.14096   \n",
              "6g5l_0                       0.45999                      2.05517   \n",
              "6hwz_0                       0.35902                      1.91231   \n",
              "6qww_0                       0.22087                      1.68091   \n",
              "6qww_1                       0.39657                      1.04229   \n",
              "\n",
              "         lk_ball_Sum_3.5  ...  geom_cn8  geom_cn9  geom_Filled  \\\n",
              "SITE_ID                   ...                                    \n",
              "6s9z_0          14.05052  ...       0.0       0.0          0.0   \n",
              "6g5l_0          12.94894  ...       0.0       0.0          0.0   \n",
              "6hwz_0          13.06378  ...       0.0       0.0          0.0   \n",
              "6qww_0          11.47631  ...       0.0       0.0          0.0   \n",
              "6qww_1          13.23736  ...       0.0       0.0          1.0   \n",
              "\n",
              "         geom_PartFilled  geom_AvgN  geom_AvgO  geom_AvgS  geom_AvgOther  \\\n",
              "SITE_ID                                                                    \n",
              "6s9z_0               1.0        3.0        0.0        0.0            0.0   \n",
              "6g5l_0               1.0        3.0        0.0        0.0            0.0   \n",
              "6hwz_0               1.0        3.0        0.0        0.0            0.0   \n",
              "6qww_0               0.0        0.0        3.0        0.0            0.0   \n",
              "6qww_1               0.0        0.0        4.0        0.0            1.0   \n",
              "\n",
              "         SC_vol_perc   Set  \n",
              "SITE_ID                     \n",
              "6s9z_0      0.910384  test  \n",
              "6g5l_0      0.862189  test  \n",
              "6hwz_0      0.991431  test  \n",
              "6qww_0      0.864546  test  \n",
              "6qww_1      0.990893  test  \n",
              "\n",
              "[5 rows x 485 columns]"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sites.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mAll scaled data-set features:\u001b[0m\n",
            "sites: 3465 \tcolumns: 484\n",
            "Catalytic\n",
            "False    2636\n",
            "True      829\n",
            "dtype: int64\n",
            "\u001b[1m\n",
            "All scaled T-metal-site features:\u001b[0m\n",
            "sites: 516 \tcolumns: 484\n",
            "Catalytic\n",
            "False    345\n",
            "True     171\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#Get scaled features\n",
        "data_scaled, Tsites_scaled = Utils.get_scaled_features(sites =sites, pkl_out=pkl_out, save_models=save_models)\n",
        "#Print stats\n",
        "print(color.BOLD + \"All scaled data-set features:\" + color.END)\n",
        "print(\"sites: %s \\tcolumns: %s\"%(data_scaled.shape[0], data_scaled.shape[1]))\n",
        "print(data_scaled.groupby([\"Catalytic\"]).size())\n",
        "\n",
        "print(color.BOLD + \"\\nAll scaled T-metal-site features:\" + color.END)\n",
        "print(\"sites: %s \\tcolumns: %s\"%(Tsites_scaled.shape[0], Tsites_scaled.shape[1]))\n",
        "print(Tsites_scaled.groupby([\"Catalytic\"]).size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [],
      "source": [
        "dir = \"/Users/sanjanayasna/csc334/MLP_MAHOMES/data/\"\n",
        "#save the scaled data\n",
        "data_scaled.to_csv(os.path.join(dir, \"data_scaled.csv\"))\n",
        "Tsites_scaled.to_csv(os.path.join(dir, \"Tsites_scaled.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "#set feature set type\n",
        "MAHOMES_feature_set = \"AllMeanSph\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Well sampled training data\n",
        "#X is train\n",
        "#y is target for train\n",
        "X, y = Utils.get_training_data(MAHOMES_feature_set, random_seed = 1, data_scaled= data_scaled)\n",
        " ## prepare test-set\n",
        "testX = Tsites_scaled.copy()\n",
        "testY = testX['Catalytic']; del testX['Catalytic']\n",
        "testX = Utils.feature_subset(testX, MAHOMES_feature_set, noBSA=True)\n",
        "\n",
        "## get multiple predictions for test-set w/ diff random seeds\n",
        "test_site_preds = {'actual': pd.Series(testY, index=testX.index)}\n",
        "\n",
        "#Overview:\n",
        "# X: training data\n",
        "# y: target for training data\n",
        "# testX: test data\n",
        "# testY: target for test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "181\n"
          ]
        }
      ],
      "source": [
        "#Train input feed params\n",
        "init_features = len(X.columns)\n",
        "twice = init_features * 2\n",
        "print(init_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Prelim mlp\n",
        "#Possible avenue for bias and weight matrix initialization:\n",
        "## Initialize weights using Xavier uniform initialization\n",
        "# init.xavier_uniform_(linear_layer.weight)\n",
        " \n",
        "# ## Initialize bias to zero\n",
        "# init.zeros_(linear_layer.bias)\n",
        "#---------------------------------\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "#Previous experiment\n",
        "class MLP(nn.Module):  # nn.Module is the base class for all models in PyTorch\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            \n",
        "            nn.Linear(init_features, 181),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(181, 90),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(90, 1),\n",
        "            nn.Sigmoid()\n",
        "            #try to make output binary (0 or 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "     #   x =  self.layers(x)\n",
        "        return self.layers(x)\n",
        "\n",
        "#Deeper model\n",
        "# class MLP(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         #wide input\n",
        "#         self.layers = nn.Sequential(\n",
        "#             nn.Linear(init_features, init_features),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear( init_features, 60),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(60, 60),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(60, 1),\n",
        "#             nn.Sigmoid()\n",
        "#         )\n",
        "#         # self.layer1 = nn.Linear((init_features * 2), init_features)\n",
        "#         # self.act1 = nn.ReLU()\n",
        "#         # self.layer2 = nn.Linear(init_features, 60)\n",
        "#         # self.act2 = nn.ReLU()\n",
        "#         # self.layer3 = nn.Linear(60, 60)\n",
        "#         # self.act3 = nn.ReLU()\n",
        "#         # self.output = nn.Linear(60, 1)\n",
        "#         # self.sigmoid = nn.Sigmoid()\n",
        " \n",
        "#     def forward(self, x):\n",
        "#         # x = self.act1(self.layer1(x))\n",
        "#         # x = self.act2(self.layer2(x))\n",
        "#         # x = self.act3(self.layer3(x))\n",
        "#         # x = self.sigmoid(self.output(x))\n",
        "#         x = self.layers(x)\n",
        "#         return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Loads to torch tensors\n",
        "class dataLoader:\n",
        "    #Use ONLY train data \n",
        "    def __init__(self, X, y):\n",
        "        #converts x and y to numpy arr so they can be torch tensor\n",
        "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
        "            X = X.to_numpy()\n",
        "            y = y.to_numpy()\n",
        "        #x_train\n",
        "        # if not torch.is_tensor(X):\n",
        "        #     self.X = torch.from_numpy(X)\n",
        "        # #y_train\n",
        "        # if not torch.is_tensor(y):\n",
        "        #     self.y = torch.from_numpy(y)\n",
        "            \n",
        "        #To convert boolean to int, do .long()\n",
        "        self.X = torch.from_numpy(X)\n",
        "        self.y = torch.from_numpy(y).long()\n",
        "    def get_trainloader(dataset):\n",
        "        return torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "    def get_testloader(dataset):\n",
        "        return torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "    #to get lenght, for enumerator use\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Epoch 1\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.007\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.006\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.006\n",
            "Loss after mini-batch    61: 0.006\n",
            "Loss after mini-batch    71: 0.006\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.006\n",
            "Loss after mini-batch   111: 0.006\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.006\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.006\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.006\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.006\n",
            "Loss after mini-batch   311: 0.005\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 1 done\n",
            "Starting Epoch 2\n",
            "Loss after mini-batch     1: 0.001\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.006\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.006\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.006\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.006\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.006\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 2 done\n",
            "Starting Epoch 3\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 3 done\n",
            "Starting Epoch 4\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 4 done\n",
            "Starting Epoch 5\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.004\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.006\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.005\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 5 done\n",
            "Starting Epoch 6\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.005\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.006\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.005\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.005\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 6 done\n",
            "Starting Epoch 7\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.005\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.005\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.004\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.003\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.004\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 7 done\n",
            "Starting Epoch 8\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.005\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.005\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.004\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.006\n",
            "Loss after mini-batch   221: 0.005\n",
            "Loss after mini-batch   231: 0.004\n",
            "Loss after mini-batch   241: 0.005\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.005\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.005\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 8 done\n",
            "Starting Epoch 9\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.005\n",
            "Loss after mini-batch    21: 0.004\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.005\n",
            "Loss after mini-batch    51: 0.004\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.006\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.003\n",
            "Loss after mini-batch   111: 0.004\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.004\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.005\n",
            "Loss after mini-batch   161: 0.005\n",
            "Loss after mini-batch   171: 0.005\n",
            "Loss after mini-batch   181: 0.004\n",
            "Loss after mini-batch   191: 0.004\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.003\n",
            "Loss after mini-batch   231: 0.006\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.004\n",
            "Loss after mini-batch   261: 0.005\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.004\n",
            "Loss after mini-batch   291: 0.004\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.005\n",
            "Epoch 9 done\n",
            "Starting Epoch 10\n",
            "Loss after mini-batch     1: 0.000\n",
            "Loss after mini-batch    11: 0.004\n",
            "Loss after mini-batch    21: 0.005\n",
            "Loss after mini-batch    31: 0.004\n",
            "Loss after mini-batch    41: 0.004\n",
            "Loss after mini-batch    51: 0.005\n",
            "Loss after mini-batch    61: 0.004\n",
            "Loss after mini-batch    71: 0.004\n",
            "Loss after mini-batch    81: 0.005\n",
            "Loss after mini-batch    91: 0.004\n",
            "Loss after mini-batch   101: 0.004\n",
            "Loss after mini-batch   111: 0.005\n",
            "Loss after mini-batch   121: 0.005\n",
            "Loss after mini-batch   131: 0.005\n",
            "Loss after mini-batch   141: 0.005\n",
            "Loss after mini-batch   151: 0.004\n",
            "Loss after mini-batch   161: 0.004\n",
            "Loss after mini-batch   171: 0.004\n",
            "Loss after mini-batch   181: 0.005\n",
            "Loss after mini-batch   191: 0.005\n",
            "Loss after mini-batch   201: 0.004\n",
            "Loss after mini-batch   211: 0.004\n",
            "Loss after mini-batch   221: 0.004\n",
            "Loss after mini-batch   231: 0.005\n",
            "Loss after mini-batch   241: 0.004\n",
            "Loss after mini-batch   251: 0.005\n",
            "Loss after mini-batch   261: 0.004\n",
            "Loss after mini-batch   271: 0.004\n",
            "Loss after mini-batch   281: 0.006\n",
            "Loss after mini-batch   291: 0.005\n",
            "Loss after mini-batch   301: 0.004\n",
            "Loss after mini-batch   311: 0.004\n",
            "Loss after mini-batch   321: 0.005\n",
            "Loss after mini-batch   331: 0.004\n",
            "Epoch 10 done\n",
            "Test data correct outputs look like this tensor([1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
            "Mean Squared Error: 0.1051256037355469\n",
            "R2 Score: 0.5255475126124418\n"
          ]
        }
      ],
      "source": [
        "#Another attempt https://github.com/daenuprobst/theia/blob/main/src/theia/ml/mlp_classifier.py \n",
        "class theiaMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(theiaMLP, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.fc1(x)\n",
        "        tanh = self.tanh(hidden)\n",
        "        output = self.fc2(tanh)\n",
        "        return output\n",
        "    \n",
        "\n",
        "#initialize dataloader with random sampling of size 10 \n",
        "dataset = dataLoader(X, y)\n",
        "trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle=True)\n",
        "\n",
        "#mlp init\n",
        "mlp = theiaMLP(init_features, 1000, 1)\n",
        "#set loss function and gradient descet optimizer\n",
        "loss_function = nn.L1Loss()\n",
        "optimizer = torch.optim.Adagrad(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "#train for this many epochs\n",
        "for epoch in range(0,10):\n",
        "    print(f'Starting Epoch {epoch+1}')\n",
        "\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0], 1))\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = mlp(inputs)\n",
        "\n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        current_loss += loss.item()\n",
        "        if i%10 == 0:\n",
        "            print(f'Loss after mini-batch %5d: %.3f'%(i+1, current_loss/500))\n",
        "            current_loss = 0.0\n",
        "    \n",
        "    print(f'Epoch {epoch+1} done')\n",
        "\n",
        "\n",
        "test_data = torch.from_numpy(testX.to_numpy()).float()\n",
        "test_targets = torch.from_numpy(testY.to_numpy()).float()\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "with torch.no_grad():\n",
        "    outputs = mlp(test_data)\n",
        "    predicted_labels = outputs.squeeze().tolist()\n",
        "\n",
        "predicted_labels = np.array(predicted_labels)\n",
        "test_targets = np.array(test_targets)\n",
        "\n",
        "mse = mean_squared_error(test_targets, predicted_labels)\n",
        "r2 = r2_score(test_targets, predicted_labels)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R2 Score:\", r2)\n",
        "#Results\n",
        "# Mean Squared Error: 0.11795077403554882\n",
        "# R2 Score: 0.46766500127569677"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 5.32963753e-01,  6.39964402e-01,  4.29306656e-01,  1.70888945e-01,\n",
              "        1.79741696e-01, -1.13772014e-02,  5.57978213e-01,  2.99851090e-01,\n",
              "        5.12628376e-01,  5.02125442e-01,  4.82025921e-01,  5.21338642e-01,\n",
              "        4.15739626e-01,  5.68720460e-01,  4.12976325e-01,  4.97355219e-03,\n",
              "       -1.91447213e-01,  5.15680239e-02,  5.63815534e-01,  5.75622976e-01,\n",
              "        6.65774882e-01,  6.59091055e-01,  3.02221239e-01,  4.89406198e-01,\n",
              "        3.14879537e-01,  6.06696963e-01,  4.26411510e-01,  6.26404464e-01,\n",
              "        5.78241348e-01,  6.09599948e-01,  6.03339314e-01,  8.39068830e-01,\n",
              "        5.46481133e-01,  1.40842333e-01,  1.82300851e-01,  3.34612355e-02,\n",
              "        4.65541855e-02,  1.13234282e-01, -5.72907273e-03,  1.93075165e-02,\n",
              "        6.69669211e-01,  3.18893284e-01,  6.07983589e-01,  8.52727413e-01,\n",
              "        5.83053827e-01,  8.14729095e-01,  1.01978624e+00,  2.45583281e-01,\n",
              "        1.22324832e-01,  5.56691408e-01, -6.38945494e-03,  4.88863498e-01,\n",
              "        5.80283582e-01,  5.37816763e-01,  8.01605657e-02,  7.33485818e-01,\n",
              "        8.93332422e-01,  9.78406250e-01,  8.80979836e-01,  8.86609554e-01,\n",
              "        1.14878643e+00,  7.43578911e-01,  6.61044955e-01,  7.20807433e-01,\n",
              "        6.44949734e-01,  8.35978448e-01,  8.57629061e-01,  8.68164673e-02,\n",
              "        5.58455765e-01,  3.30399364e-01,  4.18201059e-01,  3.76148909e-01,\n",
              "        6.43611252e-01,  6.88029051e-01,  6.61306262e-01,  5.77508569e-01,\n",
              "        5.86218715e-01,  1.15183985e+00,  9.89323914e-01,  1.14540517e+00,\n",
              "        1.11704266e+00,  5.01674891e-01,  2.02010363e-01,  6.71683788e-01,\n",
              "        6.81281209e-01,  4.61313218e-01,  1.03109515e+00,  9.54302430e-01,\n",
              "        9.65426803e-01,  6.33117199e-01,  5.85677683e-01,  7.69703925e-01,\n",
              "        7.60293126e-01,  8.06817889e-01,  1.09601915e+00,  1.14901388e+00,\n",
              "        5.31013727e-01,  8.55483294e-01,  5.41583657e-01, -3.32651809e-02,\n",
              "       -2.53105834e-02,  8.38838577e-01,  3.10061648e-02,  4.22626764e-01,\n",
              "        5.46970844e-01,  4.78276610e-01,  9.10980463e-01,  4.43231612e-01,\n",
              "        1.98101506e-01,  6.86046362e-01,  9.53722358e-01,  5.01677990e-01,\n",
              "        3.42398167e-01,  8.52045417e-02,  1.87538430e-01,  6.29516989e-02,\n",
              "        3.18966597e-01,  6.11900806e-01,  6.93753839e-01,  5.58684409e-01,\n",
              "        3.59297484e-01,  3.29440951e-01,  4.36798185e-01,  1.79389879e-01,\n",
              "        5.86407304e-01,  1.05422509e+00,  6.97127208e-02,  9.36516762e-01,\n",
              "        9.23873037e-02,  6.18307829e-01,  5.78182578e-01,  3.53940070e-01,\n",
              "        9.88314748e-01,  7.29055405e-01,  5.90509355e-01,  3.10404807e-01,\n",
              "        7.01348662e-01,  7.41675615e-01,  5.51393151e-01,  6.06328130e-01,\n",
              "        1.82304338e-01, -7.41255060e-02, -2.64544785e-01,  3.11824754e-02,\n",
              "        5.37944913e-01,  5.66310525e-01,  2.17716321e-01,  3.83848131e-01,\n",
              "        7.11460829e-01,  1.66046306e-01,  6.94661617e-01,  4.81013834e-01,\n",
              "       -1.19109802e-01,  5.94192073e-02,  9.89914656e-01,  7.31002212e-01,\n",
              "        7.60174632e-01,  4.67820078e-01,  2.42283791e-01,  6.25655353e-01,\n",
              "        3.93097788e-01,  3.02035719e-01,  5.49227417e-01,  1.08916380e-01,\n",
              "        6.07254982e-01,  7.23080158e-01,  6.32351041e-01,  4.76480395e-01,\n",
              "        4.09645826e-01,  9.68550205e-01,  3.14856410e-01,  4.87978697e-01,\n",
              "        4.57169324e-01,  1.86534569e-01,  8.15458775e-01,  8.79523754e-01,\n",
              "        2.37862304e-01,  1.13154852e+00,  3.59581679e-01,  9.02100444e-01,\n",
              "        7.62453675e-01,  7.02708364e-01,  7.10525692e-01,  9.00159359e-01,\n",
              "        6.90783560e-01,  8.02644014e-01,  6.30124211e-01,  4.54512686e-01,\n",
              "        4.84458029e-01,  6.70938134e-01,  8.95794988e-01,  5.56868970e-01,\n",
              "        4.90212679e-01,  8.48624766e-01,  6.92138433e-01,  6.78382277e-01,\n",
              "        4.29886192e-01,  8.78168762e-01,  9.85081255e-01, -8.03232491e-02,\n",
              "        2.34526470e-02,  1.17889300e-01,  1.46176070e-01,  2.62439579e-01,\n",
              "        1.11720636e-01, -1.24710957e-02,  2.04893723e-01,  9.28682983e-02,\n",
              "        9.82440487e-02, -1.18513979e-01,  5.51669300e-02, -6.89191222e-02,\n",
              "        6.57340512e-02,  5.64644635e-02,  2.02993497e-01,  6.34257615e-01,\n",
              "        4.79851142e-02,  6.76368997e-02,  5.03193200e-01,  2.97971398e-01,\n",
              "        6.35914624e-01,  8.04489374e-01, -2.15917125e-01, -3.16827521e-02,\n",
              "       -3.59342769e-02,  1.45877317e-01,  6.35158569e-02,  6.46257922e-02,\n",
              "       -6.85577318e-02, -1.42523631e-01,  1.79817781e-01,  2.35772014e-01,\n",
              "        2.55197406e-01, -4.86735348e-03, -5.48123661e-03, -4.33685556e-02,\n",
              "        3.46041024e-01,  2.07687795e-01, -2.46559545e-01, -1.36023089e-01,\n",
              "       -1.77445188e-01, -9.23948810e-02,  3.59578699e-01, -6.26785401e-03,\n",
              "        2.47080773e-02,  1.23006575e-01,  5.58451414e-01,  5.60271859e-01,\n",
              "        1.01352341e-01,  2.68884778e-01,  3.32507551e-01, -2.05130503e-02,\n",
              "        1.41380087e-01,  1.88961238e-01,  1.26565412e-01,  8.46933648e-02,\n",
              "       -7.94666186e-02,  1.81676537e-01, -1.62410513e-02, -1.12197272e-01,\n",
              "       -6.09983429e-02, -1.68011487e-02,  5.79350412e-01,  5.32048285e-01,\n",
              "        7.08009079e-02, -2.15248361e-01,  5.09183146e-02,  1.58091336e-01,\n",
              "        2.32650861e-02,  7.21067712e-02, -5.12869880e-02,  3.25795561e-02,\n",
              "        1.20813444e-01,  3.44278924e-02,  2.86413953e-02,  3.07237506e-01,\n",
              "        7.32826054e-01,  4.68720764e-01,  3.29443738e-02,  1.31840026e-02,\n",
              "       -6.22698292e-02, -1.75854340e-02, -7.85466060e-02, -1.05189987e-01,\n",
              "        2.67585069e-02, -1.82118729e-01,  1.30660340e-01,  1.24688290e-01,\n",
              "       -2.07997337e-01, -6.15927950e-02, -6.33838549e-02,  1.64280251e-01,\n",
              "        8.50116368e-03, -6.04624301e-02,  4.57335055e-01, -2.94653773e-02,\n",
              "       -6.48782030e-02,  1.94948494e-01,  6.17302246e-02,  4.46281374e-01,\n",
              "       -9.49470401e-02, -5.94092831e-02, -1.99234933e-02, -4.66924254e-03,\n",
              "       -5.52915409e-02, -9.19490829e-02, -2.11141840e-01,  4.12337482e-02,\n",
              "        2.08789721e-01, -4.66263555e-02, -1.06309587e-02, -2.73318682e-03,\n",
              "        5.66376448e-02, -1.17678083e-01, -2.06531957e-02,  1.02518536e-01,\n",
              "        1.08333051e-01, -3.59255597e-02,  6.68820590e-02, -1.84128061e-02,\n",
              "        3.24131221e-01,  1.64066195e-01,  1.75089195e-01,  3.80867809e-01,\n",
              "        8.38878825e-02,  2.93373987e-02, -1.29467010e-01,  1.75205842e-02,\n",
              "        1.21281274e-01,  8.49927738e-02,  3.27906072e-01,  4.12969962e-02,\n",
              "        2.22107381e-01,  7.01145411e-01,  5.62823951e-01,  5.46614602e-02,\n",
              "        2.69154340e-01, -6.33079484e-02, -1.50280550e-01,  2.78977305e-01,\n",
              "        2.38172308e-01,  3.29710215e-01,  1.46980569e-01,  1.94484636e-01,\n",
              "        2.28775069e-02,  4.86766770e-02,  4.05987650e-01,  6.57809302e-02,\n",
              "       -1.20347090e-01,  1.13395922e-01, -6.21157289e-02,  1.68440297e-01,\n",
              "        5.67920208e-01, -1.21975444e-01, -5.91904745e-02,  2.46372908e-01,\n",
              "       -5.37951812e-02,  3.71007323e-02,  4.58513170e-01,  2.19485730e-01,\n",
              "        2.62396067e-01,  5.34441322e-02,  4.76913229e-02, -1.07769020e-01,\n",
              "        3.49420339e-01,  3.56493801e-01,  1.37246370e-01,  8.61203372e-02,\n",
              "        2.94158012e-01, -8.28430988e-04, -5.33560887e-02,  1.55714512e-01,\n",
              "       -1.26809910e-01,  1.89165547e-01,  8.05086419e-02,  5.75770259e-01,\n",
              "        6.52674556e-01, -2.20306367e-02,  4.64805216e-02, -4.10745814e-02,\n",
              "        2.25036800e-01,  1.35641685e-02,  4.30214524e-01,  9.01162997e-02,\n",
              "       -3.83930653e-02,  6.33507669e-01,  4.51919176e-02,  6.24271967e-02,\n",
              "        5.79905417e-03,  1.29375756e-01, -1.08221851e-01, -5.30824736e-02,\n",
              "        1.60417259e-01, -1.83945030e-01,  2.63086140e-01,  1.23291230e-02,\n",
              "        3.50209512e-02,  7.94712484e-01,  4.38765496e-01, -1.53642789e-01,\n",
              "        7.89020583e-02,  5.48259541e-02,  1.46628231e-01,  2.31098831e-02,\n",
              "        2.69182548e-02, -5.10265678e-02, -1.01926014e-01,  1.62443221e-01,\n",
              "       -1.62958443e-01,  1.16452970e-01,  1.86675847e-01,  2.45338798e-01,\n",
              "        3.19845118e-02,  9.44801793e-02,  1.05012869e-02,  3.08044821e-01,\n",
              "       -2.08316550e-01,  4.49564233e-02,  1.19159287e-02,  4.77349982e-02,\n",
              "        1.00241423e+00, -6.28224984e-02,  8.27135518e-02, -4.99472693e-02,\n",
              "        1.68529227e-02,  1.00925304e-01,  6.64062332e-03,  2.36417614e-02,\n",
              "        1.00125723e-01, -6.97191153e-03,  6.99648410e-02, -1.09549969e-01,\n",
              "       -3.77545543e-02,  2.09480539e-01,  2.23512992e-01,  1.93714634e-01,\n",
              "        1.57293335e-01, -2.59081006e-01,  1.63786873e-01,  9.26785395e-02,\n",
              "        7.67903682e-03, -2.94849202e-02,  4.17305648e-01,  8.51123109e-02,\n",
              "       -3.77112657e-01,  1.33471489e-01, -4.17397529e-01,  2.00332835e-01,\n",
              "       -1.06619552e-01,  2.44725600e-01,  2.27469727e-02,  2.23602355e-02,\n",
              "       -3.69166955e-02,  1.64817557e-01,  6.83949050e-03, -6.89912513e-02,\n",
              "        1.57917142e-01,  2.71358609e-01,  1.74016714e-01,  5.46535775e-02,\n",
              "        4.06370431e-01,  9.39220339e-02,  5.40971383e-02,  6.27784058e-02,\n",
              "        1.28629103e-01,  2.33478546e-01, -1.19676732e-01,  4.70793396e-01,\n",
              "        6.89734161e-01,  3.35266948e-01,  1.35109490e-02,  1.81677327e-01,\n",
              "        1.14828115e-02,  2.12026909e-01, -4.27026451e-02,  7.48352930e-02,\n",
              "        3.87208909e-01, -2.20384464e-01,  1.25746951e-01,  6.88080564e-02,\n",
              "        2.01400682e-01,  6.31375611e-01,  1.60438456e-02,  3.20969433e-01,\n",
              "        4.90135878e-01,  9.52175818e-04, -9.77313295e-02, -2.22579315e-02,\n",
              "        7.47099146e-02,  1.19854867e+00,  3.41459721e-01,  2.16245666e-01,\n",
              "        1.62660524e-01,  3.58506531e-01,  2.32675299e-02,  1.65737197e-02,\n",
              "       -1.04860449e-02, -9.77261290e-02,  1.68499142e-01,  2.73515731e-01,\n",
              "        1.52725633e-03,  9.27861989e-01,  1.47039697e-01, -3.90163884e-02,\n",
              "        3.92070562e-01,  9.94422376e-01,  6.96903706e-01,  8.11628178e-02,\n",
              "       -1.16086463e-02,  5.27760983e-01,  4.46846485e-01,  1.75149068e-01,\n",
              "       -1.63045838e-01,  2.16223761e-01, -1.16083026e-01,  7.57717013e-01])"
            ]
          },
          "execution_count": 178,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#USING SKLEARN\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "MAHOMES_clf = MLPClassifier(learning_rate_init = 0.01, activation='relu', hidden_layer_sizes= (100,), alpha = 0.001 )\n",
        "\n",
        "#initialize dataloader with random sampling of size 10 \n",
        "dataset = dataLoader(X, y)\n",
        "trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle=True)\n",
        "\n",
        "#mlp init\n",
        "mlp = theiaMLP(init_features, 256, 1)\n",
        "#set loss function and gradient descet optimizer\n",
        "loss_function = nn.L1Loss()\n",
        "optimizer = torch.optim.Adagrad(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "#train for this many epochs\n",
        "for epoch in range(0,10):\n",
        "    print(f'Starting Epoch {epoch+1}')\n",
        "\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0], 1))\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = mlp(inputs)\n",
        "\n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        current_loss += loss.item()\n",
        "        if i%10 == 0:\n",
        "            print(f'Loss after mini-batch %5d: %.3f'%(i+1, current_loss/500))\n",
        "            current_loss = 0.0\n",
        "    \n",
        "    print(f'Epoch {epoch+1} done')\n",
        "\n",
        "\n",
        "test_data = torch.from_numpy(testX.to_numpy()).float()\n",
        "test_targets = torch.from_numpy(testY.to_numpy()).float()\n",
        "print(\"Test data correct outputs look like this\", test_targets)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "with torch.no_grad():\n",
        "    outputs = mlp(test_data)\n",
        "    predicted_labels = outputs.squeeze().tolist()\n",
        "\n",
        "predicted_labels = np.array(predicted_labels)\n",
        "test_targets = np.array(test_targets)\n",
        "\n",
        "mse = mean_squared_error(test_targets, predicted_labels)\n",
        "r2 = r2_score(test_targets, predicted_labels)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R2 Score:\", r2)\n",
        "#Results\n",
        "# Mean Squared Error: 0.10884078202201086\n",
        "# R2 Score: 0.508780183660541"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "#to set num samples variable for dataset\n",
        "num_samples = len(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.utils.data.sampler as sampler\n",
        "#Will use subsetRandomSampler (which assumes a shuffle=trfue data loading argument)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "#initialize dataloader with random sampling of size 10 \n",
        "dataset = dataLoader(X, y)\n",
        "trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=0, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "#mlp init\n",
        "mlp = MLP()\n",
        "#set loss function and gradient descet optimizer\n",
        "loss_function = nn.L1Loss()\n",
        "optimizer = torch.optim.Adagrad(mlp.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<enumerate at 0x13bf27470>"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#check that enumerate works\n",
        "enumerate(trainloader, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_result_metrics(alg, feat_set, prediction_df):\n",
        "    mcc = matthews_corrcoef(prediction_df['actual'], prediction_df['bool_pred'])\n",
        "    TN, FP, FN, TP = confusion_matrix(prediction_df['actual'], prediction_df['bool_pred']).ravel()\n",
        "\n",
        "    TPR=(TP/(TP+FN))*100\n",
        "    TNR=(TN/(TN+FP))*100\n",
        "    acc=((TP+TN)/(TP+TN+FP+FN))*100\n",
        "    Prec=(TP/(TP+FP))*100\n",
        "    return(pd.DataFrame([[alg, feat_set, acc, mcc, TPR, TNR, Prec]],\n",
        "        columns=['Algorithm', 'Feature Set', 'Accuracy', 'MCC', 'Recall', 'TrueNegRate', 'Precision']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#train for this many epochs\n",
        "for epoch in range(0,10):\n",
        "    print(f'Starting Epoch {epoch+1}')\n",
        "\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.float(), targets.float()\n",
        "        targets = targets.reshape((targets.shape[0], 1))\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = mlp(inputs)\n",
        "\n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        current_loss += loss.item()\n",
        "        if i%10 == 0:\n",
        "            print(f'Loss after mini-batch %5d: %.3f'%(i+1, current_loss/500))\n",
        "            current_loss = 0.0\n",
        "    \n",
        "    print(f'Epoch {epoch+1} done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test data correct outputs look like this tensor([1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n"
          ]
        }
      ],
      "source": [
        "test_data = torch.from_numpy(testX.to_numpy()).float()\n",
        "test_targets = torch.from_numpy(testY.to_numpy()).float()\n",
        "print(\"Test data correct outputs look like this\", test_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=181, out_features=181, bias=True)\n",
              "    (1): Sigmoid()\n",
              "    (2): Linear(in_features=181, out_features=90, bias=True)\n",
              "    (3): Sigmoid()\n",
              "    (4): Linear(in_features=90, out_features=1, bias=True)\n",
              "    (5): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Run mlp model on test data\n",
        "mlp.eval() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.21674117, 0.22062142, 0.21141651, 0.21115166, 0.19262204,\n",
              "       0.19279601, 0.22205086, 0.20826857, 0.21853697, 0.2159798 ,\n",
              "       0.21711464, 0.21479124, 0.20975791, 0.21770474, 0.20866387,\n",
              "       0.1990616 , 0.19156881, 0.1972757 , 0.22334646, 0.21868733,\n",
              "       0.2250876 , 0.22253193, 0.21718466, 0.22030586, 0.21602862,\n",
              "       0.21790211, 0.20592381, 0.2091603 , 0.21432683, 0.21131021,\n",
              "       0.21055853, 0.21681574, 0.21286863, 0.19677123, 0.19910997,\n",
              "       0.19341157, 0.20112966, 0.20306885, 0.19286221, 0.19625641,\n",
              "       0.22222982, 0.20823163, 0.21841706, 0.22265032, 0.21728173,\n",
              "       0.23444371, 0.23395725, 0.20475917, 0.20191696, 0.20894116,\n",
              "       0.2050696 , 0.21854565, 0.21890941, 0.21993507, 0.20849989,\n",
              "       0.22687183, 0.22544433, 0.23085712, 0.22505273, 0.22897832,\n",
              "       0.23596846, 0.2183799 , 0.22118898, 0.21605703, 0.21403074,\n",
              "       0.2239885 , 0.22787106, 0.20763335, 0.21379885, 0.20781212,\n",
              "       0.21181458, 0.21015316, 0.22662792, 0.22248079, 0.21807098,\n",
              "       0.21296626, 0.21988916, 0.23918423, 0.22747476, 0.23157838,\n",
              "       0.2328406 , 0.21709868, 0.21666956, 0.22361426, 0.22426204,\n",
              "       0.2207568 , 0.23323716, 0.23047772, 0.23270459, 0.21904443,\n",
              "       0.22395191, 0.22640745, 0.22885373, 0.22810686, 0.23548028,\n",
              "       0.23259053, 0.22506534, 0.22970369, 0.21672347, 0.20173611,\n",
              "       0.20372614, 0.22038811, 0.20029952, 0.20931882, 0.21867964,\n",
              "       0.21516737, 0.22944063, 0.21702605, 0.21430826, 0.22195105,\n",
              "       0.22575957, 0.20637961, 0.20333984, 0.20482972, 0.20149578,\n",
              "       0.19352126, 0.20958526, 0.21943714, 0.22282925, 0.22110288,\n",
              "       0.20948853, 0.21328343, 0.21253449, 0.20471676, 0.22130609,\n",
              "       0.23392551, 0.18577158, 0.22353214, 0.20859951, 0.22334689,\n",
              "       0.21502396, 0.21065694, 0.23659144, 0.22324921, 0.21831281,\n",
              "       0.21074039, 0.22465171, 0.2225447 , 0.22063944, 0.22751035,\n",
              "       0.19209948, 0.18624403, 0.18105306, 0.18851121, 0.21904068,\n",
              "       0.22174777, 0.21555041, 0.21811646, 0.21666628, 0.21180658,\n",
              "       0.23184018, 0.21120329, 0.19538562, 0.20380518, 0.22913937,\n",
              "       0.23677669, 0.22607367, 0.22491927, 0.20497426, 0.2234422 ,\n",
              "       0.21222229, 0.20743454, 0.21569522, 0.20829144, 0.21412787,\n",
              "       0.22229779, 0.21266556, 0.21282169, 0.21756449, 0.23817001,\n",
              "       0.21203595, 0.21321613, 0.20984536, 0.21096008, 0.2276302 ,\n",
              "       0.22312097, 0.21262453, 0.23398039, 0.21089855, 0.22116435,\n",
              "       0.2216298 , 0.21974194, 0.22061922, 0.22676623, 0.22433016,\n",
              "       0.21683502, 0.22036387, 0.21994045, 0.22263616, 0.22846794,\n",
              "       0.23083964, 0.22020714, 0.21277823, 0.2328129 , 0.22444715,\n",
              "       0.223992  , 0.21607031, 0.22554427, 0.2281286 , 0.19081028,\n",
              "       0.2048279 , 0.20670612, 0.21303624, 0.21030912, 0.20738941,\n",
              "       0.20194909, 0.20283014, 0.20495772, 0.19812714, 0.20042084,\n",
              "       0.20316462, 0.19131903, 0.19337206, 0.19995692, 0.20740524,\n",
              "       0.22387613, 0.20291206, 0.19721696, 0.21818548, 0.20155206,\n",
              "       0.2290682 , 0.2208019 , 0.19298272, 0.19410644, 0.1869344 ,\n",
              "       0.19848198, 0.20229012, 0.19791715, 0.19368419, 0.19536354,\n",
              "       0.20551924, 0.20500889, 0.21416436, 0.19428238, 0.19485287,\n",
              "       0.19060943, 0.20826149, 0.21921954, 0.1974075 , 0.19422483,\n",
              "       0.19642423, 0.19533695, 0.21114299, 0.19491905, 0.20164886,\n",
              "       0.21357514, 0.21949869, 0.22178745, 0.19770102, 0.21253245,\n",
              "       0.21570392, 0.19622055, 0.21077006, 0.20777613, 0.20013945,\n",
              "       0.2062562 , 0.18587202, 0.2056433 , 0.18894406, 0.19419098,\n",
              "       0.19659866, 0.19999158, 0.21721822, 0.22016475, 0.18930613,\n",
              "       0.19049686, 0.19993101, 0.1926239 , 0.21045153, 0.19969879,\n",
              "       0.20563209, 0.20961305, 0.19767565, 0.19492878, 0.2029483 ,\n",
              "       0.20970647, 0.22766203, 0.21343178, 0.19042645, 0.2046704 ,\n",
              "       0.19408526, 0.19511676, 0.19076335, 0.19522229, 0.20876147,\n",
              "       0.18877192, 0.19479376, 0.20473294, 0.18908639, 0.19744149,\n",
              "       0.19609803, 0.20415114, 0.18954939, 0.20635849, 0.21577196,\n",
              "       0.20594211, 0.19813925, 0.20731372, 0.21274428, 0.21095774,\n",
              "       0.19620585, 0.20357199, 0.19523206, 0.19475618, 0.19636838,\n",
              "       0.19719818, 0.1954201 , 0.203817  , 0.2010819 , 0.19011308,\n",
              "       0.19327015, 0.19552708, 0.19685005, 0.20005457, 0.191865  ,\n",
              "       0.19684391, 0.20677796, 0.19829462, 0.20410308, 0.20087324,\n",
              "       0.20438485, 0.19988857, 0.20086879, 0.21285699, 0.19775315,\n",
              "       0.20255633, 0.18740122, 0.19122173, 0.20322834, 0.19397067,\n",
              "       0.21468775, 0.20352925, 0.21769542, 0.2204199 , 0.21485905,\n",
              "       0.20481808, 0.20346147, 0.19045848, 0.1919582 , 0.20294641,\n",
              "       0.21061127, 0.20979948, 0.1933755 , 0.20633616, 0.19667704,\n",
              "       0.19482949, 0.21139827, 0.20108634, 0.19762759, 0.19921447,\n",
              "       0.18751793, 0.2028808 , 0.20866127, 0.19845003, 0.19689043,\n",
              "       0.20900178, 0.19398212, 0.20792888, 0.21869129, 0.20562683,\n",
              "       0.20254628, 0.20445195, 0.19397336, 0.19630797, 0.20739652,\n",
              "       0.19861023, 0.1944938 , 0.20583408, 0.20812523, 0.19158322,\n",
              "       0.19882931, 0.20886451, 0.20087229, 0.21063496, 0.20301865,\n",
              "       0.21688423, 0.21938384, 0.18815596, 0.19347374, 0.20062009,\n",
              "       0.20719141, 0.21413402, 0.21717724, 0.19586012, 0.19389722,\n",
              "       0.2298481 , 0.20044915, 0.19906648, 0.21123336, 0.20201102,\n",
              "       0.18909931, 0.19445916, 0.20581925, 0.18961035, 0.20798618,\n",
              "       0.20234556, 0.20806967, 0.22404709, 0.2220419 , 0.19080891,\n",
              "       0.19582167, 0.19712377, 0.20249218, 0.19787692, 0.18924187,\n",
              "       0.19555435, 0.18871811, 0.19831695, 0.19054377, 0.19566834,\n",
              "       0.2052166 , 0.21532874, 0.19544886, 0.19127151, 0.19473006,\n",
              "       0.20963205, 0.19187811, 0.20541987, 0.19563909, 0.18986464,\n",
              "       0.22765194, 0.19602321, 0.19558087, 0.19330613, 0.19161524,\n",
              "       0.18130466, 0.19986728, 0.19646457, 0.20209707, 0.20597708,\n",
              "       0.20819955, 0.20013143, 0.2044815 , 0.20670936, 0.20416191,\n",
              "       0.20062101, 0.19961567, 0.19231074, 0.20608021, 0.20601565,\n",
              "       0.20081294, 0.20570381, 0.21253756, 0.1991251 , 0.18853496,\n",
              "       0.2046303 , 0.18801932, 0.2038876 , 0.18502782, 0.20699829,\n",
              "       0.20537643, 0.20021938, 0.19427802, 0.2053272 , 0.20585991,\n",
              "       0.19968133, 0.1974837 , 0.21268207, 0.2107801 , 0.2063269 ,\n",
              "       0.21067798, 0.20003474, 0.19246741, 0.19570348, 0.2037438 ,\n",
              "       0.20887777, 0.19770943, 0.2156269 , 0.21877407, 0.20395529,\n",
              "       0.19972236, 0.1914534 , 0.19935112, 0.20478344, 0.19518675,\n",
              "       0.20069884, 0.20909953, 0.18616706, 0.20024817, 0.20846069,\n",
              "       0.20778586, 0.21854129, 0.20514494, 0.21535808, 0.21303126,\n",
              "       0.19796711, 0.18609838, 0.19647484, 0.19758682, 0.24184376,\n",
              "       0.21985887, 0.20462783, 0.20665245, 0.20652699, 0.20759971,\n",
              "       0.20424029, 0.1996114 , 0.18988638, 0.19337142, 0.21189038,\n",
              "       0.19656242, 0.23227732, 0.21148947, 0.20406459, 0.20791015,\n",
              "       0.22527915, 0.22414599, 0.20208438, 0.19377871, 0.21068135,\n",
              "       0.21867761, 0.20013055, 0.18928652, 0.20571715, 0.19932117,\n",
              "       0.21735181])"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 0.23740676504140348\n",
            "R2 Score: -0.07146333713221087\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "with torch.no_grad():\n",
        "    outputs = mlp(test_data)\n",
        "    predicted_labels = outputs.squeeze().tolist()\n",
        "\n",
        "predicted_labels = np.array(predicted_labels)\n",
        "test_targets = np.array(test_targets)\n",
        "\n",
        "mse = mean_squared_error(test_targets, predicted_labels)\n",
        "r2 = r2_score(test_targets, predicted_labels)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R2 Score:\", r2)\n",
        "# Mean Squared Error: 0.18271737790066617\n",
        "# R2 Score: 0.17536060337896386\n",
        "# So absolute crap, but it's a start"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
