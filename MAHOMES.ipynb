{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCmU7pa8mCUp"
      },
      "source": [
        "Following MAHOMES wrangling\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WN8yZ4tk4I97"
      },
      "outputs": [],
      "source": [
        "#GetFeatureSet.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Sample features and remove features that are not a part of the all-category, mean sphere feature set\n",
        "def feature_subset(df, subset, other_bad_terms = [], noBSA=False):\n",
        "    X = df.copy()\n",
        "    X = X.fillna(0)\n",
        "    not_needed = (\"Catalytic\", \"SITE_ID\", \"Set\", \"ValidSet\", 'NewSet', 'cath_class', 'cath_arch', 'scop_class', 'scop_fold', 'ECOD_arch', 'ECOD_x_poshom', 'ECOD_hom')\n",
        "    X = X.drop(columns = [term for term in X if term.startswith(not_needed)])\n",
        "    bad_terms = (\"hbond_lr_\", 'dslf_fa13', 'pro_close', 'ref', 'fa_sol_', 'MetalCodes', 'MetalAtoms', 'SEPocket', 'geom_gRMSD', 'geom_MaxgRMSDDev', 'geom_AtomRMSD')\n",
        "    X = X.drop(columns = [term for term in X if term.startswith(bad_terms)])\n",
        "    #print(X.shape)#, list(X))\n",
        "    #general terms\n",
        "    gen_set = ['Depth', 'Vol', \"SITEDistCenter\", \"SITEDistNormCenter\"]\n",
        "    gen_terms = (\"BSA\", 'SASA')\n",
        "    all_gen_set = [ term for term in X if term.startswith(gen_terms) ]\n",
        "    gen_shell = [name for name in all_gen_set if \"_S\" in name]\n",
        "    gen_sph = list(set(all_gen_set).difference(gen_shell))\n",
        "    gen_shell += gen_set\n",
        "    gen_shell += [\"BSA_3.5\", \"SASA_3.5\"]\n",
        "    gen_sph += gen_set\n",
        "    all_gen_set += gen_set\n",
        "    all_gen_set = sorted(set(all_gen_set))\n",
        "    #Rosetta terms only\n",
        "    ros_sum_sph0 = list(set([name for name in X if name.endswith(\"_Sum_3.5\")]).difference(all_gen_set))\n",
        "    ros_sum_sph1 = list(set([ name for name in X if name.endswith(\"_Sum_5\") ]).difference(all_gen_set))\n",
        "    ros_sum_sph2 = list(set([ name for name in X if name.endswith(\"_Sum_7.5\") ]).difference(all_gen_set))\n",
        "    ros_sum_sph3 = list(set([ name for name in X if name.endswith(\"_Sum_9\") ]).difference(all_gen_set))\n",
        "    ros_sum_shell1 = list(set([ name for name in X if name.endswith(\"_Sum_S5\") ]).difference(all_gen_set))\n",
        "    ros_sum_shell2 = list(set([ name for name in X if name.endswith(\"_Sum_S7.5\") ]).difference(all_gen_set))\n",
        "    ros_sum_shell3 = list(set([ name for name in X if name.endswith(\"_Sum_S9\") ]).difference(all_gen_set))\n",
        "    ros_sum_shell = ros_sum_sph0 + ros_sum_shell1 + ros_sum_shell2 + ros_sum_shell3\n",
        "    ros_sum_sph = ros_sum_sph0 + ros_sum_sph1 + ros_sum_sph2 + ros_sum_sph3\n",
        "\n",
        "    ros_mean_sph0 = list(set([name for name in X if name.endswith(\"_Mean_3.5\")]).difference(all_gen_set))\n",
        "    ros_mean_sph1 = list(set([ name for name in X if name.endswith(\"_Mean_5\") ]).difference(all_gen_set))\n",
        "    ros_mean_sph2 = list(set([ name for name in X if name.endswith(\"_Mean_7.5\") ]).difference(all_gen_set))\n",
        "    ros_mean_sph3 = list(set([ name for name in X if name.endswith(\"_Mean_9\") ]).difference(all_gen_set))\n",
        "    ros_mean_shell1 = list(set([ name for name in X if name.endswith(\"_Mean_S5\") ]).difference(all_gen_set))\n",
        "    ros_mean_shell2 = list(set([ name for name in X if name.endswith(\"_Mean_S7.5\") ]).difference(all_gen_set))\n",
        "    ros_mean_shell3 = list(set([ name for name in X if name.endswith(\"_Mean_S9\") ]).difference(all_gen_set))\n",
        "    ros_mean_shell = ros_mean_sph0 + ros_mean_shell1 + ros_mean_shell2 + ros_mean_shell3\n",
        "    ros_mean_sph = ros_mean_sph0 + ros_mean_sph1 + ros_mean_sph2 + ros_mean_sph3\n",
        "\n",
        "    electro = [name for name in X if name.startswith(\"Elec\")]\n",
        "    geom = [name for name in X if name.startswith(\"geom\")]\n",
        "    findgeo_geoms = (\"lin\", \"trv\", \"tri\", \"tev\", \"spv\",\n",
        "        \"tet\", \"spl\", \"bva\", \"bvp\", \"pyv\",\n",
        "        \"spy\", \"tbp\", \"tpv\",\n",
        "        \"oct\", \"tpr\", \"pva\", \"pvp\", \"cof\", \"con\", \"ctf\", \"ctn\",\n",
        "        \"pbp\", \"coc\", \"ctp\", \"hva\", \"hvp\", \"cuv\", \"sav\",\n",
        "        \"hbp\", \"cub\", \"sqa\", \"boc\", \"bts\", \"btt\",\n",
        "        \"ttp\", \"csa\")\n",
        "    geom = [name for name in geom if not name.endswith(findgeo_geoms)] #remove the individual geom types\n",
        "    #pocket features only\n",
        "    pocket_set = ['Depth', 'Vol', \"SITEDistCenter\", \"SITEDistNormCenter\", 'LongPath', 'farPtLow', 'PocketAreaLow', 'OffsetLow', 'LongAxLow', 'ShortAxLow', 'farPtMid', 'PocketAreaMid', 'OffsetMid', 'LongAxMid', 'ShortAxMid', 'farPtHigh', 'PocketAreaHigh', 'OffsetHigh', 'LongAxHigh', 'ShortAxHigh']\n",
        "    pocket_set = list(set(pocket_set).difference(other_bad_terms))\n",
        "    #pocket lining only\n",
        "    lining_set = ['num_pocket_bb', 'num_pocket_sc', 'avg_eisen_hp', 'min_eisen', 'max_eisen', 'skew_eisen', 'std_dev_eisen', 'avg_kyte_hp', 'min_kyte', 'max_kyte', 'skew_kyte', 'std_dev_kyte', 'occ_vol', 'NoSC_vol', 'SC_vol_perc', 'LiningArea']\n",
        "    lining_set = list(set(lining_set).difference(other_bad_terms))\n",
        "\n",
        "    #print( len(all_gen_set), len(sorted(set(ros_sum_sph+ros_sum_shell+ros_mean_shell+ros_mean_sph))), len(electro), len(geom), len(pocket_set), len(lining_set))\n",
        "    #print(len(sorted(set(ros_sum_sph+ros_sum_shell+ros_mean_shell+ros_mean_sph+all_gen_set+electro+geom+pocket_set+lining_set))))\n",
        "\n",
        "    subset_list = [\"AllSumSph\", \"AllMeanSph\", \"AllSumShell\", \"AllMeanShell\",\n",
        "                    \"GenSph\", \"GenShell\", \"Pocket\", \"Lining\",\n",
        "                    'RosSumSph', 'RosSumSph0', 'RosSumSph1', 'RosMeanSph', 'RosMeanSph0', 'RosMeanSph1', \"RosSumSphInner2\", \"RosMeanSphInner2\",\n",
        "                    'RosSumShell', 'RosSumShell1', 'RosMeanShell', 'RosMeanShell1',\"RosSumShellInner2\", \"RosMeanShellInner2\",\n",
        "                    \"LinPocket\", \"LinRosSumSph\", \"LinRosMeanSph\", \"LinRosSumShell\", \"LinRosMeanShell\",\n",
        "                    \"PocketRosSumSph\", \"PocketRosMeanSph\", \"PocketRosSumShell\", \"PocketRosMeanShell\",\n",
        "                    \"Geom\", \"LinPocketGeom\", \"GeomElectro\", \"GeomRosSumSph\", \"GeomRosSumShell\", \"GeomRosMeanSph\", \"GeomRosMeanShell\",\n",
        "\n",
        "                    \"Electro\", \"LinPocketElectro\", \"LinPocketElectroGeom\", \"ElectroRosSumSph\", \"ElectroRosSumShell\", \"ElectroRosMeanSph\", \"ElectroRosMeanShell\",\n",
        "                    \"AllSumSphMinusGen\", \"AllSumSphMinusLin\", \"AllSumSphMinusPocket\",\n",
        "                    \"AllSumSphMinusGeom\", \"AllSumSphMinusElectro\",\n",
        "                    \"AllMeanSphMinusGen\", \"AllMeanSphMinusLin\", \"AllMeanSphMinusPocket\",\n",
        "                    \"AllMeanSphMinusGeom\", \"AllMeanSphMinusElectro\", \"AllMinusRosSph\",\n",
        "\n",
        "                    \"AllSumShellMinusGen\", \"AllSumShellMinusLin\", \"AllSumShellMinusPocket\",\n",
        "                    \"AllSumShellMinusGeom\", \"AllSumShellMinusElectro\",\n",
        "                    \"AllMeanShellMinusGen\", \"AllMeanShellMinusLin\", \"AllMeanShellMinusPocket\",\n",
        "                    \"AllMeanShellMinusGeom\", \"AllMeanShellMinusElectro\", \"AllMinusRosShell\",\n",
        "                    ]\n",
        "    column_subsets = [  sorted(set(gen_sph+ros_sum_sph+pocket_set+lining_set+electro+geom)),#AllSumSph  GSP\n",
        "                        sorted(set(gen_shell+ros_mean_sph+pocket_set+lining_set+electro+geom)),#AllMeanSph  GSP\n",
        "                        sorted(set(gen_sph+ros_sum_shell+pocket_set+lining_set+electro+geom)),#AllSumShell  GSH\n",
        "                        sorted(set(gen_shell+ros_mean_shell+pocket_set+lining_set+electro+geom)), #AllMeanShell GSH\n",
        "                        gen_sph,#GenSph GSH\n",
        "                        gen_shell,#GenShell GPH\n",
        "\n",
        "                        pocket_set,#Pocket\n",
        "                        lining_set,#Lining\n",
        "                        ros_sum_sph,#RosSumSph\n",
        "                        ros_sum_sph0,#RosSumSph0\n",
        "                        ros_sum_sph1,#RosSumSph1\n",
        "                        ros_mean_sph,#RosMeanSph\n",
        "                        ros_mean_sph0,#RosMeanSph0\n",
        "                        ros_mean_sph1,#RosMeanSph1\n",
        "                        sorted(set(ros_sum_sph0+ros_sum_sph1)),#RosSumSphInner2\n",
        "                        sorted(set(ros_mean_sph0+ros_mean_sph1)),#RosMeanSphInner2\n",
        "                        ros_sum_shell, #RosSumShell\n",
        "                        ros_sum_shell1, #RosSumShell1\n",
        "                        ros_mean_shell, #RosMeanShell\n",
        "                        ros_mean_shell1, #RosMeanShell1\n",
        "                        sorted(set(ros_sum_sph0 + ros_sum_shell1)), #RosSumShellInner2\n",
        "                        sorted(set(ros_mean_sph0 + ros_mean_shell1)), #RosMeanShellInner2\n",
        "                        lining_set+pocket_set, #LinPocket\n",
        "                        lining_set+ros_sum_sph, #LinRosSumSph\n",
        "                        lining_set+ros_mean_sph, #LinRosMeanSph\n",
        "                        lining_set+ros_sum_shell, #LinRosSumShell\n",
        "                        lining_set+ros_mean_shell, #LinRosMeanShell\n",
        "                        pocket_set+ros_sum_sph, #PocketRosSumSph\n",
        "                        pocket_set+ros_mean_sph, #PocketRosMeanSph\n",
        "                        pocket_set+ros_sum_shell, #PocketRosSumShell\n",
        "                        pocket_set+ros_mean_shell, #PocketRosMeanShell\n",
        "                        geom, #Geom\n",
        "                        lining_set+pocket_set+geom, #LinPocketGeom\n",
        "                        geom+electro, #GeomElectro\n",
        "                        geom+ros_sum_sph, #GeomRosSumSph\n",
        "                        geom+ros_sum_shell,#GeomRosSumShell\n",
        "                        geom+ros_mean_sph, #GeomRosMeanSph\n",
        "                        geom+ros_mean_shell,#GeomRosMeanShell\n",
        "\n",
        "                        electro,#Electro\n",
        "                        lining_set+pocket_set+electro,#LinPocketElectro\n",
        "                        lining_set+pocket_set+electro+geom,#LinPocketElectroGeom\n",
        "                        electro+ros_sum_sph,#ElectroRosSumSph\n",
        "                        electro+ros_sum_shell,#ElectroRosSumShell\n",
        "                        electro+ros_mean_sph,#ElectroRosMeanSph\n",
        "                        electro+ros_mean_shell,#ElectroRosMeanShell\n",
        "                        sorted(set(ros_sum_sph+pocket_set+lining_set+electro+geom)),#AllSumSphMinusGen\n",
        "                        sorted(set(gen_sph+ros_sum_sph+pocket_set+electro+geom)),#AllSumSphMinusLin   GSP\n",
        "                        sorted(set(gen_sph+ros_sum_sph+lining_set+electro+geom)),#AllSumSphMinusPocket  GSP\n",
        "                        sorted(set(gen_sph+ros_sum_sph+pocket_set+lining_set+electro)),#AllSumSphMinusGeom  GSP\n",
        "                        sorted(set(gen_sph+ros_sum_sph+pocket_set+lining_set+geom)), #AllSumSphMinusElectro  GSP\n",
        "                        sorted(set(ros_mean_sph+pocket_set+lining_set+electro+geom)),#AllMeanSphMinusGen\n",
        "                        sorted(set(gen_sph+ros_mean_sph+pocket_set+electro+geom)),#AllMeanSphMinusLin   GSP\n",
        "                        sorted(set(gen_sph+ros_mean_sph+lining_set+electro+geom)),#AllMeanSphMinusPocket  GSP\n",
        "                        sorted(set(gen_sph+ros_mean_sph+pocket_set+lining_set+electro)),#AllMeanSphMinusGeom  GSP\n",
        "                        sorted(set(gen_sph+ros_mean_sph+pocket_set+lining_set+geom)),#AllMeanSphMinusElectro  GSP\n",
        "                        sorted(set(gen_sph+pocket_set+lining_set+electro+geom)),#AllMinusRosSph  GSP\n",
        "\n",
        "                        sorted(set(ros_sum_shell+pocket_set+lining_set+electro+geom)),#AllSumShellMinusGen\n",
        "                        sorted(set(gen_shell+ros_sum_shell+pocket_set+electro+geom)),#AllSumShellMinusLin  GSH\n",
        "                        sorted(set(gen_shell+ros_sum_shell+lining_set+electro+geom)),#AllSumShellMinusPocket  GSH\n",
        "                        sorted(set(gen_shell+ros_sum_shell+pocket_set+lining_set+electro)),#AllSumShellMinusGeom  GSH\n",
        "                        sorted(set(gen_shell+ros_sum_shell+pocket_set+lining_set+geom)), #AllSumShellMinusElectro  GSH\n",
        "                        sorted(set(ros_mean_shell+pocket_set+lining_set+electro+geom)),#AllMeanShellMinusGen\n",
        "                        sorted(set(gen_shell+ros_mean_shell+pocket_set+electro+geom)),#AllMeanShellMinusLin  GSH\n",
        "                        sorted(set(gen_shell+ros_mean_shell+lining_set+electro+geom)),#AllMeanShellMinusPocket  GSH\n",
        "                        sorted(set(gen_shell+ros_mean_shell+pocket_set+lining_set+electro)), #AllMeanShellMinusGeom  GSH\n",
        "                        sorted(set(gen_shell+ros_mean_shell+pocket_set+lining_set+geom)),#AllMeanShellMinusElectro   GSH\n",
        "                        sorted(set(gen_shell+pocket_set+lining_set+electro+geom)),#AllMinusRosShell  GSH\n",
        "                        ]\n",
        "    #print(column_subsets[subset_list.index(data_subset)] )\n",
        "    if subset in subset_list:\n",
        "        X = X[ column_subsets[subset_list.index(subset)] ]\n",
        "    else:\n",
        "        print(\"Not a subset in list; defaulting to AllSph\")\n",
        "        X = X[ column_subsets[0] ] #this is all for usage with PCA/UMAP; it uses the rosetta sphere terms plus all the non-rosetta terms\n",
        "    if 'groupID' in df.columns:\n",
        "        X=pd.merge(X,df['groupID'], left_index=True, right_index=True)\n",
        "    ## added to remove BSA terms for undersampling DataSet using BSA\n",
        "    if noBSA==True:\n",
        "        X = X.drop(columns = [term for term in X if term.startswith(\"BSA\")])\n",
        "        X = X.drop(columns = [term for term in X if term.startswith(\"SASA\")])\n",
        "    return(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ocFCnJwmAlw",
        "outputId": "84f3b6a3-5683-456b-bed6-21ce1234983f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mHello World !\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# libraries\n",
        "import joblib\n",
        "\n",
        "# scale features\n",
        "from sklearn import preprocessing\n",
        "from sklearn import impute\n",
        "# classifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "# scoring metrics\n",
        "from sklearn.metrics import confusion_matrix, matthews_corrcoef\n",
        "\n",
        "# custom scripts\n",
        "import sys\n",
        "sys.path.insert(0, \"%s\" % \"CV/\")\n",
        "\n",
        "# allow fancey printed strings\n",
        "# from https://stackoverflow.com/questions/8924173/how-do-i-print-bold-text-in-python/8930747\n",
        "class color:\n",
        "    PURPLE = '\\033[95m'\n",
        "    CYAN = '\\033[96m'\n",
        "    DARKCYAN = '\\033[36m'\n",
        "    BLUE = '\\033[94m'\n",
        "    GREEN = '\\033[92m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    RED = '\\033[91m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "    END = '\\033[0m'\n",
        "\n",
        "print(color.BOLD + 'Hello World !' + color.END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oDWizisk4QbB"
      },
      "outputs": [],
      "source": [
        "#path to file is drive/MyDrive/csc334/MAHOMES_MLP/sites_calculated_features.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu4-hjpH3xVI",
        "outputId": "a524142b-f693-45fc-e816-6be1d74cd1cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mAll features:\u001b[0m\n",
            "sites: 3981 \tcolumns: 485\n",
            "Set   Catalytic\n",
            "data  False        2636\n",
            "      True          829\n",
            "test  False         345\n",
            "      True          171\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#read in feature set:\n",
        "sites = pd.read_csv(\"/Users/sanjanayasna/csc334/MLP_MAHOMES/data/sites_calculated_features.txt\")\n",
        "sites = sites.set_index('SITE_ID',drop=True)\n",
        "\n",
        "# The following labels need to be changed after looking over literature (see Feehan, Franklin, Slusky 2021)\n",
        "change_site_labels = [\"5zb8_0\", \"6aci_0\", \"6oq7_0\", \"6pjv_1\", \"6q55_0\",\n",
        "                      \"6q55_2\", \"6rmg_0\", \"6rtg_0\", \"6rw0_0\", \"6v77_0\"]\n",
        "# The following sites are removed due to unkopwn correct labels (see Feehan, Franklin, Slusky 2021)\n",
        "sites.loc[sites.index.isin(change_site_labels), 'Catalytic']=True\n",
        "remove_sites = [\"6mf0_1\", \"6okh_0\", \"6qwo_0\", \"6r9n_0\"]\n",
        "sites=sites.loc[~sites.index.isin(remove_sites)]\n",
        "\n",
        "print(color.BOLD + \"All features:\" + color.END)\n",
        "print(\"sites: %s \\tcolumns: %s\"%(sites.shape[0], sites.shape[1]))\n",
        "sizes = sites.groupby([\"Set\", \"Catalytic\"]).size()\n",
        "print(sizes)\n",
        "\n",
        "#Output:\n",
        "# All features:\n",
        "# sites: 3981 \tcolumns: 485\n",
        "# Set   Catalytic\n",
        "# data  False        2636\n",
        "#       True          829\n",
        "# test  False         345\n",
        "#       True          171\n",
        "# dtype: int64\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VKicxMMw5vP8"
      },
      "outputs": [],
      "source": [
        "#save_models toggel\n",
        "save_models = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2dTgTJHEEp7L"
      },
      "outputs": [],
      "source": [
        "#sites has a .Set value of either data or test (used for testing)\n",
        "#sites.Set data values will become training set, and test labelled values will be test set\n",
        "#categorical data will be normalized to values between 0 and 1 (so final test and training set all numerical representation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9zUNDRgJ54l1"
      },
      "outputs": [],
      "source": [
        "#list current director/folder for file output\n",
        "pkl_out = \"/Users/sanjanayasna/csc334/MLP_MAHOMES/pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtXf1l9g5RKV",
        "outputId": "72781cfd-ae11-410c-b08d-633a0be85752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mAll scaled data-set features:\u001b[0m\n",
            "sites: 3465 \tcolumns: 484\n",
            "Catalytic\n",
            "False    2636\n",
            "True      829\n",
            "dtype: int64\n",
            "\u001b[1m\n",
            "All scaled T-metal-site features:\u001b[0m\n",
            "sites: 516 \tcolumns: 484\n",
            "Catalytic\n",
            "False    345\n",
            "True     171\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#Normalization:\n",
        "import os\n",
        "def get_scaled_features():\n",
        "    #change to pkl_out folder directory\n",
        "    os.chdir(pkl_out)\n",
        "    # seperate the sets (only dataset will be used to set scaling)\n",
        "    data = sites.loc[sites.Set == \"data\"].copy()\n",
        "    Tsites = sites.loc[sites.Set == \"test\"].copy()\n",
        "\n",
        "    #split for scaling into categorical and not categorical\n",
        "    not_ctg_geom = (\"geom_gRMSD\", \"geom_MaxgRMSDDev\",\"geom_val\", \"geom_nVESCUM\",\"geom_AtomRMSD\", \"geom_AvgO\", \"geom_AvgN\", \"geom_AvgS\", \"geom_AvgOther\", \"geom_Charge\")\n",
        "    geom = [name for name in data if name.startswith(\"geom\")]\n",
        "\n",
        "    ctg_data = [x for x in geom if not x in not_ctg_geom]\n",
        "    ctg_data.extend([\"Set\", 'Catalytic'])\n",
        "\n",
        "    ## scale cont. features\n",
        "    cont_scaler = preprocessing.RobustScaler(quantile_range=(20,80))\n",
        "    #Fit scaler to X, then transform it\n",
        "    data_nonctg = data[data.columns.difference(ctg_data)]#so that I can have columns\n",
        "    data_scaled = pd.DataFrame(cont_scaler.fit_transform(data_nonctg), columns=data_nonctg.columns, index=data_nonctg.index)\n",
        "\n",
        "    #scale the test set based on the scale of the training set\n",
        "    Tsites_nonctg  = Tsites[Tsites.columns.difference(ctg_data)]\n",
        "    Tsites_scaled = pd.DataFrame(cont_scaler.transform(Tsites_nonctg), columns=Tsites_nonctg.columns, index=Tsites_nonctg.index)\n",
        "\n",
        "    #replace continuous feature null values with mean\n",
        "    cont_imputer = impute.SimpleImputer(strategy=\"mean\")\n",
        "    data_scaled = pd.DataFrame(cont_imputer.fit_transform(data_scaled), columns=data_scaled.columns, index=data_scaled.index)\n",
        "    Tsites_scaled = pd.DataFrame(cont_imputer.transform(Tsites_scaled), columns=Tsites_scaled.columns, index=Tsites_scaled.index)\n",
        "\n",
        "    if save_models==True:\n",
        "      joblib.dump(cont_scaler, \"/ContVarScaler.pkl\")\n",
        "      joblib.dump(cont_imputer, \"/ContVarImpute.pkl\")\n",
        "\n",
        "    #remove groupID and target value Catalytic so that it also isn't MinMax scaled either\n",
        "    ctg_data.remove(\"Set\");ctg_data.remove(\"Catalytic\")\n",
        "    #transform categorical data to [0,1] interval using fit_transform (StandardScaler), and then imputer for null values\n",
        "    if len(data.columns.intersection(ctg_data)) > 0:\n",
        "        ctg_scaler = preprocessing.MinMaxScaler()\n",
        "\n",
        "        # fit the scaler to the data-set (training) and scale\n",
        "        data_ctg = data[data.columns.intersection(ctg_data)]\n",
        "        data_ctg_scaled = pd.DataFrame(ctg_scaler.fit_transform(data_ctg), columns=data_ctg.columns, index=data_ctg.index)\n",
        "\n",
        "        #scale the test set based on the scale of the training set\n",
        "        Tsites_ctg = Tsites[Tsites.columns.intersection(ctg_data)]\n",
        "        Tsites_ctg_scaled = pd.DataFrame(ctg_scaler.transform(Tsites_ctg), columns=Tsites_ctg.columns, index=Tsites_ctg.index)\n",
        "\n",
        "        #replace categoric features null values with median value\n",
        "        ctg_imputer = impute.SimpleImputer(strategy=\"median\")\n",
        "        data_ctg_scaled = pd.DataFrame(ctg_imputer.fit_transform(data_ctg_scaled), columns=data_ctg_scaled.columns, index=data_ctg_scaled.index)\n",
        "        Tsites_ctg_scaled = pd.DataFrame(ctg_imputer.transform(Tsites_ctg_scaled), columns=Tsites_ctg_scaled.columns, index=Tsites_ctg_scaled.index)\n",
        "\n",
        "        #concatenate the scaled categorical data to the robustly scaled data\n",
        "        data_scaled = pd.merge(data_scaled, data_ctg_scaled, left_index=True, right_index=True)\n",
        "        Tsites_scaled = pd.merge(Tsites_scaled, Tsites_ctg_scaled, left_index=True, right_index=True)\n",
        "\n",
        "        if save_models==True:\n",
        "            joblib.dump(ctg_scaler, \"/CtgVarScaler.pkl\")\n",
        "            joblib.dump(ctg_imputer, \"/CtgVarImpute.pkl\")\n",
        "    #add back the Catalytic column\n",
        "    data_scaled = pd.merge(data_scaled, data['Catalytic'], left_index=True, right_index=True)\n",
        "    Tsites_scaled = pd.merge(Tsites_scaled, Tsites['Catalytic'], left_index=True, right_index=True)\n",
        "\n",
        "    return(data_scaled, Tsites_scaled)\n",
        "\n",
        "data_scaled, Tsites_scaled = get_scaled_features()\n",
        "\n",
        "print(color.BOLD + \"All scaled data-set features:\" + color.END)\n",
        "print(\"sites: %s \\tcolumns: %s\"%(data_scaled.shape[0], data_scaled.shape[1]))\n",
        "print(data_scaled.groupby([\"Catalytic\"]).size())\n",
        "\n",
        "print(color.BOLD + \"\\nAll scaled T-metal-site features:\" + color.END)\n",
        "print(\"sites: %s \\tcolumns: %s\"%(Tsites_scaled.shape[0], Tsites_scaled.shape[1]))\n",
        "print(Tsites_scaled.groupby([\"Catalytic\"]).size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GUsvVukY7D3x"
      },
      "outputs": [],
      "source": [
        "## returns relevent data-set data for training ML model\n",
        "def get_training_data(feature_set, random_seed):\n",
        "    ## random under sample data-set (1+:3-)\n",
        "    X_Cat = data_scaled[data_scaled['Catalytic']==True].copy()\n",
        "    X_nonCat = data_scaled[data_scaled['Catalytic']==False].copy()\n",
        "    #sample X_nonCat so that it is 3 parts non-Catalytic to 1 part Catalytic data\n",
        "    #NOTE: allow reuse of samples (duplicate sites, possibly) to avoid issue of sample size errors if the catalytic site is less than 3 times the non catalytic site count.\n",
        "    #Make sure to toggle replace to false if preferred\n",
        "    X_nonCat = X_nonCat.sample(n=len(X_Cat)*3, axis=0, random_state=random_seed, replace = True)\n",
        "    X_prep = pd.concat([X_Cat, X_nonCat], axis = 0)\n",
        "   # X_Cat.append(X_nonCat)\n",
        "\n",
        "    ## seperate target value\n",
        "    y = X_prep['Catalytic']; del X_prep['Catalytic']\n",
        "\n",
        "    ## only return features in specific feature set\n",
        "    X = feature_subset(X_prep, feature_set, noBSA=True)\n",
        "\n",
        "    return(X, y)\n",
        "\n",
        "## number of iterations to improve reproducability\n",
        "num_rand_seeds = 10 # 10 provides 3 decimal level reproducability across my machines)\n",
        "def evaluate_model_with_Tsite(clf, feature_set):\n",
        "    ## prepare test-set\n",
        "    testX = Tsites_scaled.copy();\n",
        "    testY = testX['Catalytic']; del testX['Catalytic']\n",
        "    testX = feature_subset(testX, feature_set, noBSA=True)\n",
        "\n",
        "    ## get multiple predictions for test-set w/ diff random seeds\n",
        "    test_site_preds = {'actual': pd.Series(testY, index=testX.index)}\n",
        "    for rand_seed in range(0,num_rand_seeds):\n",
        "        # get undersampled training data for feature set\n",
        "        X, y = get_training_data(feature_set, rand_seed)\n",
        "        print(\"random_seed = %s\"%(rand_seed), end=\"\\t\")\n",
        "        print(\"(num. training sites= %s (%s+ : %s-) \\tnum. features: %s)\"%(X.shape[0], len(y[y==True]),len(y[y==False]), X.shape[1]))\n",
        "\n",
        "        ## train classifier and make test-set predictions (alreacy put in random seed when doing hte relevant data sampling and splitting in the @get_training_data)\n",
        "        #The model itself has a random state of 0 - 9 for each of htese iterations\n",
        "        clf.set_params(random_state=rand_seed)\n",
        "        #fit in training set with random seed\n",
        "        clf.fit(X, y)\n",
        "        #predict with test-set input\n",
        "        test_preds = clf.predict(testX)\n",
        "        test_site_preds['prediction_%s'%(rand_seed)]= pd.Series(test_preds, index=testX.index)\n",
        "        if save_models==True:\n",
        "            joblib.dump(clf, \"/MAHOMES%s.pkl\"%(rand_seed))\n",
        "\n",
        "        ## output results for this random seed to get an idea of prediction variation levels\n",
        "        TN, FP, FN, TP = confusion_matrix(testY, test_preds).ravel()\n",
        "        mcc = matthews_corrcoef(testY, test_preds)\n",
        "        print(\"\\tTP=%s \\tTN=%s \\tFP=%s \\tFN=%s\"%(TP, TN, FP, FN))\n",
        "\n",
        "    ## calcualte the average of all random seed predictions\n",
        "    test_predictions = pd.DataFrame(test_site_preds)\n",
        "    test_predictions['prediction']=0\n",
        "    for rand_seed in range(0,num_rand_seeds):\n",
        "        test_predictions['prediction']+=test_predictions['prediction_%s'%(rand_seed)]\n",
        "    test_predictions['prediction']=test_predictions['prediction']/num_rand_seeds\n",
        "\n",
        "    ## make final prediction\n",
        "    test_predictions['bool_pred']=False\n",
        "    test_predictions.loc[test_predictions['prediction']>=0.5, 'bool_pred']=True\n",
        "\n",
        "    return(test_predictions)\n",
        "\n",
        "## return result metrics for final predictions\n",
        "def check_result_metrics(alg, feat_set, prediction_df):\n",
        "    mcc = matthews_corrcoef(prediction_df['actual'], prediction_df['bool_pred'])\n",
        "    TN, FP, FN, TP = confusion_matrix(prediction_df['actual'], prediction_df['bool_pred']).ravel()\n",
        "\n",
        "    TPR=(TP/(TP+FN))*100\n",
        "    TNR=(TN/(TN+FP))*100\n",
        "    acc=((TP+TN)/(TP+TN+FP+FN))*100\n",
        "    Prec=(TP/(TP+FP))*100\n",
        "    return(pd.DataFrame([[alg, feat_set, acc, mcc, TPR, TNR, Prec]],\n",
        "        columns=['Algorithm', 'Feature Set', 'Accuracy', 'MCC', 'Recall', 'TrueNegRate', 'Precision']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppfAtUeDmxP2"
      },
      "source": [
        "Following ideal params script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yjTF8TR_-DkQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "# specify algorithm and feature set for MAHOMES (top model from outer CV results)\n",
        "MAHOMES_alg = \"NeurNet\" #MLP classifier\n",
        "MAHOMES_feature_set = \"AllMeanSph\"\n",
        "## set extra trees classifier to suggezted params @https://github.com/SluskyLab/MAHOMES/blob/main/CV/MLwGrid.py\n",
        "MAHOMES_clf = MLPClassifier(learning_rate_init = 0.01, activation='relu')\n",
        "additional_params =  {\"hidden_layer_sizes\": [(50,), (100,), (200,)], \"alpha\": [0.1, 0.01, 0.001]  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "D-P_bdnqmvyc"
      },
      "outputs": [],
      "source": [
        "#Training data\n",
        "X, y = get_training_data(MAHOMES_feature_set, random_seed = 1)\n",
        " ## prepare test-set\n",
        "testX = Tsites_scaled.copy()\n",
        "testY = testX['Catalytic']; del testX['Catalytic']\n",
        "testX = feature_subset(testX, MAHOMES_feature_set, noBSA=True)\n",
        "\n",
        "## get multiple predictions for test-set w/ diff random seeds\n",
        "test_site_preds = {'actual': pd.Series(testY, index=testX.index)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "S-oH7TxdnT-b"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, GroupShuffleSplit, StratifiedShuffleSplit, cross_validate, StratifiedKFold\n",
        "from sklearn.metrics import roc_curve, auc, recall_score, accuracy_score, precision_score, confusion_matrix, make_scorer, matthews_corrcoef, jaccard_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_Mnzkm7qnc90"
      },
      "outputs": [],
      "source": [
        "#prepare jobs and cv\n",
        "num_jobs = 15\n",
        "inner_cv_type = StratifiedShuffleSplit(n_splits=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bML_8G6Xn4Sx"
      },
      "outputs": [],
      "source": [
        "#prepare output\n",
        "outer_cv_type=StratifiedKFold(n_splits=7)\n",
        "outer_cv_results = []\n",
        "outer_coeffs = []\n",
        "outer_params = []\n",
        "outer_feat_imp = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sSGlvvQtoRxu"
      },
      "outputs": [],
      "source": [
        "#accuracy metrics:\n",
        "def prec_score_custom(y_true, y_pred, this_label = True):\n",
        "    return( precision_score(y_true, y_pred, pos_label= this_label) )\n",
        "def mcc_score(y_true, y_pred):\n",
        "    return( matthews_corrcoef(y_true, y_pred))\n",
        "def jac_score(y_true, y_pred, this_label = True):\n",
        "    return( jaccard_score(y_true, y_pred, pos_label=this_label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aiwugdQfd0C"
      },
      "outputs": [],
      "source": [
        "#main loop that outputs ieal combos in outer_params, and prints accuracies associated with such\n",
        "for i, (train_idx, test_idx) in enumerate(outer_cv_type.split(X,y)):\n",
        "    print(\"OUTER LOOP NUMBER:\", i)\n",
        "    X_train, X_outerCV = X.iloc[train_idx].copy(), X.iloc[test_idx].copy()\n",
        "    y_train, y_outerCV = y.iloc[train_idx].copy(), y.iloc[test_idx].copy()\n",
        "    #run feature selection and CV\n",
        "    clf = GridSearchCV(estimator = MAHOMES_clf,\n",
        "                       cv=inner_cv_type,\n",
        "                       param_grid = additional_params,\n",
        "                       scoring = make_scorer(prec_score_custom, # Used \"prec\" sorer\n",
        "                                             greater_is_better = True),\n",
        "                     #  iid = True,\n",
        "                       refit = False,\n",
        "                       verbose=100,\n",
        "                       n_jobs = num_jobs)\n",
        "    #Train for the given combinations\n",
        "    clf.fit(X_train.reset_index(drop=True), y_train.reset_index(drop=True))\n",
        "    results = clf.cv_results_\n",
        "\n",
        "    #somehow get best combination of multiple scoring terms\n",
        "    print(results)\n",
        "    ranks = []\n",
        "    for key in results:\n",
        "        if \"rank_test_\" in key:\n",
        "            ranks.append(results[key])\n",
        "    #best params will have to be identified for full data set for final model building after best model is selected\n",
        "    best_params = results['params'][np.argmin(np.sum(np.asarray(ranks), axis = 0))]\n",
        "    print(\"ideal params\", best_params)\n",
        "    outer_params.append(best_params)\n",
        "\n",
        "    ## set the new classifier to these parameters\n",
        "    outer_clf = MAHOMES_clf.set_params(**best_params)\n",
        "    ## fit on all training data - this is what GridSearchCV(refit = True) will do anyways,\n",
        "    ## but its selection of params is not necessary Meghans\n",
        "    outer_clf.fit(X_train.reset_index(drop=True), y_train.reset_index(drop=True))\n",
        "\n",
        "    outerCV = pd.DataFrame(y_outerCV, columns=['Catalytic'])\n",
        "\n",
        "    #predict based on fitted outer CV model\n",
        "    outerCV_preds =  pd.DataFrame(outer_clf.predict(X_outerCV.reset_index(drop=True)), columns=['Prediction'])\n",
        "    outerCV_preds['SITE_ID']=X_outerCV.index\n",
        "    outerCV_preds = outerCV_preds.set_index('SITE_ID', drop=True)\n",
        "\n",
        "    outerCV = pd.merge(outerCV, outerCV_preds, left_index=True, right_index=True)\n",
        "\n",
        "    ## calculate stats\n",
        "    accuracy = accuracy_score(outerCV.Catalytic, outerCV.Prediction)\n",
        "    recall = recall_score(outerCV.Catalytic, outerCV.Prediction)\n",
        "    precision = precision_score(outerCV.Catalytic, outerCV.Prediction)\n",
        "    true_neg_rate = len( outerCV[(outerCV.Catalytic == 0) & (outerCV.Prediction == 0)] )/ len(outerCV[(outerCV.Catalytic == 0)])\n",
        "    mcc = matthews_corrcoef(outerCV.Catalytic, outerCV.Prediction)\n",
        "    dist_rand = (recall + -1*(1-true_neg_rate)) / np.sqrt(2)\n",
        "    TN, FP, FN, TP = confusion_matrix(outerCV.Catalytic, outerCV.Prediction ).ravel()\n",
        "    outer_cv_results.append([ accuracy, precision, recall, true_neg_rate, mcc, dist_rand, TP, TN, FP, FN ])\n",
        "    print(\"accuracy measures\", mcc, accuracy, recall, dist_rand)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNygVzv8oiKm",
        "outputId": "7ce74781-b25c-4b36-f8e2-aca345e2a91c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "## check that all hyperparmeters match up for each inner CV run => model stability\n",
        "outer_params_df = pd.DataFrame(outer_params)\n",
        "outer_params_df.to_csv(\"%s_%s_%s_params.txt\"%(\"MLP\", MAHOMES_feature_set, \"Prec\"))\n",
        "stable = True\n",
        "if max(outer_params_df.nunique())>1:\n",
        "    stable = False\n",
        "print(stable)\n",
        "#params don't match up\n",
        "# ideal params output\n",
        "# Loop number,alpha,hidden_layer_sizes\n",
        "# 0,0.001,\"(100,)\"\n",
        "# 1,0.01,\"(50,)\"\n",
        "# 2,0.001,\"(200,)\"\n",
        "# 3,0.001,\"(100,)\"\n",
        "# 4,0.001,\"(200,)\"\n",
        "# 5,0.001,\"(100,)\"\n",
        "# 6,0.001,\"(100,)\"\n",
        "\n",
        "#No clear consensus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvdUHuzHsXjo",
        "outputId": "ae999d5b-ebe8-4bf7-e4dc-6fed1a6cbf44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'alpha': 0.001, 'hidden_layer_sizes': (100,)}\n"
          ]
        }
      ],
      "source": [
        "#most common overall best params:\n",
        "overall_best_params={}\n",
        "for param in outer_params_df.columns:\n",
        "    most_pop_param_val = outer_params_df[param].iloc[0]\n",
        "    for cur_param_val in outer_params_df[param].unique():\n",
        "        num_most_vals = len(outer_params_df[outer_params_df[param]==most_pop_param_val])\n",
        "        num_cur_vals = len(outer_params_df[outer_params_df[param]==cur_param_val])\n",
        "        if num_cur_vals>num_most_vals:\n",
        "            most_pop_param_val=cur_param_val\n",
        "\n",
        "    overall_best_params[param]=most_pop_param_val\n",
        "print(overall_best_params)\n",
        "#OUtput: {'alpha': 0.001, 'hidden_layer_sizes': (100,)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LoqRKnCbRm5",
        "outputId": "0481634e-3ae2-437c-8256-66c338879659"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "random_seed = 0\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
            "\tTP=152 \tTN=319 \tFP=26 \tFN=19\n",
            "random_seed = 1\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
            "\tTP=154 \tTN=313 \tFP=32 \tFN=17\n",
            "random_seed = 2\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
            "\tTP=146 \tTN=317 \tFP=28 \tFN=25\n",
            "random_seed = 3\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
            "\tTP=154 \tTN=318 \tFP=27 \tFN=17\n",
            "random_seed = 4\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
            "\tTP=145 \tTN=314 \tFP=31 \tFN=26\n",
            "random_seed = 5\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
            "\tTP=154 \tTN=314 \tFP=31 \tFN=17\n",
            "random_seed = 6\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
            "\tTP=146 \tTN=319 \tFP=26 \tFN=25\n",
            "random_seed = 7\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
            "\tTP=149 \tTN=319 \tFP=26 \tFN=22\n",
            "random_seed = 8\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
            "\tTP=158 \tTN=316 \tFP=29 \tFN=13\n",
            "random_seed = 9\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
            "\tTP=151 \tTN=315 \tFP=30 \tFN=20\n"
          ]
        }
      ],
      "source": [
        "#Evaluate model with these computed ideal params\n",
        "MAHOMES_alg = \"NeurNet\" #MLP classifier\n",
        "MAHOMES_feature_set = \"AllMeanSph\"\n",
        "MAHOMES_clf = MLPClassifier(learning_rate_init = 0.01, activation='relu', hidden_layer_sizes= (100,), alpha = 0.001 )\n",
        "MAHOMES_Tsite_predictions = evaluate_model_with_Tsite(MAHOMES_clf, MAHOMES_feature_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'actual': SITE_ID\n",
              " 6s9z_0     True\n",
              " 6g5l_0     True\n",
              " 6hwz_0     True\n",
              " 6qww_0     True\n",
              " 6qww_1    False\n",
              "           ...  \n",
              " 6cda_0    False\n",
              " 6ee7_2    False\n",
              " 6nq5_0    False\n",
              " 6fx6_0    False\n",
              " 6nef_0    False\n",
              " Name: Catalytic, Length: 516, dtype: bool}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_site_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "sczgFQhkyPlM",
        "outputId": "0f722a38-8f2d-4fec-e8b7-76af3c657a84"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"#Give or take a percent\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"Algorithm\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"NeurNet\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Feature Set\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"AllMeanSph\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 92.05,\n        \"max\": 92.05,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          92.05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MCC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.83,\n        \"max\": 0.83,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.83\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 93.57,\n        \"max\": 93.57,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          93.57\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TrueNegRate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 91.3,\n        \"max\": 91.3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          91.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 84.21,\n        \"max\": 84.21,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          84.21\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-b653e2cd-4b3f-44fa-abf4-7be29328b69b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Algorithm</th>\n",
              "      <th>Feature Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>MCC</th>\n",
              "      <th>Recall</th>\n",
              "      <th>TrueNegRate</th>\n",
              "      <th>Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NeurNet</td>\n",
              "      <td>AllMeanSph</td>\n",
              "      <td>92.05</td>\n",
              "      <td>0.83</td>\n",
              "      <td>93.57</td>\n",
              "      <td>91.3</td>\n",
              "      <td>84.21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b653e2cd-4b3f-44fa-abf4-7be29328b69b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b653e2cd-4b3f-44fa-abf4-7be29328b69b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b653e2cd-4b3f-44fa-abf4-7be29328b69b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "  Algorithm Feature Set  Accuracy   MCC  Recall  TrueNegRate  Precision\n",
              "0   NeurNet  AllMeanSph     92.05  0.83   93.57         91.3      84.21"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#display results\n",
        "scores = check_result_metrics(MAHOMES_alg, MAHOMES_feature_set,  MAHOMES_Tsite_predictions)\n",
        "display(scores.round(2))\n",
        "#REsults:\n",
        "# Algorithm\tFeature Set\tAccuracy\tMCC\tRecall\tTrueNegRate\tPrecision\n",
        "# 0\tNeurNet\tAllMeanSph\t91.28\t0.81\t92.4\t90.72\t83.16\n",
        "\n",
        "#Give or take a percent..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh5xmwBmOPHX"
      },
      "source": [
        "Custom MLP Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bToM2TNo3dPK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
